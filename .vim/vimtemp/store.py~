#!/usr/bin/env python
__test__=False

import os
import sys
import subprocess
import calendar
import urllib
import urllib2
import requests
import re
import datetime
import errno
import stat
import tempfile
import gzip
import fnmatch
import time
import collections
import shutil
import arrow
import random
import json

import deepy.map
import deepy.cfg
import deepy.log as log
import deepy.timerange
import deepy.util
import deepy.deploy

import boto
import fabric.api
import fabric.contrib.files

import snakebite.errors as se
from snakebite.client import AutoConfigClient

try:
    import ujson
except ImportError:
    import json as ujson

# Cache bucket to speed up s3 retrieval
s3_bucket_cache_name = None
s3_bucket_cache = None

# Cache to require less s3 retrieval
# last_update_cache[filename] --> timedelta of last update time
last_update_cache = {}

S3Info = collections.namedtuple('S3Info', 'bucket, key_prefix')

def s3_get_info(deployment_id=None):
    retval = None
    bucket = s3_get_bucket(deployment_id)
    key_prefix = s3_get_key_prefix(deployment_id)
    if bucket is not None and key_prefix is not None:
        retval = S3Info(bucket, key_prefix)
    return retval

def s3_get_key_prefix(deployment_id=None):
    retval = ''
    if deployment_id is None:
        deployment_id = deepy.cfg.deployment_id
        if deployment_id is None:
            log.warn('could-not-determine-deployment')
            return None
    slice_data = deepy.cfg.slice_config
    if slice_data.get('shared_bucket', None) is not None:
        retval = 'd/%s.pdrlabs.net/' % deployment_id
    return retval


# Connect to s3 and get bucket.
# deployment_id is used to select the bucket name. If not provided,
# it's autoset from cfg (command line or slice).
# Returns bucket or None.
def s3_get_bucket(deployment_id=None):
    global s3_bucket_cache
    global s3_bucket_cache_name

    if deepy.cfg.isolated_deployment:
        return None

    if deployment_id is None:
        deployment_id = deepy.cfg.deployment_id
        if deployment_id is None:
            log.warn('could-not-determine-deployment')
            return None

    # Use cache
    if s3_bucket_cache_name == deployment_id and s3_bucket_cache != None:
        return s3_bucket_cache

    # Pull settings from slice, if it's there.
    slice_data = deepy.cfg.slice_config

    if 'credentials' in slice_data:
        access_id = slice_data['credentials'][0]
        access_secret = slice_data['credentials'][1]
    else:
        access_id = None
        access_secret = None

    try:
        # boto.cfg or env vars.
        s3 = boto.connect_s3()
    except:
        # Normal deployment case
        try:
            s3 = boto.connect_s3(access_id, access_secret)
        except:
            log.warn('s3-connect-failed')
            return None

    # XXX Need to bootstrap and search for correct bucket if slice is
    # not available.

    bucket_id = deployment_id
    if slice_data.get('shared_bucket', None) is not None:
        bucket_id = slice_data['shared_bucket']

    bucket_path = '%s.pdrlabs.net' % bucket_id
    try:
        bucket = boto.s3.bucket.Bucket(s3, name=bucket_path)
    except Exception as e:
        # This used to be warn, but not finding a bucket is actually not
        # exceptional behavior at this point -- we might be checking that the
        # bucket indeed does not exist. The caller should warn if not finding
        # the bucket is a problem.
        log.debug(e)
        log.debug('get-bucket-failed (s3_get_bucket) %s' % (bucket_path))
        return None


    # This seems to solve problem of telarg and avoid redirect problem
    # with more strict SSL CERT checking
    # https://github.com/boto/boto/issues/2836
    location = bucket.get_location()
    if location:
        s3 = boto.s3.connect_to_region(
            location,
            aws_access_key_id=access_id,
            aws_secret_access_key=access_secret
            )
        bucket = boto.s3.bucket.Bucket(s3, name=bucket_path)

        
    # Cache!
    s3_bucket_cache_name = deployment_id
    s3_bucket_cache = bucket

    return bucket

aws_timestr_unknown = time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.localtime(0))

#Getting an individual key returns a different timestr...
RFC1123 = '%a, %d %b %Y %H:%M:%S %Z'
def iso8601_to_timestamp(iso8601_time):
    SUBSECOND_RE = re.compile('\.[0-9]+')
    iso8601_time = SUBSECOND_RE.sub('', iso8601_time)
    try:
        return time.mktime(time.strptime(iso8601_time, boto.utils.ISO8601))
    except ValueError:
        return time.mktime(time.strptime(iso8601_time, RFC1123))

#Convert fnames for flow/h5flow to their s3 friendly equivalent (needed because normal conversion assumes they are inside of cache)
#Can take a list or a single path
def local_flow_path_to_s3(local_path):
    remote_path = local_path
    if deepy.cfg.hood_dir in local_path:
        remote_path = local_path.replace(deepy.cfg.hood_dir, "")
    return remote_path

# Keep this in one place so if AWS changes the format it will be easy for us
# to update.
def aws_timestr_to_timestamp(aws_timestr):
    # Ex: 2012-07-26T19:13:54.000Z
    return int(calendar.timegm(time.strptime(aws_timestr[:-5],
                           '%Y-%m-%dT%H:%M:%S')))

# Generate a for a given remote store dir, generate a full listing and store it
# in the directory.
#
# Might be nice to integrate with the rest of our remote_ls stuff
def generate_dir_list(local_dirname, local_path=None, deployment_id=None,
                     force_remote=None, verbose=False, dir_mtimes=False):

    dir_list = ls_dir_remote(local_dirname, deployment_id=deployment_id,
                             force_remote=force_remote, dir_mtimes=dir_mtimes)
    if dir_mtimes:
        dir_list = [{'name': os.path.basename(n['name']), 'mtime':n['mtime']} for n in dir_list]
    else:
        dir_list = [os.path.basename(n) for n in dir_list]

    obj = { 'dir_list': dir_list }
    if local_path is None:
        if local_dirname[len(local_dirname)-1] != '/':
            local_dirname += '/'
        local_path = local_dirname + 'dir_list.json.gz'

    log.info("saving-dir-list %s" % (local_path))
    simple_save_json(obj, local_path, deployment_id=deployment_id,
                     force_remote=force_remote, verbose=verbose)


def rm_files(local_paths, deployment_id=None, force_remote=None):
    '''
    Remove files from disk and from remote. Returns a dict of boolean tuples
    (local, remote), True if something was removed, False on error, None if not
    applicable.

    (True, False): Indicates local removal but remote removal failed
    (True, None): Common result with remote==local
    (True, None): File removed remotely but didn't exist on remote
    (False, True): File removed remotely but local rm failed
    '''
    # Prevent confusion, only accept list.
    assert isinstance(local_paths, list)

    remote_types = _get_remote_types(local_paths, force_remote=force_remote)

    results = {}
    for remote_type, local_path in zip(remote_types, local_paths):
        res_local = None
        res_remote = None

        if os.path.exists(local_path):
            res_local = False
            try:
                os.remove(local_path)
                res_local = True
            except OSError:
                log.exception('removing-local-file {}'.format(local_path))

        if remote_type == 's3':
            res_remote = False
            s3_info = s3_get_info(deployment_id)
            if s3_info is not None:
                mtime = _ls_file_s3(s3_info, local_path)
                if mtime is None:
                    res_remote = None
                else:
                    res_remote = _rm_file_s3(s3_info, local_path)
        elif remote_type == 'store':
            raise NotImplementedError("HTTP host store no longer implemented!")
        elif remote_type == 'hdfs':
            _rm_file_hdfs(local_path)

        results[local_path] = (res_local, res_remote)

    return results

def _rm_file_hdfs(local_path):
    client = get_hdfs_client()
    dname = os.path.dirname(local_path)
    list(client.mkdir([dname], create_parent=True)) # generator returned
    try:
        _hdfs_rm(local_path)
    except IOError:
        log.warn('hdfs-rm-failed')


def _rm_file_s3(s3_info, local_path):
    # Cleanup path for s3. Access functions will do the same.
    local_path = _path_normalize(local_path)
    if local_path.startswith(deepy.cfg.cache_dir + '/'):
        local_path = local_path[len(deepy.cfg.cache_dir + '/'):]
    # Also, keys don't start with slash.
    if len(local_path) > 0 and local_path[0] == '/':
        local_path = local_path[1:]

    k = s3_info.bucket.get_key(s3_info.key_prefix + local_path)
    if k is None:
        return None

    k.delete()
    return True

def ls_files_remote(local_paths, deployment_id=None, force_remote=None, 
    bulk=False):
    '''
    Check for specific files rather than glob. Return dict of files that
    exist and their mtimes.
    '''

    # Prevent confusion, only accept list.
    assert isinstance(local_paths, list)

    remote_types = _get_remote_types(local_paths, force_remote=force_remote)
    # Maybe change this to do all of a particular remote_type at once.
    # Probably doesn't matter if each has to be individually queried.
    results = {}

    # Group by remote type
    remote_type_path_dict = collections.defaultdict(list)
    for remote_type, local_path in zip(remote_types, local_paths):
        remote_type_path_dict[remote_type].append(local_path)

    for remote_type, local_paths in remote_type_path_dict.iteritems():
        local_paths = set(local_paths)

        #
        # Non-bulk. Don't worry about bulk checking if there are less than 200 paths to check
        #
        if not bulk or len(local_paths) < 200 or not remote_type in {"s3"}:
            for local_path in local_paths:
                if remote_type == 'local':
                    try:
                        mtime = int(os.stat(local_path).st_mtime)
                    except OSError:
                        mtime = None
                elif remote_type == 's3':
                    s3_info = s3_get_info(deployment_id)
                    if s3_info is None:
                        mtime = None
                    else:
                        mtime = _ls_file_s3(s3_info, local_path)
                elif remote_type == 'store':
                    raise NotImplementedError("HTTP host store no longer implemented!")
                elif remote_type == 'hdfs':
                    start = time.time()
                    client = get_hdfs_client()
                    mtime = None
                    try:
                        filenames = list(client.ls([local_path]))
                        if filenames:
                            mtime = filenames[0]['modification_time']
                            mtime = mtime/1000.0 # We get millseconds since the epoch and need to massage to python standard
                        log.debug('hdfs-ls seconds=%f,file=%s' % (time.time() - start, local_path))
                    except se.FileNotFoundException:
                       mtime = None
                       log.debug('hdfs-ls-not-found-for-%s' % (local_path))
                # None signifies file doesn't exist.
                if mtime is not None:
                    results[local_path] = mtime
        # Bulk checking
        else:
            if remote_type == 's3':
                s3_info = s3_get_info(deployment_id)
                if s3_info is None:
                    ls_results = {}
                else:
                    ls_results = _ls_file_bulk_s3(s3_info, local_paths)

                for local_path in local_paths:
                    results[local_path] = ls_results.get(local_path)

    return results



def _ls_file_bulk_s3(s3_info, local_paths):
    relevant_directories = _get_common_prefix(local_paths, date=False)
    results = {}
    for local_directory in relevant_directories:
        log.debug("listing {}".format(local_directory))
        s3_directory = get_s3_location(local_directory)
        # local_directory = os.path.dirname(local_directory)
        s3_list = s3_info.bucket.list(s3_directory)

        for i, key in enumerate(s3_list):
            if i % 1000 == 0:
                log.debug("listing {}: {}".format(local_directory, i))
            local_path = os.path.join(local_directory, os.path.basename(key.key))
            mtime = key.last_modified
            results[local_path] = arrow.get(mtime).float_timestamp

    return results
# ls a directory
def ls_dir_remote(local_dirname, deployment_id=None, force_remote=None,
                  dir_mtimes=False):
    return ls_glob_remote(local_dirname, '*', deployment_id, force_remote,
                          dir_mtimes=dir_mtimes)

# Does the path split for you.
def ls_glob_simple(local_pat,
        deployment_id=None, force_remote=None, dir_mtimes=False):
    head, tail = os.path.split(local_pat)
    return ls_glob_remote(head, tail, deployment_id, force_remote, dir_mtimes)

# ls with glob matching
def ls_glob_remote(local_dirname, file_pat,
        deployment_id=None, force_remote=None, dir_mtimes=False):
    return _ls_match_remote('glob', local_dirname, file_pat,
        deployment_id, force_remote, dir_mtimes=dir_mtimes)

# ls with regexp matching
def ls_re_remote(local_dirname, file_pat,
        deployment_id=None, force_remote=None, dir_mtimes=False):
    return _ls_match_remote('re', local_dirname, file_pat,
        deployment_id, force_remote, dir_mtimes=dir_mtimes)

# Return most recent file, or None.
def ls_re_remote_last(local_dirname, file_pat,
        deployment_id=None, force_remote=None, dir_mtimes=False):
    files = ls_re_remote_tail(local_dirname, file_pat, 1,
            deployment_id, force_remote, dir_mtimes=dir_mtimes)
    if len(files) == 1:
        return files[0]
    return None


def ls_glob_remote_last(local_dirname, file_pat,
        deployment_id=None, force_remote=None, dir_mtimes=False):
    files = ls_glob_remote_tail(local_dirname, file_pat, 1,
            deployment_id, force_remote, dir_mtimes=dir_mtimes)
    if len(files) == 1:
        return files[0]
    return None

# Sorts the matching files, and returns the last n.
# Since our files are named sequentially by time, this lets you get the
# n most recent files.
def ls_re_remote_tail(local_dirname, file_pat, n,
        deployment_id=None, force_remote=None, dir_mtimes=False):
    files = deepy.store.ls_re_remote(local_dirname, file_pat,
            deployment_id, force_remote, dir_mtimes=dir_mtimes)
    files.sort()
    return files[-n:]

def ls_glob_remote_tail(local_dirname, file_pat, n,
        deployment_id=None, force_remote=None, dir_mtimes=False):
    files = deepy.store.ls_glob_remote(local_dirname, file_pat,
            deployment_id, force_remote, dir_mtimes=dir_mtimes)
    files.sort()
    return files[-n:]

def is_file_fresh(local_path, cache_age):
    '''
    Check if a file requires pulling from s3
    '''
    global last_update_cache

    if not os.path.isfile(local_path):
        # Local file not on disk, use s3
        return False

    if local_path not in last_update_cache:
        # Local file not in cache yet, update it
        update_cache(local_path)
        return False

    current_time = datetime.datetime.now()
    last_update = last_update_cache[local_path]
    last_modified_time = os.path.getmtime(local_path)
    last_modified_time = datetime.datetime.fromtimestamp(last_modified_time)

    if last_modified_time > last_update:
        # Another process has updated the file more recently
        # than this one has, update time
        update_cache(local_path)

    if (current_time - last_update) > cache_age:
        # Local file is out-of-date, update it
        update_cache(local_path)
        return False

    return True

def update_cache(local_path):
    '''
    Update cache of when file was last written to
    '''
    global last_update_cache
    last_update_cache[local_path] = datetime.datetime.now()

def clear_cache():
    '''
    Clear cache of when files were last written to
    '''
    global last_update_cache
    last_update_cache = {}

def make_list(param):
    '''
    Make single values iterable
    '''
    return param if isinstance(param, list) else [param]

def cache_load_from_remote(local_paths, deployment_id=None, force_remote=None, \
        verbose=False, cache_age=30):
    '''
    Bring a remote file to local cache.
    '''

    cache_age = datetime.timedelta(minutes=cache_age)

    # Accept a list or single item
    local_paths = make_list(local_paths)

    remote_types = _get_remote_types(local_paths, force_remote)
    for remote_type, local_path in zip(remote_types, local_paths):

        # XXX Temporary check to help catch code that isn't fully specifying
        # path. (Some old code depended on cache_load_from_remote
        # automatically prepending cache_dir.)
        if not local_path.startswith(deepy.cfg.home_dir):
            log.debug('possible-bad-path {}'.format(local_path))

        if remote_type == 'local' or is_file_fresh(local_path, cache_age):
            continue
        elif remote_type == 's3':
            s3_info = s3_get_info(deployment_id)
            if s3_info:
                _cache_load_from_s3(s3_info, local_path)
        elif remote_type == 'store':
            raise NotImplementedError("HTTP host store no longer implemented!")
        elif remote_type == 'hdfs':
            _cache_load_from_hdfs(local_path)

    for local_path in local_paths:
        permission = stat.S_IWUSR | stat.S_IRUSR | stat.S_IROTH | stat.S_IWOTH
        try:
            os.chmod(local_path, permission)
        except OSError:
            pass

def cache_save_to_remote(local_paths, deployment_id=None, force_remote=None, \
        verbose=False, s3replace=True, audit_string=None, ignore_md5=False):
    '''
    Save a cached file to remote storage.
    '''
    # Accept a list or single item
    local_paths = make_list(local_paths)

    if audit_string:
        deepy.log.log_this_call("{}: {}".format(audit_string, ",".join(local_paths)))

    remote_types = _get_remote_types(local_paths, force_remote)
    for remote_type, local_path in zip(remote_types, local_paths):
        if remote_type == 'local':
            continue
        elif remote_type == 's3':
            s3_info = s3_get_info(deployment_id)
            if s3_info:
                _cache_save_to_s3(s3_info, local_path, replace=s3replace, ignore_md5=ignore_md5)
            else:
                log.warn('s3-info-failed {}'.format(local_path))
        elif remote_type == 'store':
            raise NotImplementedError("HTTP host store no longer implemented!")
        elif remote_type == 'hdfs':
            if not _hdfs_save_to_remote(local_path):
                log.warn('hdfs-save-failed-{}'.format(local_path))

# Load json file.
# Can generally leave deployment_id and force_remote as None.
def simple_load_json(local_path, deployment_id=None, force_remote=None,
                     object_pairs_hook=dict, cache_age=30):

    cache_load_from_remote([local_path], deployment_id, force_remote, cache_age=cache_age)

    if not os.path.isfile(local_path):
        log.debug('json-file-not-found %s' % (local_path))
        return None
    try:

        if local_path.endswith(".gz"):
            fp = gzip.open(local_path)
        else:
            fp = open(local_path)

        success=False
        try:
            if object_pairs_hook == dict:
                j = ujson.load(fp)
                success=True
        except TypeError:
            log.exception('ujson-load-failed')
            pass

        if not success:
            j = json.load(fp, object_pairs_hook=object_pairs_hook)

    except:
        log.exception('load-json-failed')
        return None

    return j

# Save json to cache and remote.
# obj can either be a string (already encoded) or a dict/list.
# Can generally leave deployment_id and force_remote as None.
def simple_save_json(obj, local_path, deployment_id=None, force_remote=None, verbose=False, s3replace=True, audit_string=None, ignore_md5=False):
    if audit_string:
        deepy.log.log_this_call("{}: {}".format(audit_string, local_path))

    if not os.path.exists(deepy.cfg.data_tmp):
        os.makedirs(deepy.cfg.data_tmp)

    if not os.path.exists(os.path.dirname(local_path)):
        os.makedirs(os.path.dirname(local_path))

    # Save to cache
    tmp_file = tempfile.NamedTemporaryFile(delete=False, dir=deepy.cfg.data_tmp)
    tmp_name = tmp_file.name
    tmp_file.close()

    if local_path.endswith(".gz"):
        fn = gzip.open
    else:
        fn = open

    with fn(tmp_name, 'w') as fd:
        json.dump(obj, fd, default=deepy.util.default_json_encoder, indent=2)

    try:
        os.rename(tmp_name, local_path)
    except OSError:
        log.exception("rename-failed %s %s" % (tmp_name, local_path))
        return
    try:
        permissions = stat.S_IWUSR | stat.S_IRUSR | stat.S_IROTH | stat.S_IWOTH
        os.chmod(local_path, permissions)
    except OSError:
        log.exception("chmod-failed %s" % (local_path))
        return

    cache_save_to_remote(local_path, deployment_id, force_remote,
                         verbose=verbose, s3replace=s3replace, ignore_md5=ignore_md5)

def simple_save_content(content, local_path, deployment_id=None, force_remote=None, verbose=False, s3replace=True, audit_string=None):
    if audit_string:
        deepy.log.log_this_call("{}: {}".format(audit_string, local_path))

    if not os.path.exists(deepy.cfg.data_tmp):
        os.makedirs(deepy.cfg.data_tmp)

    if not os.path.exists(os.path.dirname(local_path)):
        os.makedirs(os.path.dirname(local_path))

    # Save to cache
    tmp_file = tempfile.NamedTemporaryFile(delete=False, dir=deepy.cfg.data_tmp)
    tmp_name = tmp_file.name
    tmp_file.close()

    if local_path.endswith(".gz"):
        fn = gzip.open
    else:
        fn = open

    with fn(tmp_name, 'w') as fd:
        fd.write(content)

    try:
        os.rename(tmp_name, local_path)
    except OSError:
        log.exception("rename-failed %s %s" % (tmp_name, local_path))
        return
    try:
        permissions = stat.S_IWUSR | stat.S_IRUSR | stat.S_IROTH | stat.S_IWOTH
        os.chmod(local_path, permissions)
    except OSError:
        log.exception("chmod-failed %s" % (local_path))
        return

    cache_save_to_remote(local_path, deployment_id, force_remote,
                         verbose=verbose, s3replace=s3replace)
###############################
#
# Internal functions. Do not access directly.
#
def _hdfs_rm(local_path):
    start = time.time()
    client = get_hdfs_client()
    try:
        list(client.delete([local_path], recurse=True))
    except se.FileNotFoundException:
        log.info("hdfs-rm-failed-for-%s" % (local_path))
        raise IOError
    log.info('hdfs-delete seconds=%f,file=%s' % (time.time() - start, local_path))

def _hdfs_put(local_path):
    start = time.time()
    #Test with support
    cmd = "HADOOP_USER_NAME=hdfs hdfs dfs -put -f {} {}".format(local_path,local_path)
    try:
        subprocess.check_output([cmd], shell=True)
    except subprocess.CalledProcessError:
        log.debug("put-failed-making-sure-directories-are-made")
        try:
            #Make directory every time w/ snakebite
            mkdir_cmd = "HADOOP_USER_NAME=hdfs hdfs dfs -mkdir -p {}".format(os.path.dirname(local_path))
            subprocess.check_output([mkdir_cmd], shell=True)
            subprocess.check_output([cmd], shell=True)
        except subprocess.CalledProcessError as e:
            log.debug(e.output)
            log.debug(cmd)
            log.warn("hdfs-put-failed-for-%s" % (local_path))
            raise IOError
    log.info('hdfs-put seconds=%f,file=%s' % (time.time() - start, local_path))

def _hdfs_save_to_remote(local_path):
    client = get_hdfs_client()
    dname = os.path.dirname(local_path)
    list(client.mkdir([dname], create_parent=True)) # generator returned
    try:
        _hdfs_put(local_path)
        return True
    except IOError:
        log.warn('hdfs-put-failed {}'.format(local_path))
        return False

def _cache_save_to_s3(s3_info, local_path, replace=True, ignore_md5=False):
    key_path = s3_info.key_prefix + local_path[len(deepy.cfg.cache_dir + '/'):]
    key = s3_info.bucket.get_key(key_path)

    if not ignore_md5:
        try:
            fp = open(local_path)
            md5_local = key.compute_md5(fp)[0]
            md5_key = key.etag.strip("\"")
            fp.close()

            if md5_local == md5_key:
                log.debug('md5-matches %s %s' % (local_path, key_path))
                return
        except:
            pass

    if key is None:
        key = s3_info.bucket.new_key(key_path)

    key.set_contents_from_filename(local_path, replace=replace)

    # sometimes a race condition?
    try:
        key.set_acl('private')
    except:
        pass

    log.debug("Wrote to S3 %s  -> s3://%s/%s (%2.1f Kbytes)" % (local_path, s3_info.bucket.name, key_path, key.size / 1000))

def _hdfs_store_tmp_path():
    '''
    Namespaced temoporary file for hdfs copy
    '''
    file_temp = 'hdfs-store.%Y-%m-%d-%H-%M-%S.{}'.format(random.random())
    return os.path.join(deepy.cfg.data_tmp, deepy.timerange.substitute_timestamp(file_temp, arrow.now()))

def _hdfs_check_stale(local_path, client):
    return _hdfs_check_stale_ts(local_path, client)

def _hdfs_check_stale_ts(local_path, client):
    '''Returns None if the file does not exist
    Returns True if the file is stale
    Returns False if it is not
    '''
    try:
        stat = client.stat([local_path])
    except se.FileNotFoundException:
        log.debug('%s-doesnt-exist-not-fetching' % (local_path))
        return None

    hdfs_mtime = stat.get('modification_time')
    if not hdfs_mtime:
        return False


    if not os.path.exists(local_path):
        return True

    mtime = int(os.stat(local_path).st_mtime)*1000
    #hdfs_mtime is in msecs

    if mtime <= int(hdfs_mtime):
        return True
    else:
        return False

#XXX For now check hdfs followed by the traditional store
def _cache_load_from_hdfs(local_path):
    start = time.time()
    client = get_hdfs_client()
    dname = os.path.dirname(local_path)
    if not os.path.exists(dname):
        os.makedirs(dname)

    stale = _hdfs_check_stale(local_path, client)
    if stale is False:
        log.debug("%s-isnt-stale" % (local_path))
        return True
    elif stale is None:
        log.info("%s-does-not-exist" % (local_path))
        return False

    res = None
    #Warn on exception and keep return value check
    tmp_path = _hdfs_store_tmp_path()
    try:
        res = list(client.copyToLocal([local_path], tmp_path, check_crc=False))
    except se.FileNotFoundException:
        pass
    if not res:
        log.warn('possible-bad-hdfs-path {} {}'.format(local_path, tmp_path))
        return False
    log.info('hdfs-get seconds=%f,file=%s' % (time.time() - start, local_path))

    try:
       os.rename(tmp_path, local_path)
    except OSError:
       log.warn("rename-failed %s %s" % (tmp_path, local_path))
       return

    try:
       os.chmod(local_path, stat.S_IWUSR|stat.S_IRUSR|stat.S_IROTH|stat.S_IWOTH)
    except OSError:
       log.warn("chmod-failed %s %s" % (tmp_path, local_path))

    return True

def _cache_load_from_s3(s3_info, local_path):
    key_path = s3_info.key_prefix + local_path[len(deepy.cfg.cache_dir + '/'):]

    key = s3_info.bucket.get_key(key_path)
    if key is None:
        log.debug('No key for s3://%s/%s' % (s3_info.bucket.name, key_path))
        return

    #last_modified = time.strptime (key.last_modified, "%a, %d %b %Y %H:%M:%S %Z")

    try:
        fp = open(local_path)
        md5_local = key.compute_md5(fp)[0]
        md5_key = key.etag.strip("\"")
        fp.close()

        if md5_local == md5_key:
            log.debug("MD5 matches s3://%s/%s == %s" %
                    (s3_info.bucket.name, key_path, local_path))
            return
    except:
        pass


    try:
        os.makedirs(os.path.dirname(local_path))
    except:
        pass

    try:
        os.makedirs(os.path.dirname(deepy.cfg.data_tmp + "/"))
    except:
        pass

    tempFile = tempfile.NamedTemporaryFile(delete=False, dir=deepy.cfg.data_tmp)
    tmp_name = tempFile.name
    tempFile.close()

    try:
        start = time.time()
        log.debug("Fetching s3://%s/%s  -> %s  (%2.1f Kbytes)" %
                (s3_info.bucket.name, key_path, local_path, key.size / 1000))
        key.get_contents_to_filename(tmp_name)
        log.debug("Fetching Done (%f seconds)" % (time.time() - start))
    except:
        log.debug("Could not fetch %s://%s -> %s" % (s3_info.bucket.name, key_path, local_path))
        return

    try:
        os.rename(tmp_name, local_path)
        os.chmod(local_path, stat.S_IWUSR|stat.S_IRUSR|stat.S_IROTH|stat.S_IWOTH)
    except Exception,e:
        print "Rename / chmod failed for ", tmp_name, local_path, str(e)
        return

def get_s3_location(local_path):
    # Cleanup path for s3. Access functions will do the same.
    local_path = _path_normalize(local_path)
    if local_path.startswith(deepy.cfg.cache_dir + '/'):
        local_path = local_path[len(deepy.cfg.cache_dir + '/'):]
    # Also, keys don't start with slash.
    if len(local_path) > 0 and local_path[0] == '/':
        local_path = local_path[1:]
    return local_path

def _get_common_prefix(local_paths, date=True):
    if date == False:
        relevant_directories = set()
        for local_path in local_paths:
            relevant_directories.add(os.path.dirname(local_path))
        return relevant_directories

    relevant_directories = set()
    for local_path in local_paths:
        try:
            time, granularity = deepy.timerange.parse_datetime_timeperiod(local_path, time_sep="-")
        except ValueError as e:
            if e.message in ["month must be in 1..12",
                             "day is out of range for month",
                             "hour must be in 0..23",
                             "minute must be in 0..59"]:
                granularity = None
            else:
                raise e

        template_length = None
        if granularity == "minute":
            template_length = 11
            time_string = arrow.get(time).format("YYYY-MM-DD-HH-mm")
        elif granularity == "hour":
            template_length = 11
            time_string = arrow.get(time).format("YYYY-MM-DD-HH")
        elif granularity == "day":
            template_length = 8
            time_string = arrow.get(time).format("YYYY-MM-DD")
        elif granularity == "month":
            template_length = 5
            time_string = arrow.get(time).format("YYYY-MM")

        if template_length is not None:
            time_index = local_path.index(time_string) + template_length
            prefix = local_path[:time_index]
        elif granularity == "year":
            prefix = local_path
        else:
            prefix = os.path.join(os.path.dirname(local_path), "")

        relevant_directories.add(prefix)

    return relevant_directories


def list_files_remote(local_paths, deployment_id=None, force_remote=None, date=True):
    '''
    Glob remote files and return a dictionary of local_file_path -> mtime
    '''

    retval = {}
    for path in local_paths:
        files = ls_glob_remote(path, "*", deployment_id=deployment_id, force_remote=force_remote, dir_mtimes=True)
        if not files:
            continue
        for f in files:
            retval[f['name']] = f['mtime']
    return retval


def _ls_file_s3(s3_info, local_path):
    local_path = get_s3_location(local_path)

    k = s3_info.bucket.get_key(s3_info.key_prefix + local_path)
    if k is None:
        return None
    # Getting an individual key returns a different timestamp format than
    # listing the keys... Thanks aws...
    return int(iso8601_to_timestamp(k.last_modified))

def _ls_match_remote(match_type, local_dirname, file_pat, \
        deployment_id=None, force_remote=None, dir_mtimes=False):
    '''
    match_type should be re or glob.
    '''

    # Normalize dirname
    local_dirname = local_dirname.rstrip('/')

    remote_type = _get_remote_type(local_dirname, force_remote=force_remote)

    if remote_type == 'local':
        files = _ls_local(local_dirname, dir_mtimes=dir_mtimes)
    elif remote_type == 's3':
        s3_info = s3_get_info(deployment_id)
        if s3_info is None:
            return None
        else:
            files = _ls_s3(s3_info, local_dirname, dir_mtimes=dir_mtimes)
    elif remote_type == 'store':
        raise NotImplementedError("HTTP host store no longer implemented!")
    elif remote_type == 'hdfs':
        client = get_hdfs_client()
        try:
            _files = list(client.ls([local_dirname]))
        except se.FileNotFoundException:
            _files = []
        # add this so we don't change previous
        # behavior below (prep
        def _rm_root(x):
            if x.startswith(local_dirname):
                x = x[len(local_dirname + '/'):]
            return x
        files = [_rm_root(x['path']) for x in _files]

    matches = []
    for f in files:
        filename = f
        if isinstance(f, dict):
            filename = f['name']
        if match_type == 're' and re.search(file_pat, filename):
            match = True
        elif match_type == 'glob' and fnmatch.fnmatchcase(filename, file_pat):
            match = True
        else:
            match = False
        if match:
            if len(local_dirname) > 0:
                # Add separator.
                filename = local_dirname + '/' + filename
            if isinstance(f, dict):
                f['name'] = filename
                matches.append(f)
            else:
                matches.append(filename)
    return matches

def _ls_local(local_dirname, dir_mtimes=False):
    if local_dirname == '':
        # listdir() doesn't like empty string.
        local_dirname = '.'
    retval = []
    try:
        retval = os.listdir(local_dirname)
        if dir_mtimes:
            filenames = retval
            retval = []
            for fname in filenames:
                mtime = int(os.stat(local_dirname + '/' + fname).st_mtime)
                retval.append({'name': fname, 'mtime': mtime})
    except OSError:
        pass

    return retval

def _ls_s3(s3_info, local_dirname, dir_mtimes=False):
    dirname = _path_normalize(local_dirname)
    if dirname.startswith(deepy.cfg.cache_dir + '/'):
        dirname = dirname[len(deepy.cfg.cache_dir + '/'):]
    # s3 list needs dirname to end with slash, unless you're trying to ls
    # the root, which needs the empty string.
    if len(dirname) > 0 and dirname[-1] != '/':
        dirname += '/'
    # Also, keys don't start with slash.
    if len(dirname) > 0 and dirname[0] == '/':
        dirname = dirname[1:]
    dirname = s3_info.key_prefix + dirname
    results = s3_info.bucket.list(prefix=dirname, delimiter='/')
    # Strip prefix, it's added by ls_re_remote().
    results = [(r.name[len(dirname):],
               getattr(r, 'last_modified', aws_timestr_unknown))
                  for r in results]

    if dir_mtimes:
        results = [{'name': name, 'mtime': aws_timestr_to_timestamp(aws_mtime)}
                   for name, aws_mtime in results]
    else:
        results = [name for name, last_modified in results]

    return results

def http_time(ts):
    # RFC 1123 time
    return time.strftime("%a, %d %b %Y %H:%M:%S GMT", time.gmtime(ts))

# tornado seems more picky about this with static dirs.
def _path_join(*args):
    path = ''
    for p in args:
        if p[0] == '/':
            path += p
        else:
            path += '/' + p
    return path

def _path_normalize(path):
    # Python's os.path.normpath() leaves leading '//' for some reason (but
    # changes '///' into '/' ?!)
    return re.sub('^//', '/', os.path.normpath(path))

# List of allowed remote types.
remote_choices = ['s3', 'store', 'local', 'hdfs']

# Testing for _get_remote_type. Add as decorator.
def test_log(fn):
    def new_fn(*args, **kwargs):
        rv = fn(*args, **kwargs)
        if rv != 'local':
            log.info('fn={} args={} kwargs={} ret={}'.format(fn.__name__,
                args, kwargs, rv))
        return rv
    return new_fn

#@test_log
def _get_remote_type(filename, force_remote=None):
    '''
    Five ways of setting the remote:
    1. If deepy.cfg.force_remote is set, use that. This is should only be
       set for dev work, via command line.
    2. If force_remote paramater is set, use that. This is for overrides of
       slice within code. ***DEPRECATED***
    3. If filename isn't in deepy.cfg.cache_dir, use local. Generally only
       things in cache are under purview of deepy.store, but this allows
       loading local files. May eventually fold this into the directory
       override stuff.
    4. If override_store is set. This is for per directory overrides, we now
       use this so we don't have to have force_remote=whatever sprinkled
       throughout the code
    5. Check slice. Slice can be set either within the global or vm specific.
       vm specific is currently only used to prevent the store host from
       contacting it's own UI for files.
    These are in order of decreasing precedence.

    '''
    if deepy.cfg.force_remote is not None:
        return deepy.cfg.force_remote

    if force_remote in remote_choices:
        return force_remote

    if not filename.startswith(deepy.cfg.cache_dir):
        return 'local'

    if deepy.cfg.override_store and filename:
        dirs = deepy.cfg.override_directories
        if any(directory in filename for directory in dirs):
            return deepy.cfg.override_store

    def check_slice_section(section_dict):
        if section_dict is None:
            return None

        if section_dict.get('store_type'):
            return section_dict.get('store_type')

        if section_dict.get('store_local') == True:
            return 'local'
        elif 'store_host' in section_dict:
            raise NotImplementedError("HTTP host store no longer implemented!")
        elif section_dict.get('sync_data_in_s3') == False:
            # sync_data_in_s3 is deprecated. Use store_local.
            return 'local'

    vm_remote_type = check_slice_section(deepy.cfg.vm_config)
    slice_remote_type = check_slice_section(deepy.cfg.slice_config)

    if vm_remote_type is not None:
        return vm_remote_type
    elif slice_remote_type is not None:
        return slice_remote_type

    return 's3'

def _get_remote_types(filenames, force_remote=None):
    '''
    Return a list of remote_types, i.e.
    ['s3', 's3', 'local', 's3', 'store', 'hdfs']
    '''
    return [_get_remote_type(filename, force_remote=force_remote) \
        for filename in filenames]

def deployment_sync(only_config=False, force_flag=False, verbose_flag=False, dry_run=False):

    # create tmp so you can just use deployment_sync to create deployment
    # required later by deepy.store
    if not os.path.exists(deepy.cfg.data_tmp):
        os.makedirs(deepy.cfg.data_tmp)

    files = deepy.cfg.get_config_files(only_config)
    if files is None:
        deepy.log.error('get_config_files-returned-none')
        files = []

    # Dimensions 
    # CI-963 make sure to get dimensions that might not yet be in dimensions_idx
    dimension_frag_files = deepy.store.ls_glob_remote(deepy.cfg.dimensions_dir, '*.json.gz')
    if dimension_frag_files is None:
        deepy.log.error('remote-dimension-ls-returned-none')
        dimension_frag_files = []
    else:
        # Prune local files not in Store
        base_names = set(map(os.path.basename, dimension_frag_files))
        local_names = deepy.store.ls_glob_remote(deepy.cfg.dimensions_dir, '*.json.gz', force_remote='local')
        for local_name in local_names:
            if os.path.basename(local_name) not in base_names:
                deepy.log.info("deploymen-syn-prune-localfile-not-in-store %s" % local_name)
                os.remove( local_name)
        
    files = list(set(files + dimension_frag_files))
    files = filter(lambda x: 'timestamp.json.gz' not in x, files)

    # Contexts
    context_files = deepy.store.ls_glob_remote(deepy.cfg.context_dir, '*.json')
    if context_files:
        files += context_files

        # Prune local files not in Store
        base_names = set(map(os.path.basename, context_files))
        local_names = deepy.store.ls_glob_remote(deepy.cfg.context_dir, '*.json.gz', force_remote='local')
        for local_name in local_names:
            if os.path.basename(local_name) not in base_names:
                deepy.log.info("deploymen-syn-prune-localfile-not-in-store %s" % local_name)
                os.remove( local_name)
                

    if only_config and not force_flag:
        last_sync = 0
        try:
            last_sync = \
                 os.stat(deepy.cfg.config_local_last_update_file).st_mtime
        except OSError, e:
            if e.errno != errno.ENOENT:
                print 'Error stat()ing last_update file', e

        last_commit = deepy.store.ls_glob_simple(deepy.cfg.config_last_update_file, dir_mtimes=True)
        try:
            if last_commit[0]['mtime'] < last_sync:
                if verbose_flag:
                    print 'Config cache up to date'
                return
        except (KeyError, IndexError):
            pass

    if not dry_run:
        if force_flag:
            deepy.store.cache_load_from_remote(files, verbose=verbose_flag, cache_age=0)
        else:
            deepy.store.cache_load_from_remote(files, verbose=verbose_flag)

    config_dir = os.path.dirname(deepy.cfg.config_local_last_update_file)
    if not os.path.exists(config_dir):
        os.makedirs(config_dir)
    fp = open(deepy.cfg.config_local_last_update_file, 'w+')
    fp.write(json.dumps({"mtime": time.time()}))
    fp.close()

    return files

def deployment_pull(shadow_deployment_id, only_config=False,
                    extra_slice_key_del=None):
    '''
    Formerly just a stand-alone script, this copies configuration from a
    shadow's master (as configured in the shadow slice) to either bootstrap or
    update a shadow with data from the master.

    Puts a copy of the master's config in a shadow subfolder at the top-level.

    Other relevant slice config settings:
        deployment_master.deployment_pull: "partial"
            Does a "partial" deployment pull, used for the demo box, does not
            pull:
                slice: build_updates
                slice: connectors
                the files enumerated in partial_skip below (e.g.
                    tile_config.json)
                dimensions (this may need to be revisited)
        deployment_master:skip_pull_files (list)
            Does not pull if os.path.basename(shadow_fname) in skip_pull_files
    '''

    # XXX move some subdirs (eg shadow) into global constants.
    # XXX add a flag to remove files that don't exist on master

    if extra_slice_key_del is None:
        extra_slice_key_del = []

    orig_deployment_id = deepy.cfg.deployment_id
    if shadow_deployment_id == None:
        orig_deployment_id = None

    deepy.cfg.init(shadow_deployment_id)
    shadow_slice_json = deepy.cfg.slice_config

    master_id = shadow_slice_json.get('deployment_master',
            {}).get('deployment_id')
    pull_option = shadow_slice_json.get('deployment_master',
            {}).get('deployment_pull')
    skip_pull_files = shadow_slice_json.get('deployment_master',
            {}).get('skip_pull_files', [])
    skip_pull_files.append('deployment.json')
    skip_pull_files.append('dimensions_db.json')
    skip_pull_files.append('dimensions_db_small.json')
    skip_pull_files.append('dimensions_idx.json')

    if master_id is None:
        deepy.log.error('no-master-deployment-to-pull-from')
        deepy.cfg.init(orig_deployment_id)
        return False

    shadow_hood = os.path.join(deepy.cfg.home_dir, "shadow")

    deepy.cfg.init(master_id, pipedream_hood_override=shadow_hood)
    shadow_cache = deepy.cfg.cache_dir
    files = deployment_sync(only_config=only_config,
            force_flag=True)

    deepy.cfg.init(shadow_deployment_id)

    # Failsafe - delete master's access keys, and some other attrs that we
    # will never use on the shadow but might break things.
    #
    # BE VERY CAREFUL HERE.  Master's keys are on disk at this point, DON'T
    # USE THEM.
    master_slice_path = os.path.join(deepy.cfg.home_dir, "shadow", master_id,
            "cache", "config", "slice.json")

    master_slice = simple_load_json(master_slice_path,
                                                force_remote='local')

    # XXX Make shadowing of VM config a little more configurable. For now,
    # just require local config..
    for always_del in ['credentials', 'license', 'deployment_master',
                       'version', 'saved_by', 'customer_id', 'vms',
                       'store_host', 'sync_data_in_s3', 'ui_processes',
                       'proxy'] + \
                      extra_slice_key_del:
        master_slice.pop(always_del, None)

    if pull_option == 'partial':
        for partial_del in ['build_updates', 'connectors']:
            master_slice.pop(partial_del, None)

    simple_save_json(master_slice, master_slice_path,
                                 force_remote='local')

    partial_keep = [
        # demo tweaks
        "tile_config.json",
        "ui.json",

        # generated on demo box
        "dimensions_idx.json",
        "dimensions_db.json.gz",
        "dimensions_db_small.json.gz",

        # anonymized
        "member.json.gz",
        "peer.json.gz",
        "as_origin.json.gz"
        ]

    # XXX Merge overrides ?
    push_files = []
    for shadow_fname in files:
        if not shadow_fname.startswith(shadow_cache):
            shadow_fname = os.path.join(shadow_cache, shadow_fname)

        if not os.path.exists(shadow_fname):
            deepy.log.debug('not-found %s' % (shadow_fname))
            continue

        fname = '%s%s' % (deepy.cfg.cache_dir, shadow_fname[len(shadow_cache):])
        if os.path.basename(shadow_fname) in skip_pull_files:
            deepy.log.debug("slice-specified-skip {}".format(shadow_fname))
            continue
        elif os.path.basename(shadow_fname) == 'slice.json':
            # XXX Merge config files w/ prod overriding
            deepy.log.debug('merge-to %s %s' % (shadow_fname, fname))
            sh_cfg = simple_load_json(shadow_fname,
                                                  force_remote='local')
            if sh_cfg is None:
                continue
            cfg = simple_load_json(fname)
            if cfg is None:
                cfg = sh_cfg
            else:
                deepy.util.update_recursive(cfg, sh_cfg, merge_lists=False)

            simple_save_json(cfg, fname)
            #print cfg
        elif pull_option == "partial" and os.path.basename(shadow_fname) in partial_keep:
            deepy.log.debug("partial-skip {}".format(shadow_fname))
            continue
        elif pull_option == "partial" and 'dimensions' in shadow_fname:
            deepy.log.debug("partial-skip {}".format(shadow_fname))
            continue
        else:
            deepy.log.debug('copy-to %s %s' % (shadow_fname, fname))
            if not os.path.exists(os.path.dirname(fname)):
                os.makedirs(os.path.dirname(fname))
            shutil.copy2(shadow_fname, fname)
            push_files.append(fname)

    if len(push_files) > 0:
        # FIXME refactor in to a s3->s3 utility function
        s3 = boto.connect_s3()
        dst_bucket = s3.get_bucket('{}.pdrlabs.net'.format(deepy.cfg.deployment_id))
        source_bucket_name = '{}.pdrlabs.net'.format(master_id)

        push_keys = map(lambda x: x[len(deepy.cfg.cache_dir + '/'):], push_files)

        deepy.log.debug('s3-to-s3-config-push')
        for aws_path in push_keys:
            deepy.log.debug('s3://{source}/{aws_path} -> s3://{dest}/{aws_path}'.format(source=source_bucket_name, dest=dst_bucket.name, aws_path=aws_path))
            dst_bucket.copy_key(aws_path, source_bucket_name, aws_path)

    deepy.cfg.init(orig_deployment_id)
    return True

def _install (key, aws_path, deployment_id):
    print "Copy ", aws_path, deployment_id + ".pdrlabs.net"

    if deployment_id == 'map':
        return

    try:
       key.copy(deployment_id + ".pdrlabs.net", aws_path)
    except:
        return

def install_each_deployment (aws_path, deployment_id=None):
    s3 = boto.connect_s3()
    key = s3.get_bucket('map.pdrlabs.net').get_key(aws_path)

    if not key:
        print "install_each_deployment could not get key %s://%s" % ('map.pdrlabs.net', aws_path)
        return

    if deployment_id:
        _install (key, aws_path, deployment_id)
    else:
        # Copy to each deployment bucket
        ids = deepy.deploy.get_cached_deployment_ids()
        ids.append("subscription")
        for id in ids:
            _install (key, aws_path, id)

def get_hdfs_client():
    return AutoConfigClient()

#Sanity Checks for hdfs operations. Not intended to be run in the testing framework
def setUp():
    #Create a temp file and add it to hdfs
    tmpFileHandler = tempfile.NamedTemporaryFile(delete=False)
    tmpFile = tmpFileHandler.name
    tmpFileHandler.write('{"test":1}')
    return tmpFile

def cleanup(tmpFile, testFile):
    try:
        os.remove(tmpFile)
    except:
        pass
    try:
        os.remove(testFile)
    except:
        pass

    try:
        rm_files([testFile], force_remote='hdfs')
    except:
        pass

    try:
        rm_files([tmpFile], force_remote='hdfs')
    except:
        pass


def test_hdfs_get():
    tmpFile = setUp()
    test_fname = "{}.get_test".format(tmpFile)
    shutil.copy(tmpFile, test_fname)

    cache_save_to_remote(test_fname, force_remote='hdfs')
    os.remove(test_fname)
    assert set(simple_load_json(test_fname, force_remote='hdfs').keys()) == set(json.load(open(tmpFile)).keys())
    os.remove(test_fname)
    cleanup(test_fname, tmpFile)

def test_hdfs_put():
    tmpFile = setUp()
    test_fname = "{}.put_test".format(tmpFile)
    shutil.copy(tmpFile, test_fname)
    cache_save_to_remote(test_fname, force_remote='hdfs')
    os.remove(test_fname)
    out = simple_load_json(test_fname, force_remote='hdfs')
    assert set(out.keys()) ==  set(json.load(open(tmpFile)).keys())
    cleanup(tmpFile, test_fname)

def test_hdfs_del():
    tmpFile = setUp()
    test_fname = "{}.del_test".format(tmpFile)
    shutil.copy(tmpFile, test_fname)
    cache_save_to_remote(test_fname, force_remote='hdfs')
    os.remove(test_fname)
    assert simple_load_json(test_fname, force_remote='hdfs')
    os.remove(test_fname)
    rm_files([test_fname], force_remote='hdfs')
    assert not simple_load_json(test_fname, force_remote='hdfs')
    cleanup(tmpFile, test_fname)

def test_hdfs_ls():
    tmpFile = setUp()
    test_fname = "{}.ls_test".format(tmpFile)
    shutil.copy(tmpFile, test_fname)
    cache_save_to_remote(test_fname, force_remote='hdfs')
    os.remove(test_fname)
    assert ls_files_remote([test_fname], force_remote='hdfs')
    cleanup(tmpFile, test_fname)

def save_mysql_table(table_name, dry_run=False):
    
    table_file = os.path.join(deepy.cfg.mysql_dir, '{}.%Y-%m-%d-%H.sql.gz'.format(table_name))

    #generate the right timestamp
    table_file = deepy.timerange.substitute_timestamp(table_file, datetime.datetime.utcnow())

    if not dry_run:
        if not os.path.exists(deepy.cfg.mysql_dir):
            os.makedirs(deepy.cfg.mysql_dir)
        log.debug("Dumping table {}".format(table_name))
        subprocess.call(['mysqldump -u {} -p{} --add-locks {} | gzip > {}'.format(deepy.cfg.db_database_user,
                                                                                  deepy.cfg.db_database_pass, 
                                                                                  table_name, table_file)] , shell=True)

        # save dump to bucket
        log.debug("Saving {} to remote".format(table_file))
        cache_save_to_remote(table_file)
    return table_file
