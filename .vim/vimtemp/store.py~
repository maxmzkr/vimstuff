#!/usr/bin/env python
__test__=False

import os
import sys
import subprocess
import calendar
import urllib
import urllib2
import requests
import re
import datetime
import errno
import stat
import tempfile
import gzip
import fnmatch
import time
import collections
import shutil
import arrow
import random
import json

import deepy.map
import deepy.cfg
import deepy.log as log
import deepy.timerange
import deepy.util
import deepy.deploy

import boto
import fabric.api
import fabric.contrib.files

import snakebite.errors as se
from snakebite.client import AutoConfigClient

try:
    import ujson
except ImportError:
    import json as ujson

# Cache bucket to speed up s3 retrieval
s3_bucket_cache_name = None
s3_bucket_cache = None

# Cache to require less s3 retrieval
# last_update_cache[filename] --> timedelta of last update time
last_update_cache = {}

S3Info = collections.namedtuple('S3Info', 'bucket, key_prefix')

def s3_get_info(deployment_id=None):
    retval = None
    bucket = s3_get_bucket(deployment_id)
    key_prefix = s3_get_key_prefix(deployment_id)
    if bucket is not None and key_prefix is not None:
        retval = S3Info(bucket, key_prefix)
    return retval

def s3_get_key_prefix(deployment_id=None):
    retval = ''
    if deployment_id is None:
        deployment_id = deepy.cfg.deployment_id
        if deployment_id is None:
            log.warn('could-not-determine-deployment')
            return None
    slice_data = deepy.cfg.slice_config
    if slice_data.get('shared_bucket', None) is not None:
        retval = 'd/%s.pdrlabs.net/' % deployment_id
    return retval


# Connect to s3 and get bucket.
# deployment_id is used to select the bucket name. If not provided,
# it's autoset from cfg (command line or slice).
# Returns bucket or None.
def s3_get_bucket(deployment_id=None):
    global s3_bucket_cache
    global s3_bucket_cache_name

    if deepy.cfg.isolated_deployment:
        return None

    if deployment_id is None:
        deployment_id = deepy.cfg.deployment_id
        if deployment_id is None:
            log.warn('could-not-determine-deployment')
            return None

    # Use cache
    if s3_bucket_cache_name == deployment_id and s3_bucket_cache != None:
        return s3_bucket_cache

    # Pull settings from slice, if it's there.
    slice_data = deepy.cfg.slice_config

    if 'credentials' in slice_data:
        access_id = slice_data['credentials'][0]
        access_secret = slice_data['credentials'][1]
    else:
        access_id = None
        access_secret = None

    try:
        # boto.cfg or env vars.
        s3 = boto.connect_s3()
    except:
        # Normal deployment case
        try:
            s3 = boto.connect_s3(access_id, access_secret)
        except:
            log.warn('s3-connect-failed')
            return None

    # XXX Need to bootstrap and search for correct bucket if slice is
    # not available.

    bucket_id = deployment_id
    if slice_data.get('shared_bucket', None) is not None:
        bucket_id = slice_data['shared_bucket']

    bucket_path = '%s.pdrlabs.net' % bucket_id
    try:
        bucket = s3.get_bucket(bucket_path)
    except Exception as e:
        # This used to be warn, but not finding a bucket is actually not
        # exceptional behavior at this point -- we might be checking that the
        # bucket indeed does not exist. The caller should warn if not finding
        # the bucket is a problem.
        log.debug(e)
        log.debug('get-bucket-failed (s3_get_bucket) %s' % (bucket_path))
        return None

    # Cache!
    s3_bucket_cache_name = deployment_id
    s3_bucket_cache = bucket

    return bucket

aws_timestr_unknown = time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.localtime(0))

#Getting an individual key returns a different timestr...
RFC1123 = '%a, %d %b %Y %H:%M:%S %Z'
def iso8601_to_timestamp(iso8601_time):
    SUBSECOND_RE = re.compile('\.[0-9]+')
    iso8601_time = SUBSECOND_RE.sub('', iso8601_time)
    try:
        return time.mktime(time.strptime(iso8601_time, boto.utils.ISO8601))
    except ValueError:
        return time.mktime(time.strptime(iso8601_time, RFC1123))

#Convert fnames for flow/h5flow to their s3 friendly equivalent (needed because normal conversion assumes they are inside of cache)
#Can take a list or a single path
def local_flow_path_to_s3(local_path):
    remote_path = local_path
    if deepy.cfg.hood_dir in local_path:
        remote_path = local_path.replace(deepy.cfg.hood_dir, "")
    return remote_path

# Keep this in one place so if AWS changes the format it will be easy for us
# to update.
def aws_timestr_to_timestamp(aws_timestr):
    # Ex: 2012-07-26T19:13:54.000Z
    return int(calendar.timegm(time.strptime(aws_timestr[:-5],
                           '%Y-%m-%dT%H:%M:%S')))

# Generate a for a given remote store dir, generate a full listing and store it
# in the directory.
#
# Might be nice to integrate with the rest of our remote_ls stuff
def generate_dir_list(local_dirname, local_path=None, deployment_id=None,
                     force_remote=None, verbose=False, dir_mtimes=False):

    dir_list = ls_dir_remote(local_dirname, deployment_id=deployment_id,
                             force_remote=force_remote, dir_mtimes=dir_mtimes)
    if dir_mtimes:
        dir_list = [{'name': os.path.basename(n['name']), 'mtime':n['mtime']} for n in dir_list]
    else:
        dir_list = [os.path.basename(n) for n in dir_list]

    obj = { 'dir_list': dir_list }
    if local_path is None:
        if local_dirname[len(local_dirname)-1] != '/':
            local_dirname += '/'
        local_path = local_dirname + 'dir_list.json.gz'

    log.info("saving-dir-list %s" % (local_path))
    simple_save_json(obj, local_path, deployment_id=deployment_id,
                     force_remote=force_remote, verbose=verbose)


def rm_files(local_paths, deployment_id=None, force_remote=None):
    '''
    Remove files from disk and from remote. Returns a dict of boolean tuples
    (local, remote), True if something was removed, False on error, None if not
    applicable.

    (True, False): Indicates local removal but remote removal failed
    (True, None): Common result with remote==local
    (True, None): File removed remotely but didn't exist on remote
    (False, True): File removed remotely but local rm failed
    '''
    # Prevent confusion, only accept list.
    assert isinstance(local_paths, list)

    remote_types = _get_remote_types(local_paths, force_remote=force_remote)

    results = {}
    for remote_type, local_path in zip(remote_types, local_paths):
        res_local = None
        res_remote = None

        if os.path.exists(local_path):
            res_local = False
            try:
                os.remove(local_path)
                res_local = True
            except OSError:
                log.exception('removing-local-file {}'.format(local_path))

        if remote_type == 's3':
            res_remote = False
            s3_info = s3_get_info(deployment_id)
            if s3_info is not None:
                mtime = _ls_file_s3(s3_info, local_path)
                if mtime is None:
                    res_remote = None
                else:
                    res_remote = _rm_file_s3(s3_info, local_path)
        elif remote_type == 'store':
            store_path = local_path.replace(deepy.cfg.cache_dir, "")
            _store_del(store_path)
            return
        elif remote_type == 'hdfs':
            _rm_file_hdfs(local_path)

        results[local_path] = (res_local, res_remote)

    return results

def _rm_file_hdfs(local_path):
    client = get_hdfs_client()
    dname = os.path.dirname(local_path)
    list(client.mkdir([dname], create_parent=True)) # generator returned
    try:
        _hdfs_rm(local_path)
    except IOError:
        log.warn('hdfs-rm-failed')


def _rm_file_s3(s3_info, local_path):
    # Cleanup path for s3. Access functions will do the same.
    local_path = _path_normalize(local_path)
    if local_path.startswith(deepy.cfg.cache_dir + '/'):
        local_path = local_path[len(deepy.cfg.cache_dir + '/'):]
    # Also, keys don't start with slash.
    if len(local_path) > 0 and local_path[0] == '/':
        local_path = local_path[1:]

    k = s3_info.bucket.get_key(s3_info.key_prefix + local_path)
    if k is None:
        return None

    k.delete()
    return True

def ls_files_remote(local_paths, deployment_id=None, force_remote=None, fallback_from=None):
    '''
    Check for specific files rather than glob. Return dict of files that
    exist and their mtimes.
    '''

    # Prevent confusion, only accept list.
    assert isinstance(local_paths, list)

    remote_types = _get_remote_types(local_paths, force_remote=force_remote, fallback_from=fallback_from)
    # Maybe change this to do all of a particular remote_type at once.
    # Probably doesn't matter if each has to be individually queried.
    results = {}
    for remote_type, local_path in zip(remote_types, local_paths):
        if remote_type == 'local':
            try:
                mtime = int(os.stat(local_path).st_mtime)
            except OSError:
                mtime = None
        elif remote_type == 's3':
            s3_info = s3_get_info(deployment_id)
            if s3_info is None:
                mtime = None
            else:
                mtime = _ls_file_s3(s3_info, local_path)
        elif remote_type == 'store':
            mtime = _ls_file_store(local_path)
        elif remote_type == 'hdfs':
            start = time.time()
            client = get_hdfs_client()
            mtime = None
            try:
                filenames = list(client.ls([local_path]))
                if filenames:
                    mtime = filenames[0]['modification_time']
                log.debug('hdfs-ls seconds=%f,file=%s' % (time.time() - start, local_path))
            except se.FileNotFoundException:
                log.debug('hdfs-ls-not-found-for-%s' % (local_path))
                log.debug('falling-back-to-secondary-cache')
                mtime = ls_files_remote([local_path], deployment_id=deployment_id, fallback_from='hdfs')
                #recursive call will return {} instead of None
                if not mtime:
                    mtime = None

        # None signifies file doesn't exist.
        if mtime is not None:
            results[local_path] = mtime

    return results

# ls a directory
def ls_dir_remote(local_dirname, deployment_id=None, force_remote=None,
                  dir_mtimes=False):
    return ls_glob_remote(local_dirname, '*', deployment_id, force_remote,
                          dir_mtimes=dir_mtimes)

# Does the path split for you.
def ls_glob_simple(local_pat,
        deployment_id=None, force_remote=None, dir_mtimes=False):
    head, tail = os.path.split(local_pat)
    return ls_glob_remote(head, tail, deployment_id, force_remote, dir_mtimes)

# ls with glob matching
def ls_glob_remote(local_dirname, file_pat,
        deployment_id=None, force_remote=None, dir_mtimes=False):
    return _ls_match_remote('glob', local_dirname, file_pat,
        deployment_id, force_remote, dir_mtimes=dir_mtimes)

# ls with regexp matching
def ls_re_remote(local_dirname, file_pat,
        deployment_id=None, force_remote=None, dir_mtimes=False):
    return _ls_match_remote('re', local_dirname, file_pat,
        deployment_id, force_remote, dir_mtimes=dir_mtimes)

# Return most recent file, or None.
def ls_re_remote_last(local_dirname, file_pat,
        deployment_id=None, force_remote=None, dir_mtimes=False):
    files = ls_re_remote_tail(local_dirname, file_pat, 1,
            deployment_id, force_remote, dir_mtimes=dir_mtimes)
    if len(files) == 1:
        return files[0]
    return None


def ls_glob_remote_last(local_dirname, file_pat,
        deployment_id=None, force_remote=None, dir_mtimes=False):
    files = ls_glob_remote_tail(local_dirname, file_pat, 1,
            deployment_id, force_remote, dir_mtimes=dir_mtimes)
    if len(files) == 1:
        return files[0]
    return None

# Sorts the matching files, and returns the last n.
# Since our files are named sequentially by time, this lets you get the
# n most recent files.
def ls_re_remote_tail(local_dirname, file_pat, n,
        deployment_id=None, force_remote=None, dir_mtimes=False):
    files = deepy.store.ls_re_remote(local_dirname, file_pat,
            deployment_id, force_remote, dir_mtimes=dir_mtimes)
    files.sort()
    return files[-n:]

def ls_glob_remote_tail(local_dirname, file_pat, n,
        deployment_id=None, force_remote=None, dir_mtimes=False):
    files = deepy.store.ls_glob_remote(local_dirname, file_pat,
            deployment_id, force_remote, dir_mtimes=dir_mtimes)
    files.sort()
    return files[-n:]

def is_file_fresh(local_path, cache_age):
    '''
    Check if a file requires pulling from s3
    '''
    global last_update_cache

    if not os.path.isfile(local_path):
        # Local file not on disk, use s3
        return False

    if local_path not in last_update_cache:
        # Local file not in cache yet, update it
        update_cache(local_path)
        return False

    current_time = datetime.datetime.now()
    last_update = last_update_cache[local_path]
    last_modified_time = os.path.getmtime(local_path)
    last_modified_time = datetime.datetime.fromtimestamp(last_modified_time)

    if last_modified_time > last_update:
        # Another process has updated the file more recently
        # than this one has, update time
        update_cache(local_path)

    if (current_time - last_update) > cache_age:
        # Local file is out-of-date, update it
        update_cache(local_path)
        return False

    return True

def update_cache(local_path):
    '''
    Update cache of when file was last written to
    '''
    global last_update_cache
    last_update_cache[local_path] = datetime.datetime.now()

def clear_cache():
    '''
    Clear cache of when files were last written to
    '''
    global last_update_cache
    last_update_cache = {}

def make_list(param):
    '''
    Make single values iterable
    '''
    return param if isinstance(param, list) else [param]

def cache_load_from_remote(local_paths, deployment_id=None, force_remote=None, \
        verbose=False, cache_age=30, fallback_from=None):
    '''
    Bring a remote file to local cache.
    '''

    cache_age = datetime.timedelta(minutes=cache_age)

    # Accept a list or single item
    local_paths = make_list(local_paths)

    remote_types = _get_remote_types(local_paths, force_remote, fallback_from=fallback_from)
    for remote_type, local_path in zip(remote_types, local_paths):

        # XXX Temporary check to help catch code that isn't fully specifying
        # path. (Some old code depended on cache_load_from_remote
        # automatically prepending cache_dir.)
        if not local_path.startswith(deepy.cfg.home_dir):
            log.debug('possible-bad-path {}'.format(local_path))

        if remote_type == 'local' or is_file_fresh(local_path, cache_age):
            continue
        elif remote_type == 's3':
            s3_info = s3_get_info(deployment_id)
            if s3_info:
                _cache_load_from_s3(s3_info, local_path)
        elif remote_type == 'store':
            _cache_load_from_store(local_path)
        elif remote_type == 'hdfs':
            if not _cache_load_from_hdfs(local_path):

                if cache_age:
                    try:
                         cache_age = int(cache_age.seconds/60)
                    except AttributeError:
                         #Cache age was passed in as an int and not converted to timedelta
                         #Leave it as an int
                         pass

                cache_load_from_remote([local_path], deployment_id=deployment_id, cache_age=cache_age, verbose=verbose,\
                        fallback_from='hdfs')

    for local_path in local_paths:
        permission = stat.S_IWUSR | stat.S_IRUSR | stat.S_IROTH | stat.S_IWOTH
        try:
            os.chmod(local_path, permission)
        except OSError:
            pass

def cache_save_to_remote(local_paths, deployment_id=None, force_remote=None, \
        verbose=False, s3replace=True, audit_string=None, ignore_md5=False, fallback_from=None):
    '''
    Save a cached file to remote storage.
    '''
    # Accept a list or single item
    local_paths = make_list(local_paths)

    if audit_string:
        deepy.log.log_this_call("{}: {}".format(audit_string, ",".join(local_paths)))

    remote_types = _get_remote_types(local_paths, force_remote, fallback_from=fallback_from)
    for remote_type, local_path in zip(remote_types, local_paths):
        if remote_type == 'local':
            continue
        elif remote_type == 's3':
            s3_info = s3_get_info(deployment_id)
            if s3_info:
                _cache_save_to_s3(s3_info, local_path, replace=s3replace, ignore_md5=ignore_md5)
            else:
                log.warn('s3-info-failed {}'.format(local_path))
        elif remote_type == 'store':
            _cache_save_to_store(local_path)
        elif remote_type == 'hdfs':
            if not _hdfs_save_to_remote(local_path):
                cache_save_to_remote([local_path], deployment_id=deployment_id, verbose=verbose,\
                        s3replace=s3replace,fallback_from='hdfs', ignore_md5=ignore_md5)

# Load json file.
# Can generally leave deployment_id and force_remote as None.
def simple_load_json(local_path, deployment_id=None, force_remote=None,
                     object_pairs_hook=dict, cache_age=30):

    cache_load_from_remote([local_path], deployment_id, force_remote, cache_age=cache_age)

    if not os.path.isfile(local_path):
        log.debug('json-file-not-found %s' % (local_path))
        return None
    try:

        if local_path.endswith(".gz"):
            fp = gzip.open(local_path)
        else:
            fp = open(local_path)

        success=False
        try:
            if object_pairs_hook == dict:
                j = ujson.load(fp)
                success=True
        except TypeError:
            log.exception('ujson-load-failed')
            pass

        if not success:
            j = json.load(fp, object_pairs_hook=object_pairs_hook)

    except:
        log.exception('load-json-failed')
        return None

    return j

# Save json to cache and remote.
# obj can either be a string (already encoded) or a dict/list.
# Can generally leave deployment_id and force_remote as None.
def simple_save_json(obj, local_path, deployment_id=None, force_remote=None, verbose=False, s3replace=True, audit_string=None, ignore_md5=False):
    if audit_string:
        deepy.log.log_this_call("{}: {}".format(audit_string, local_path))

    if not os.path.exists(deepy.cfg.data_tmp):
        os.makedirs(deepy.cfg.data_tmp)

    if not os.path.exists(os.path.dirname(local_path)):
        os.makedirs(os.path.dirname(local_path))

    # Save to cache
    tmp_file = tempfile.NamedTemporaryFile(delete=False, dir=deepy.cfg.data_tmp)
    tmp_name = tmp_file.name
    tmp_file.close()

    if local_path.endswith(".gz"):
        fn = gzip.open
    else:
        fn = open

    with fn(tmp_name, 'w') as fd:
        json.dump(obj, fd, default=deepy.util.default_json_encoder, indent=2)

    try:
        os.rename(tmp_name, local_path)
    except OSError:
        log.exception("rename-failed %s %s" % (tmp_name, local_path))
        return
    try:
        permissions = stat.S_IWUSR | stat.S_IRUSR | stat.S_IROTH | stat.S_IWOTH
        os.chmod(local_path, permissions)
    except OSError:
        log.exception("chmod-failed %s" % (local_path))
        return

    cache_save_to_remote(local_path, deployment_id, force_remote,
                         verbose=verbose, s3replace=s3replace, ignore_md5=ignore_md5)

def simple_save_content(content, local_path, deployment_id=None, force_remote=None, verbose=False, s3replace=True, audit_string=None):
    if audit_string:
        deepy.log.log_this_call("{}: {}".format(audit_string, local_path))

    if not os.path.exists(deepy.cfg.data_tmp):
        os.makedirs(deepy.cfg.data_tmp)

    if not os.path.exists(os.path.dirname(local_path)):
        os.makedirs(os.path.dirname(local_path))

    # Save to cache
    tmp_file = tempfile.NamedTemporaryFile(delete=False, dir=deepy.cfg.data_tmp)
    tmp_name = tmp_file.name
    tmp_file.close()

    if local_path.endswith(".gz"):
        fn = gzip.open
    else:
        fn = open

    with fn(tmp_name, 'w') as fd:
        fd.write(content)

    try:
        os.rename(tmp_name, local_path)
    except OSError:
        log.exception("rename-failed %s %s" % (tmp_name, local_path))
        return
    try:
        permissions = stat.S_IWUSR | stat.S_IRUSR | stat.S_IROTH | stat.S_IWOTH
        os.chmod(local_path, permissions)
    except OSError:
        log.exception("chmod-failed %s" % (local_path))
        return

    cache_save_to_remote(local_path, deployment_id, force_remote,
                         verbose=verbose, s3replace=s3replace)
###############################
#
# Internal functions. Do not access directly.
#
def _hdfs_rm(local_path):
    start = time.time()
    client = get_hdfs_client()
    try:
        list(client.delete([local_path], recurse=True))
    except se.FileNotFoundException:
        log.info("hdfs-rm-failed-for-%s" % (local_path))
        raise IOError
    log.info('hdfs-delete seconds=%f,file=%s' % (time.time() - start, local_path))

def _hdfs_put(local_path):
    start = time.time()
    #Test with support
    cmd = "HADOOP_USER_NAME=hdfs hdfs dfs -put -f {} {}".format(local_path,local_path)
    try:
        subprocess.check_output([cmd], shell=True)
    except subprocess.CalledProcessError:
        log.debug("put-failed-making-sure-directories-are-made")
        try:
            #Make directory every time w/ snakebite
            mkdir_cmd = "HADOOP_USER_NAME=hdfs hdfs dfs -mkdir -p {}".format(os.path.dirname(local_path))
            subprocess.check_output([mkdir_cmd], shell=True)
            subprocess.check_output([cmd], shell=True)
        except subprocess.CalledProcessError as e:
            log.debug(e.output)
            log.debug(cmd)
            log.warn("hdfs-put-failed-for-%s" % (local_path))
            raise IOError
    log.info('hdfs-put seconds=%f,file=%s' % (time.time() - start, local_path))

def _hdfs_save_to_remote(local_path):
    client = get_hdfs_client()
    dname = os.path.dirname(local_path)
    list(client.mkdir([dname], create_parent=True)) # generator returned
    try:
        _hdfs_put(local_path)
        return True
    except IOError:
        log.warn('hdfs-put-failed {}'.format(local_path))
        return False

def _cache_save_to_s3(s3_info, local_path, replace=True, ignore_md5=False):
    key_path = s3_info.key_prefix + local_path[len(deepy.cfg.cache_dir + '/'):]
    key = s3_info.bucket.get_key(key_path)

    if not ignore_md5:
        try:
            fp = open(local_path)
            md5_local = key.compute_md5(fp)[0]
            md5_key = key.etag.strip("\"")
            fp.close()

            if md5_local == md5_key:
                log.debug('md5-matches %s %s' % (local_path, key_path))
                return
        except:
            pass

    if key is None:
        key = s3_info.bucket.new_key(key_path)

    key.set_contents_from_filename(local_path, replace=replace)

    # sometimes a race condition?
    try:
        key.set_acl('private')
    except:
        pass

    log.debug("Wrote to S3 %s  -> s3://%s/%s (%2.1f Kbytes)" % (local_path, s3_info.bucket.name, key_path, key.size / 1000))

def _configure_fabric(host=None):
    '''Configure fabric for ssh_store support'''
    fe = fabric.api.env
    if host:
        fe.host_string = host
    else:
        fe.host_string = deepy.util.vm_or_slice_config_get('store_host')

    # These are hard-coded because of the salt state
    fe.user = 'support'
    fe.key_filename = ['/home/support/.ssh/id_rsa']

    # Probably unnecessary
    fe.password = ''
    return fe

def _hdfs_store_tmp_path():
    '''
    Namespaced temoporary file for hdfs copy
    '''
    file_temp = 'hdfs-store.%Y-%m-%d-%H-%M-%S.{}'.format(random.random())
    return os.path.join(deepy.cfg.data_tmp, deepy.timerange.substitute_timestamp(file_temp, arrow.now()))

def _ssh_store_tmp_path():
    '''
    Unfortunately this isn't perfect, but tempfile  module doesn't do exactly what I need.
    See callers for details.
    '''
    file_temp = 'ssh-store.%Y-%m-%d-%H-%M-%S.{}'.format(random.random())
    return os.path.join(deepy.cfg.data_tmp, deepy.timerange.substitute_timestamp(file_temp, arrow.now()))

def _cache_save_to_store(local_path):
    if deepy.util.vm_or_slice_config_get('ssh_store'):
        fe = _configure_fabric()
        log.info('ssh-put-to-store {}@{}:{}'.format(fe.user, fe.host_string, local_path))

        # temp_dir is not respected in this version, so we can't use this api fn
        #fabric.api.put(local_path=local_path, remote_path=local_path, use_sudo=True, temp_dir=deepy.cfg.data_tmp)

        # We need to create a unique temp file on the remote, this isn't
        # perfect but will work fine
        tmp_path = _ssh_store_tmp_path()
        args = ['scp', '-i', fe.key_filename[0], local_path,
            '{user}@{host}:{dst}'.format(user=fe.user, host=fe.host_string, dst=tmp_path)
        ]
        subprocess.call(args)
        fabric.api.sudo('mkdir -p {}'.format(os.path.dirname(local_path)))
        fabric.api.sudo('mv {} {}'.format(tmp_path, local_path))
        return

    host = deepy.util.vm_or_slice_config_get('store_host')
    cred = deepy.cfg.slice_config.get('credentials')
    if host is None or cred is None:
        log.error('store-save-error host=%s credentials=%s' % (host, cred))
        sys.exit(0)
    # Using s3 secret as key
    api_key = cred[1]

    store_path = local_path[len(deepy.cfg.cache_dir + '/'):]
    url_path = 'api/store/' + store_path
    url_path = urllib.quote(url_path)
    params = urllib.urlencode({'api_key': api_key})
    url = 'https://%s/%s?%s' % (host, url_path, params)

    headers = {
        'content-type': 'application/octet-stream',
        'content-length': os.path.getsize(local_path)
    }

    with open(local_path, 'rb') as fp:
        try:
            requests.put(url, data=fp, headers=headers)
        except Exception:
            # Catch errors?
            raise

def _hdfs_check_stale(local_path, client):
    return _hdfs_check_stale_ts(local_path, client)

def _hdfs_check_stale_ts(local_path, client):
    '''Returns None if the file does not exist
    Returns True if the file is stale
    Returns False if it is not
    '''
    try:
        stat = client.stat([local_path])
    except se.FileNotFoundException:
        log.debug('%s-doesnt-exist-not-fetching' % (local_path))
        return None

    hdfs_mtime = stat.get('modification_time')
    if not hdfs_mtime:
        return False


    if not os.path.exists(local_path):
        return True

    mtime = int(os.stat(local_path).st_mtime)*1000
    #hdfs_mtime is in msecs

    if mtime <= int(hdfs_mtime):
        return True
    else:
        return False

#XXX For now check hdfs followed by the traditional store
def _cache_load_from_hdfs(local_path):
    start = time.time()
    client = get_hdfs_client()
    dname = os.path.dirname(local_path)
    if not os.path.exists(dname):
        os.makedirs(dname)

    stale = _hdfs_check_stale(local_path, client)
    if stale is False:
        log.debug("%s-isnt-stale" % (local_path))
        return True
    elif stale is None:
        log.info("%s-does-not-exist" % (local_path))
        return False

    res = None
    #Warn on exception and keep return value check
    tmp_path = _hdfs_store_tmp_path()
    try:
        res = list(client.copyToLocal([local_path], tmp_path, check_crc=False))
    except se.FileNotFoundException:
        pass
    if not res:
        log.warn('possible-bad-hdfs-path {} {}'.format(local_path, tmp_path))
        return False
    log.info('hdfs-get seconds=%f,file=%s' % (time.time() - start, local_path))

    try:
       os.rename(tmp_path, local_path)
    except OSError:
       log.warn("rename-failed %s %s" % (tmp_path, local_path))
       return

    try:
       os.chmod(local_path, stat.S_IWUSR|stat.S_IRUSR|stat.S_IROTH|stat.S_IWOTH)
    except OSError:
       log.warn("chmod-failed %s %s" % (tmp_path, local_path))

    return True

def _cache_load_from_store(local_path):
    if not os.path.exists(os.path.dirname(local_path)):
        os.makedirs(os.path.dirname(local_path))

    # Check for local cache.
    if os.path.exists(local_path):
        ts = os.path.getmtime(local_path)
    else:
        ts = 0

    if not os.path.exists(os.path.dirname(deepy.cfg.data_tmp)):
        os.makedirs(os.path.dirname(deepy.cfg.data_tmp))

    if deepy.util.vm_or_slice_config_get('ssh_store'):
        start = time.time()
        tmp_name = _ssh_store_get(local_path, ts)
        log.debug('ssh-get seconds=%f' % (time.time() - start))
        if not tmp_name:
            return
    else:
        req_fp = _store_get(local_path, ts)
        if req_fp is None:
            # Cached or error
            return

        start = time.time()
        log.debug('fetch-from-store %s' % (local_path))

        tempFile = tempfile.NamedTemporaryFile(delete=False, dir=deepy.cfg.data_tmp)
        tmp_name = tempFile.name
        tempFile.close()

        with open(tmp_name, 'w') as fp:
            fp.write(req_fp.read())
        req_fp.close()

        log.debug('fetch-done seconds=%f' % (time.time() - start))

    try:
       os.rename(tmp_name, local_path)
    except OSError:
       log.warn("rename-failed %s %s" % (tmp_name, local_path))
       return

    try:
       os.chmod(local_path, stat.S_IWUSR|stat.S_IRUSR|stat.S_IROTH|stat.S_IWOTH)
    except OSError:
       log.warn("chmod-failed %s %s" % (tmp_name, local_path))
       return

def _ssh_store_get(store_path, ts):
    fe = _configure_fabric()
    if fabric.contrib.files.exists(store_path, use_sudo=True):
        mtime = fabric.api.sudo('stat -c %Y {}'.format(store_path), quiet=True)
        try:
            mtime = int(mtime)
        except:
            mtime = None

        if mtime and ts >= mtime:
            log.info('ssh-get-cached {} {} >= {}'.format(store_path, ts, mtime))
        else:
            log.info('ssh-get-from-store {}@{}:{}'.format(fe.user, fe.host_string, store_path))
            fabric.api.sudo('chmod +r {}'.format(store_path), quiet=True)

            # Would love to use tempfile but this version of the fabric api
            # complains very loudly if local_path exists.
            tmp_path = _ssh_store_tmp_path()

            start = time.time()
            # XXX sometimes super slow
            #fabric.api.get(local_path=tmp_path, remote_path=store_path)
            args = ['scp', '-i', fe.key_filename[0],
                '{user}@{host}:{src}'.format(user=fe.user, host=fe.host_string, src=store_path),
                tmp_path
            ]
            subprocess.call(args)
            log.debug('ssh-scp-get seconds=%f' % (time.time() - start))

            return tmp_path
    else:
        log.info('ssh-get-not-found {}@{}:{}'.format(fe.user, fe.host_string, store_path))
    return None


# Returns an fd or None. Specify ts to check modified time.
def _store_get(store_path, ts=0, dir_mtimes=False):
    if store_path.startswith(deepy.cfg.cache_dir + '/'):
        store_path = store_path[len(deepy.cfg.cache_dir + '/'):]

    host = deepy.util.vm_or_slice_config_get('store_host')
    cred = deepy.cfg.slice_config.get('credentials')
    if host is None or cred is None:
        log.error('store-load-error host=%s credentials=%s' % (host, cred))
        sys.exit(0)
    # Using s3 secret as key
    api_key = cred[1]

    url_path = 'api/store/' + store_path
    url_path = urllib.quote(url_path)
    dir_mtimes_param = '0'
    if dir_mtimes:
        dir_mtimes_param = '1'
    params = urllib.urlencode({'api_key': api_key,
                               'dir_mtimes': dir_mtimes_param})
    url = 'https://%s/%s?%s' % (host, url_path, params)

    req = urllib2.Request(url)
    req.add_header('If-Modified-Since', http_time(ts))

    # XXX This doesn't check the server cert.
    try:
        req_fp = urllib2.urlopen(req)
    except urllib2.HTTPError as e:
        if e.code == 304:
            log.debug('store-cached %s' % (store_path))
            return None
        if e.code == 404:
            log.debug('store-not-found %s' % (store_path))
            return None
        else:
            raise
    except urllib2.URLError as e:
        print "URL error", store_path, e
        return None
    except KeyboardInterrupt as e:
        raise e
    except:
        print "Failed loading", url
        return None

    return req_fp

def _store_del(store_path):
    host = deepy.util.vm_or_slice_config_get('store_host')
    cred = deepy.cfg.slice_config.get('credentials')
    if host is None or cred is None:
        log.error('store-load-error host=%s credentials=%s' % (host, cred))
        sys.exit(0)
    # Using s3 secret as key
    api_key = cred[1]

    url_path = 'api/store/' + store_path
    url_path = urllib.quote(url_path)
    params = urllib.urlencode({'api_key': api_key})
    url = 'https://%s/%s?%s' % (host, url_path, params)

    req = urllib2.Request(url)
    req.get_method = lambda: 'DELETE'

    # XXX This doesn't check the server cert.
    try:
        urllib2.urlopen(req)
    except urllib2.HTTPError as e:
        if e.code == 404:
            log.debug('store-not-found %s' % (store_path))
            return
        else:
            raise
    except urllib2.URLError as e:
        print "URL error", store_path, e
        return
    except KeyboardInterrupt as e:
        raise e
    except:
        print "Failed loading", url
        return

def _cache_load_from_s3(s3_info, local_path):
    key_path = s3_info.key_prefix + local_path[len(deepy.cfg.cache_dir + '/'):]

    key = s3_info.bucket.get_key(key_path)
    if key is None:
        log.debug('No key for s3://%s/%s' % (s3_info.bucket.name, key_path))
        return

    #last_modified = time.strptime (key.last_modified, "%a, %d %b %Y %H:%M:%S %Z")

    try:
        fp = open(local_path)
        md5_local = key.compute_md5(fp)[0]
        md5_key = key.etag.strip("\"")
        fp.close()

        if md5_local == md5_key:
            log.debug("MD5 matches s3://%s/%s == %s" %
                    (s3_info.bucket.name, key_path, local_path))
            return
    except:
        pass


    try:
        os.makedirs(os.path.dirname(local_path))
    except:
        pass

    try:
        os.makedirs(os.path.dirname(deepy.cfg.data_tmp + "/"))
    except:
        pass

    tempFile = tempfile.NamedTemporaryFile(delete=False, dir=deepy.cfg.data_tmp)
    tmp_name = tempFile.name
    tempFile.close()

    try:
        start = time.time()
        log.debug("Fetching s3://%s/%s  -> %s  (%2.1f Kbytes)" %
                (s3_info.bucket.name, key_path, local_path, key.size / 1000))
        key.get_contents_to_filename(tmp_name)
        log.debug("Fetching Done (%f seconds)" % (time.time() - start))
    except:
        log.debug("Could not fetch %s://%s -> %s" % (s3_info.bucket.name, key_path, local_path))
        return

    try:
        os.rename(tmp_name, local_path)
        os.chmod(local_path, stat.S_IWUSR|stat.S_IRUSR|stat.S_IROTH|stat.S_IWOTH)
    except Exception,e:
        print "Rename / chmod failed for ", tmp_name, local_path, str(e)
        return

def get_s3_location(local_path):
    # Cleanup path for s3. Access functions will do the same.
    local_path = _path_normalize(local_path)
    if local_path.startswith(deepy.cfg.cache_dir + '/'):
        local_path = local_path[len(deepy.cfg.cache_dir + '/'):]
    # Also, keys don't start with slash.
    if len(local_path) > 0 and local_path[0] == '/':
        local_path = local_path[1:]
    return local_path

def _get_common_prefix(local_paths, date=True):
    if date == False:
        relevant_directories = set()
        for local_path in local_paths:
            relevant_directories.add(os.path.dirname(local_path))
        return relevant_directories

    relevant_directories = set()
    time_string_pattern = re.compile("[\d-]+")
    for local_path in local_paths:
        time_strings = time_string_pattern.findall(local_path)
        longest_string = ""
        for time_string in time_strings:
            if len(time_string) > len(longest_string):
                longest_string = time_string

        time_string = longest_string
        if time_string:

            time, granularity = deepy.timerange.parse_datetime_timeperiod(time_string, time_sep="-")

            template_length = None
            if granularity == "minute":
                template_length = 10
            elif granularity == "hour":
                template_length = 10
            elif granularity == "day":
                template_length = 7
            elif granularity == "month":
                template_length = 4
            else:
                log.warn("searching for date file with no date {}".format(local_path))


            if template_length:
                time_index = local_path.index(time_string) + template_length
                prefix = local_path[:time_index]
            else:
                prefix = os.path.dirname(local_path)
        else:
            prefix = os.path.dirname(local_path)
        relevant_directories.add(prefix)

    return relevant_directories


def list_files_remote(local_paths, deployment_id=None, force_remote=None, date=True):
    '''
    Check for specific files rather than glob. Return dict of files that
    exist and their mtimes.
    Turn date to false to not use date prefixes
    '''

    # Prevent confusion, only accept list.
    assert isinstance(local_paths, list)

    remote_types = _get_remote_types(local_paths, force_remote=force_remote)
    # Maybe change this to do all of a particular remote_type at once.
    # Probably doesn't matter if each has to be individually queried.
    results = {}
    for remote_type, local_path in zip(remote_types, local_paths):
        if remote_type == 'local':
            try:
                mtime = int(os.stat(local_path).st_mtime)
            except OSError:
                mtime = None
        elif remote_type == 'store':
            mtime = _ls_file_store(local_path)
        else:
            mtime = None

        # None signifies file doesn't exist.
        if mtime is not None:
            results[local_path] = mtime

    s3_info = s3_get_info(deployment_id)
    if s3_info is not None:
        local_paths = set(local_paths)
        relevant_directories = _get_common_prefix(local_paths, date=date)

        remote_types = _get_remote_types(relevant_directories, force_remote=force_remote)

        for remote_type, local_directory in zip(remote_types, relevant_directories):
            log.debug("listing {}".format(local_directory))
            if remote_type == 's3':
                s3_directory = get_s3_location(local_directory)
                local_directory = os.path.dirname(local_directory)
                s3_list = s3_info.bucket.list(s3_directory)

                for i, key in enumerate(s3_list):
                    if i % 1000 == 0:
                        log.debug("listing {}: {}".format(local_directory, i))
                    local_path = os.path.join(local_directory, os.path.basename(key.key))
                    mtime = key.last_modified
                    results[local_path] = arrow.get(mtime).float_timestamp

    return results


def _ls_file_s3(s3_info, local_path):
    local_path = get_s3_location(local_path)

    k = s3_info.bucket.get_key(s3_info.key_prefix + local_path)
    if k is None:
        return None
    # Getting an individual key returns a different timestamp format than
    # listing the keys... Thanks aws...
    return int(iso8601_to_timestamp(k.last_modified))

def _ls_file_store(local_path):
    # XXX Really inefficient. But, expect 'store' type to go away soon.
    dirname, filename = os.path.split(local_path)
    files = _ls_store(dirname, dir_mtimes=True)
    files = {item['name']: item['mtime'] for item in files}
    return files.get(filename)

def _ls_match_remote(match_type, local_dirname, file_pat, \
        deployment_id=None, force_remote=None, dir_mtimes=False):
    '''
    match_type should be re or glob.
    '''

    # Normalize dirname
    local_dirname = local_dirname.rstrip('/')

    remote_type = _get_remote_type(local_dirname, force_remote=force_remote)

    if remote_type == 'local':
        files = _ls_local(local_dirname, dir_mtimes=dir_mtimes)
    elif remote_type == 's3':
        s3_info = s3_get_info(deployment_id)
        if s3_info is None:
            return None
        else:
            files = _ls_s3(s3_info, local_dirname, dir_mtimes=dir_mtimes)
    elif remote_type == 'store':
        files = _ls_store(local_dirname, dir_mtimes=dir_mtimes)
    elif remote_type == 'hdfs':
        client = get_hdfs_client()
        try:
            _files = list(client.ls([local_dirname]))
        except se.FileNotFoundException:
            _files = []
        # add this so we don't change previous
        # behavior below (prep
        def _rm_root(x):
            if x.startswith(local_dirname):
                x = x[len(local_dirname + '/'):]
            return x
        files = [_rm_root(x['path']) for x in _files]

    matches = []
    for f in files:
        filename = f
        if isinstance(f, dict):
            filename = f['name']
        if match_type == 're' and re.search(file_pat, filename):
            match = True
        elif match_type == 'glob' and fnmatch.fnmatchcase(filename, file_pat):
            match = True
        else:
            match = False
        if match:
            if len(local_dirname) > 0:
                # Add separator.
                filename = local_dirname + '/' + filename
            if isinstance(f, dict):
                f['name'] = filename
                matches.append(f)
            else:
                matches.append(filename)
    return matches

def _ls_local(local_dirname, dir_mtimes=False):
    if local_dirname == '':
        # listdir() doesn't like empty string.
        local_dirname = '.'
    retval = []
    try:
        retval = os.listdir(local_dirname)
        if dir_mtimes:
            filenames = retval
            retval = []
            for fname in filenames:
                mtime = int(os.stat(local_dirname + '/' + fname).st_mtime)
                retval.append({'name': fname, 'mtime': mtime})
    except OSError:
        pass

    return retval

def _ls_s3(s3_info, local_dirname, dir_mtimes=False):
    dirname = _path_normalize(local_dirname)
    if dirname.startswith(deepy.cfg.cache_dir + '/'):
        dirname = dirname[len(deepy.cfg.cache_dir + '/'):]
    # s3 list needs dirname to end with slash, unless you're trying to ls
    # the root, which needs the empty string.
    if len(dirname) > 0 and dirname[-1] != '/':
        dirname += '/'
    # Also, keys don't start with slash.
    if len(dirname) > 0 and dirname[0] == '/':
        dirname = dirname[1:]
    dirname = s3_info.key_prefix + dirname
    results = s3_info.bucket.list(prefix=dirname, delimiter='/')
    # Strip prefix, it's added by ls_re_remote().
    results = [(r.name[len(dirname):],
               getattr(r, 'last_modified', aws_timestr_unknown))
                  for r in results]

    if dir_mtimes:
        results = [{'name': name, 'mtime': aws_timestr_to_timestamp(aws_mtime)}
                   for name, aws_mtime in results]
    else:
        results = [name for name, last_modified in results]

    return results

def _ls_store(local_dirname, dir_mtimes=False):
    fp = _store_get(_path_normalize(local_dirname), dir_mtimes=dir_mtimes)
    if fp is None:
        log.info("ls-store-no-file-{}".format(local_dirname))
        return []
    results = fp.read()
    fp.close()
    results = json.loads(results)
    log.info("ls-store-got-file-{}".format(local_dirname))
    return results.get('contents', [])

def http_time(ts):
    # RFC 1123 time
    return time.strftime("%a, %d %b %Y %H:%M:%S GMT", time.gmtime(ts))

# tornado seems more picky about this with static dirs.
def _path_join(*args):
    path = ''
    for p in args:
        if p[0] == '/':
            path += p
        else:
            path += '/' + p
    return path

def _path_normalize(path):
    # Python's os.path.normpath() leaves leading '//' for some reason (but
    # changes '///' into '/' ?!)
    return re.sub('^//', '/', os.path.normpath(path))

# List of allowed remote types.
remote_choices = ['s3', 'store', 'local', 'hdfs']

# Testing for _get_remote_type. Add as decorator.
def test_log(fn):
    def new_fn(*args, **kwargs):
        rv = fn(*args, **kwargs)
        if rv != 'local':
            log.info('fn={} args={} kwargs={} ret={}'.format(fn.__name__,
                args, kwargs, rv))
        return rv
    return new_fn

#@test_log
def _get_remote_type(filename, force_remote=None, fallback_from=None):
    '''
    Five ways of setting the remote:
    1. If deepy.cfg.force_remote is set, use that. This is should only be
       set for dev work, via command line.
    2. If force_remote paramater is set, use that. This is for overrides of
       slice within code. ***DEPRECATED***
    3. If filename isn't in deepy.cfg.cache_dir, use local. Generally only
       things in cache are under purview of deepy.store, but this allows
       loading local files. May eventually fold this into the directory
       override stuff.
    4. If override_store is set. This is for per directory overrides, we now
       use this so we don't have to have force_remote=whatever sprinkled
       throughout the code
    5. Check slice. Slice can be set either within the global or vm specific.
       vm specific is currently only used to prevent the store host from
       contacting it's own UI for files.
    These are in order of decreasing precedence.

    fallback_from: If this is set this means that we cannot select this store
                   type because we already tried this type

    '''
    if deepy.cfg.force_remote is not None:
        return deepy.cfg.force_remote

    if force_remote in remote_choices:
        return force_remote

    if not filename.startswith(deepy.cfg.cache_dir):
        return 'local'

    if deepy.cfg.override_store and filename:
        dirs = deepy.cfg.override_directories
        if any(directory in filename for directory in dirs):
            return deepy.cfg.override_store

    def check_slice_section(section_dict):
        if section_dict is None:
            return None

        if section_dict.get('store_type'):
            if section_dict.get('store_type') != fallback_from:
                return section_dict.get('store_type')

        if section_dict.get('store_local') == True:
            return 'local'
        elif 'store_host' in section_dict:
            return 'store'
        elif section_dict.get('sync_data_in_s3') == False:
            # sync_data_in_s3 is deprecated. Use store_local.
            return 'local'

    vm_remote_type = check_slice_section(deepy.cfg.vm_config)
    slice_remote_type = check_slice_section(deepy.cfg.slice_config)

    if vm_remote_type is not None:
        return vm_remote_type
    elif slice_remote_type is not None:
        return slice_remote_type

    return 's3'

def _get_remote_types(filenames, force_remote=None, fallback_from=None):
    '''
    Return a list of remote_types, i.e.
    ['s3', 's3', 'local', 's3', 'store', 'hdfs']
    '''
    return [_get_remote_type(filename, force_remote=force_remote, fallback_from=fallback_from) \
        for filename in filenames]

def deployment_sync(only_config=False, force_flag=False, verbose_flag=False, dry_run=False):

    # create tmp so you can just use deployment_sync to create deployment
    # required later by deepy.store
    if not os.path.exists(deepy.cfg.data_tmp):
        os.makedirs(deepy.cfg.data_tmp)

    files = deepy.cfg.get_config_files(only_config)
    if files is None:
        deepy.log.error('get_config_files-returned-none')
        files = []

    # CI-963 make sure to get dimensions that might not yet be in dimensions_idx
    dimension_frag_files = deepy.store.ls_glob_remote(deepy.cfg.dimensions_dir, '*.json.gz')
    if dimension_frag_files is None:
        deepy.log.error('remote-dimension-ls-returned-none')
        dimension_frag_files = []

    files = list(set(files + dimension_frag_files))
    files = filter(lambda x: 'timestamp.json.gz' not in x, files)

    # Pull in context files
    context_files = deepy.store.ls_glob_remote(deepy.cfg.context_dir, '*.json')
    if context_files:
        files += context_files

    if only_config and not force_flag:
        last_sync = 0
        try:
            last_sync = \
                 os.stat(deepy.cfg.config_local_last_update_file).st_mtime
        except OSError, e:
            if e.errno != errno.ENOENT:
                print 'Error stat()ing last_update file', e

        last_commit = deepy.store.ls_glob_simple(deepy.cfg.config_last_update_file, dir_mtimes=True)
        try:
            if last_commit[0]['mtime'] < last_sync:
                if verbose_flag:
                    print 'Config cache up to date'
                return
        except (KeyError, IndexError):
            pass

    if not dry_run:
        if force_flag:
            deepy.store.cache_load_from_remote(files, verbose=verbose_flag, cache_age=0)
        else:
            deepy.store.cache_load_from_remote(files, verbose=verbose_flag)

    config_dir = os.path.dirname(deepy.cfg.config_local_last_update_file)
    if not os.path.exists(config_dir):
        os.makedirs(config_dir)
    fp = open(deepy.cfg.config_local_last_update_file, 'w+')
    fp.write(json.dumps({"mtime": time.time()}))
    fp.close()

    return files

def deployment_pull(shadow_deployment_id, only_config=False,
                    extra_slice_key_del=None):
    '''
    Formerly just a stand-alone script, this copies configuration from a
    shadow's master (as configured in the shadow slice) to either bootstrap or
    update a shadow with data from the master.

    Puts a copy of the master's config in a shadow subfolder at the top-level.

    Other relevant slice config settings:
        deployment_master.deployment_pull: "partial"
            Does a "partial" deployment pull, used for the demo box, does not
            pull:
                slice: build_updates
                slice: connectors
                the files enumerated in partial_skip below (e.g.
                    tile_config.json)
                dimensions (this may need to be revisited)
        deployment_master:skip_pull_files (list)
            Does not pull if os.path.basename(shadow_fname) in skip_pull_files
    '''

    # XXX move some subdirs (eg shadow) into global constants.
    # XXX add a flag to remove files that don't exist on master

    if extra_slice_key_del is None:
        extra_slice_key_del = []

    orig_deployment_id = deepy.cfg.deployment_id
    if shadow_deployment_id == None:
        orig_deployment_id = None

    deepy.cfg.init(shadow_deployment_id)
    shadow_slice_json = deepy.cfg.slice_config

    master_id = shadow_slice_json.get('deployment_master',
            {}).get('deployment_id')
    pull_option = shadow_slice_json.get('deployment_master',
            {}).get('deployment_pull')
    skip_pull_files = shadow_slice_json.get('deployment_master',
            {}).get('skip_pull_files', [])
    skip_pull_files.append('deployment.json')
    skip_pull_files.append('dimensions_db.json')
    skip_pull_files.append('dimensions_db_small.json')
    skip_pull_files.append('dimensions_idx.json')

    if master_id is None:
        deepy.log.error('no-master-deployment-to-pull-from')
        deepy.cfg.init(orig_deployment_id)
        return False

    shadow_hood = os.path.join(deepy.cfg.home_dir, "shadow")

    deepy.cfg.init(master_id, pipedream_hood_override=shadow_hood)
    shadow_cache = deepy.cfg.cache_dir
    files = deployment_sync(only_config=only_config,
            force_flag=True)

    deepy.cfg.init(shadow_deployment_id)

    # Failsafe - delete master's access keys, and some other attrs that we
    # will never use on the shadow but might break things.
    #
    # BE VERY CAREFUL HERE.  Master's keys are on disk at this point, DON'T
    # USE THEM.
    master_slice_path = os.path.join(deepy.cfg.home_dir, "shadow", master_id,
            "cache", "config", "slice.json")

    master_slice = simple_load_json(master_slice_path,
                                                force_remote='local')

    # XXX Make shadowing of VM config a little more configurable. For now,
    # just require local config..
    for always_del in ['credentials', 'license', 'deployment_master',
                       'version', 'saved_by', 'customer_id', 'vms',
                       'store_host', 'sync_data_in_s3', 'ui_processes',
                       'proxy'] + \
                      extra_slice_key_del:
        master_slice.pop(always_del, None)

    if pull_option == 'partial':
        for partial_del in ['build_updates', 'connectors']:
            master_slice.pop(partial_del, None)

    simple_save_json(master_slice, master_slice_path,
                                 force_remote='local')

    partial_keep = [
        # demo tweaks
        "tile_config.json",
        "ui.json",

        # generated on demo box
        "dimensions_idx.json",
        "dimensions_db.json.gz",
        "dimensions_db_small.json.gz",

        # anonymized
        "member.json.gz",
        "peer.json.gz",
        "as_origin.json.gz"
        ]

    # XXX Merge overrides ?
    push_files = []
    for shadow_fname in files:
        if not shadow_fname.startswith(shadow_cache):
            shadow_fname = os.path.join(shadow_cache, shadow_fname)

        if not os.path.exists(shadow_fname):
            deepy.log.debug('not-found %s' % (shadow_fname))
            continue

        fname = '%s%s' % (deepy.cfg.cache_dir, shadow_fname[len(shadow_cache):])
        if os.path.basename(shadow_fname) in skip_pull_files:
            deepy.log.debug("slice-specified-skip {}".format(shadow_fname))
            continue
        elif os.path.basename(shadow_fname) == 'slice.json':
            # XXX Merge config files w/ prod overriding
            deepy.log.debug('merge-to %s %s' % (shadow_fname, fname))
            sh_cfg = simple_load_json(shadow_fname,
                                                  force_remote='local')
            if sh_cfg is None:
                continue
            cfg = simple_load_json(fname)
            if cfg is None:
                cfg = sh_cfg
            else:
                deepy.util.update_recursive(cfg, sh_cfg, merge_lists=False)

            simple_save_json(cfg, fname)
            #print cfg
        elif pull_option == "partial" and os.path.basename(shadow_fname) in partial_keep:
            deepy.log.debug("partial-skip {}".format(shadow_fname))
            continue
        elif pull_option == "partial" and 'dimensions' in shadow_fname:
            deepy.log.debug("partial-skip {}".format(shadow_fname))
            continue
        else:
            deepy.log.debug('copy-to %s %s' % (shadow_fname, fname))
            if not os.path.exists(os.path.dirname(fname)):
                os.makedirs(os.path.dirname(fname))
            shutil.copy2(shadow_fname, fname)
            push_files.append(fname)

    if len(push_files) > 0:
        # FIXME refactor in to a s3->s3 utility function
        s3 = boto.connect_s3()
        dst_bucket = s3.get_bucket('{}.pdrlabs.net'.format(deepy.cfg.deployment_id))
        source_bucket_name = '{}.pdrlabs.net'.format(master_id)

        push_keys = map(lambda x: x[len(deepy.cfg.cache_dir + '/'):], push_files)

        deepy.log.debug('s3-to-s3-config-push')
        for aws_path in push_keys:
            deepy.log.debug('s3://{source}/{aws_path} -> s3://{dest}/{aws_path}'.format(source=source_bucket_name, dest=dst_bucket.name, aws_path=aws_path))
            dst_bucket.copy_key(aws_path, source_bucket_name, aws_path)

    deepy.cfg.init(orig_deployment_id)
    return True

def _install (key, aws_path, deployment_id):
    print "Copy ", aws_path, deployment_id + ".pdrlabs.net"

    if deployment_id == 'map':
        return

    try:
       key.copy(deployment_id + ".pdrlabs.net", aws_path)
    except:
        return

def install_each_deployment (aws_path, deployment_id=None):
    s3 = boto.connect_s3()
    key = s3.get_bucket('map.pdrlabs.net').get_key(aws_path)

    if not key:
        print "install_each_deployment could not get key %s://%s" % ('map.pdrlabs.net', aws_path)
        return

    if deployment_id:
        _install (key, aws_path, deployment_id)
    else:
        # Copy to each deployment bucket
        ids = deepy.deploy.get_cached_deployment_ids()
        ids.append("subscription")
        for id in ids:
            _install (key, aws_path, id)

def get_hdfs_client():
    return AutoConfigClient()

#Sanity Checks for hdfs operations. Not intended to be run in the testing framework
def setUp():
    #Create a temp file and add it to hdfs
    tmpFileHandler = tempfile.NamedTemporaryFile(delete=False)
    tmpFile = tmpFileHandler.name
    tmpFileHandler.write('{"test":1}')
    return tmpFile

def cleanup(tmpFile, testFile):
    try:
        os.remove(tmpFile)
    except:
        pass
    try:
        os.remove(testFile)
    except:
        pass

    try:
        rm_files([testFile], force_remote='hdfs')
    except:
        pass

    try:
        rm_files([tmpFile], force_remote='hdfs')
    except:
        pass


def test_hdfs_get():
    tmpFile = setUp()
    test_fname = "{}.get_test".format(tmpFile)
    shutil.copy(tmpFile, test_fname)

    cache_save_to_remote(test_fname, force_remote='hdfs')
    os.remove(test_fname)
    assert set(simple_load_json(test_fname, force_remote='hdfs').keys()) == set(json.load(open(tmpFile)).keys())
    os.remove(test_fname)
    cleanup(test_fname, tmpFile)

def test_hdfs_put():
    tmpFile = setUp()
    test_fname = "{}.put_test".format(tmpFile)
    shutil.copy(tmpFile, test_fname)
    cache_save_to_remote(test_fname, force_remote='hdfs')
    os.remove(test_fname)
    out = simple_load_json(test_fname, force_remote='hdfs')
    assert set(out.keys()) ==  set(json.load(open(tmpFile)).keys())
    cleanup(tmpFile, test_fname)

def test_hdfs_del():
    tmpFile = setUp()
    test_fname = "{}.del_test".format(tmpFile)
    shutil.copy(tmpFile, test_fname)
    cache_save_to_remote(test_fname, force_remote='hdfs')
    os.remove(test_fname)
    assert simple_load_json(test_fname, force_remote='hdfs')
    os.remove(test_fname)
    rm_files([test_fname], force_remote='hdfs')
    assert not simple_load_json(test_fname, force_remote='hdfs')
    cleanup(tmpFile, test_fname)

def test_hdfs_ls():
    tmpFile = setUp()
    test_fname = "{}.ls_test".format(tmpFile)
    shutil.copy(tmpFile, test_fname)
    cache_save_to_remote(test_fname, force_remote='hdfs')
    os.remove(test_fname)
    assert ls_files_remote([test_fname], force_remote='hdfs')
    cleanup(tmpFile, test_fname)

