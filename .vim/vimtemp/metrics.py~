
import multiprocessing
import abc
import os
import gzip
import subprocess
import datetime
import dateutil
import collections
import socket
import re
import json
import psutil
import pexpect
import argparse
import glob
import functools
import urllib2
import funcy
import math
try:
    import sensors
except ImportError:
    # No sensors on mac
    sensors = None

import arrow
import time
import pandas as pd
import ipaddr
import tables
from fabric.api import run, settings
from fabric.exceptions import NetworkError
from fabric.state import output

import deepy.log
import deepy.cfg
import deepy.dimensions
import deepy.event
import deepy.ui_status
import deepy.store
import deepy.aws
import deepy.local_queue
import deepy.util
import deepy.cube
import deepy.status
import deepy.h5flowutil
import deepy.pybird
import deepy.timerange
import deepy.build.deepy_jobs
arrow = deepy.timerange.arrow_factory

def is_vm_running_daemon(daemon):
    return True if daemon in deepy.cfg.vm_config.get("daemons", []) else False

def is_vm_running_extra_cron_job(job):
    return True if job in deepy.cfg.vm_config.get('extra_cron_jobs', []) else False

def load_previous_heartbeat(now, uuid):
    prev = now - datetime.timedelta(seconds=300)
    prev_time_str = now.strftime("%Y-%m-%d-%H-%M")
    filename = 'vm.{}.json.gz'.format(prev_time_str)
    path = os.path.join(deepy.cfg.heartbeat_dir, uuid, 'vm', filename)
    return deepy.store.simple_load_json(path)

def load_uuid():
    uuid = deepy.cfg.vm_uuid
    if not uuid:
        return None
    return uuid

def exists(path):
    return os.path.exists(path) and os.path.isfile(path)

def add_delta(current, prev, key):
    if not current or key not in current:
        return
    dk = key + '_delta'
    current[dk] = 0
    if not prev or key not in prev:
        return
    current[dk] = current[key] - prev[key]

def get_dir_size(directory='.'):
    try:
        return sum(os.path.getsize(os.path.join(dirpath, filename))
                for dirpath, dirnames, filenames in os.walk(directory)
                for filename in filenames if not
                os.path.islink(os.path.join(dirpath, filename)))
    except OSError:
        deepy.log.exception('Failed dir size for {}'.format(directory))
        return 0

def parse_bundle2(args_in):
    # XXX copypasta from bundle2.py
    p = argparse.ArgumentParser(description='''
        E.g.:
        bundle2.py -M bundle2_drill_cdn1 -o '/tmp' -t '2013-10'
        ''', formatter_class=argparse.RawTextHelpFormatter)

    p.add_argument('-d', dest='deployment_id')
    p.add_argument('-l', dest='log_cfg')
    p.add_argument('-L', dest='force_local', action='store_true', default=False)
    p.add_argument('-M', dest='make_rule', required=True, default=None)
    p.add_argument('--partition', dest='partition', action='store_true', default=False)
    p.add_argument('-t', dest='timestamp', help='used for make_rule file glob')
    p.add_argument('--ts-day', dest='tsday', default=False, action='store_true', help='generate day timestamp from now')
    p.add_argument('--ts-month', dest='tsmonth', default=False, action='store_true', help='generate month timestamp from now')
    p.add_argument('-m', dest='marker_file')
    p.add_argument('-o', dest='output_location')
    p.add_argument('-N', dest='dry_run', action='store_true')
    p.add_argument('-q', dest='query', action='store', default=None, help='use only this query')

    args = p.parse_args(args_in)
    return args

def sample_latest_5min(template, step_limit=288):
    '''
    Given a template such as /pipedream/h5flow/flow..., return the
    number of seconds that have elapsed this the most recent file was
    built, relative to the time the function was called.

    Returns None if no file was found within 5*step_limit minutes.
    '''
    now = datetime.datetime.utcnow()
    end = deepy.timerange.floor_timestamp_to(now, '5T')
    minutes = 5 * step_limit
    begin = end - datetime.timedelta(minutes=minutes)
    timerange = arrow.range('5T', begin, end)
    last = None
    for ts in reversed(timerange):
        path = deepy.timerange.substitute_timestamp(template, ts)
        deepy.store.cache_load_from_remote(path)
        if os.path.isfile(path):
            last = ts
            break;

    if last is None:
        return None
    else:
        return (now - last).seconds

def check_file_mtime(template):
    '''
    Given a template such as /pipedream/cache/bgp/bgp.h5, return the amount
    of elapsed time since the file was created
    '''
    now = time.time()
    if not os.path.isfile(template):
        return None

    mtime = os.path.getmtime(template)
    return int(now - mtime)


def find_gaps(cube, step, seconds_back=86400):
    '''
    Given a template such as /pipedream/h5flow/flow..., return the percentage
    of files that are built compared to the number of files that should be built
    '''

    rule = deepy.cube.get_rule(cube, step)
    if not rule:
        return

    end_ts = int(time.time())

    step = int(deepy.timerange.convert_to_seconds(step))

    end_ts = int(math.floor(end_ts / step)) * step
    start_ts = end_ts - seconds_back
    end_ts += step

    timesteps = range(start_ts, end_ts, step)
    paths = []
    expanded_targets = rule.expand(
        {
            "start_time": start_ts,
            "end_time": end_ts,
        }
    )
    for expanded_target in expanded_targets:
        paths.append(expanded_target.unique_id)

    deepy.store.cache_load_from_remote(paths)

    all_files = zip(timesteps, paths)
    missing = funcy.remove(funcy.compose(exists, funcy.second), all_files)

    return 100*(float(len(missing))/len(all_files))


def want_rule(ignored_cubes, time_steps, rule_name, make_rule):
    # As a first pass, we only want to alert on non-fake hour cubes.
    # We might want to be more dynamic here in the future.

    # XXX Side-effect cubes are totally missed (such as sub_counts from
    # cubes_from_h5flow.py)

    # XXX Might want to check fake cubes in the future to capture the side
    # effects, but need to somehow find out when cubes can be legitimately
    # missing (e.g. sub_count on non sub deployments, backbone on no
    # backbone deployments). Also need to be careful about transitional
    # cubes such as map1, which are there to enable querying of historical
    # data, but should not alert on new missing stuff.
    if not make_rule:
        return None

    cube_id = make_rule.get('meta', {}).get('cube_id')
    if not cube_id or cube_id in ignored_cubes:
        return None

    if cube_id == "cube_snmp":
        if "snmp" not in deepy.cfg.vm_config.get('extra_cron_jobs', []):
            return None

    rule = make_rule

    time_step = rule.get('time_step')
    if time_step not in time_steps:
        return None

    fake = any(filter(lambda x: "echo" in x, rule.get('recipe', [])))
    if fake:
        return None

    # XXX: These are cubes that all deployments should be running, should be
    # representative of cubes as a whole, and are at the end of the cube
    # pipeline. So if these cubes are failing we're probably having issues
    # with others as well. We decided to do this to try and reduce the number
    # of false positives on the status dashboard.
    cube_alert_list = ["drill_small_hour", "backbone_small_hour"]
    if not any([l in rule_name for l in cube_alert_list]):
        return None

    return make_rule

def carp_master(carp_ip):
    '''
    Determine if this machine is currently the CARP master
    '''

    # Quiet fabric
    output["stdout"] = False
    output["running"] = False
    output["debug"] = False

    settings_kwargs = {
        "host_string": carp_ip,
        "user": "support",
        "key_filename": deepy.cfg.pdrops_key_file,
        "timeout": 5,
        "connection_attempts": 1,
        "disable_known_hosts": True
    }
    with settings(**settings_kwargs):
        try:
            remote_uuid = run('cat /pipedream/uuid.txt')
        except NetworkError:
            deepy.log.error("network-error-to-carp-ip {}".format(carp_ip))

            # On failure always match
            return True

    return deepy.cfg.vm_uuid == remote_uuid

class Metric(object):
    tags = []

    @abc.abstractmethod
    def collect(self):
        '''
        Collect this metric. Return None if cannot collect or if it is invalid
        to do so
        '''
        pass

    @abc.abstractmethod
    def publish(self, metric, client=None):
        '''
        Given an output of collect, publish the data to event bus.
        '''
        pass

    def send(self, name, metric=None, client=None, data=None, meta=None, **kwargs):
        '''
        Send event to event bus
        '''
        tags = self.tags
        if 'tags' in kwargs:
            tags = self.tags + kwargs['tags']
            del kwargs['tags']

        return deepy.event.send_event(name, metric, tags=tags, data=data, meta=meta, client=client, **kwargs)

    @abc.abstractmethod
    def snapshot(self, client=None):
        '''
        Collect and send a snapshot of the metric
        '''
        pass

    def send_snapshot(self, service, metric=None, client=None, data=None, meta=None, **kwargs):
        '''
        Collect and send a snapshot of the metric
        '''
        collected_data = self.collect()
        tags = ['heartbeat-compat', 'persist']
        return deepy.event.send_event(service, metric, tags=tags, data=collected_data, meta=meta, client=client, **kwargs)

class CPUMetric(Metric):
    tags = ['heartbeat', 'status', 'persist']

    def collect(self):

        cpu_temp = self.get_cpu_temp()
        cpu_utilization = psutil.cpu_percent(interval=1)

        result = \
        {
            "cpu_utilization": cpu_utilization,
            'cpu_temp_max' : cpu_temp['max'],
            'cpu_temp_avg' : cpu_temp['avg']
        }

        return result

    def get_cpu_temp(self):
        if sensors is None:
            return {}
        sensors.init()
        temps = {}
        temps['cores'] = {}
        cores = temps['cores']
        for chip in sensors.iter_detected_chips():
            try:
                for feature in chip:
                    cores[feature.label] = (feature.get_value())
            except:
                pass

        max_temp = 0
        avg_temp = 0
        for temp in cores:
            core_temp = cores[temp]
            if max_temp < core_temp:
                max_temp = core_temp
            avg_temp += core_temp
        if len(cores) > 0:
            avg_temp = avg_temp / len(cores)
        temps['max'] = max_temp
        temps['avg'] = avg_temp
        sensors.cleanup()
        return temps


    def publish(self, metric, client=None):
        prefix = 'metric/vm/system/'
        if metric is None:
            deepy.log.warning("Failed to collect CPUMetric")
            return
        for k, v in metric.iteritems():
            if k == 'cpu_temp_max' or k == 'cpu_temp_avg':
                self.send('metric/debug/vm/' + k,
                metric[k],
                tags=['persist', 'heartbeat', 'debug'],
                client=client)

            else:
                name = prefix + k
                self.send(
                    name,
                    metric[k],
                    client=client
                )

    def snapshot(self, client=None):
        self.send_snapshot('cpu_utilization')

class UIStatusMetric(Metric):

    tags = ['ui']

    def format(self, data={}):
        return {
            "ui_status:reachable": ("counter",  int(data["reachable"]))
        }
    def collect(self):
        ui_status = {}
        ui_status['reachable'] = deepy.ui_status.is_ui_reachable(deepy.cfg.deployment_id, timeout=5, base_url="https://localhost")
        return ui_status

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log("Failed to collect UIStatusMetric")
            return
        self.send('pipedream/ui/reachable', metric['reachable'], client=client)


class SNMPStatusMetric(Metric):

    tags = ['snmp']

    def collect(self):
        local_path = os.path.join(deepy.cfg.cache_dir, "snmp", "snmp.json.gz")
        last_mod = 0
        snmp = deepy.store.simple_load_json(local_path)

        if snmp:
            last_mod = os.stat(local_path).st_mtime
            for router in snmp['routers']:
                if 'intfs' in router:
                    router['intfs'] = len(router['intfs'])

        return {
            'snmp': snmp,
            'last_mod': last_mod
        }

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log("Failed to collect SNMPStatusMetric")
            return
        last_mod = metric['last_mod']
        if last_mod != 0:
            self.send('snmp/last_mod', last_mod, client=client)

        snmp = metric['snmp']
        if not snmp:
            return

        prefix = 'snmp/routers/'
        for router in snmp['routers']:
            router_name = router['name']
            metric_base_name = prefix + router_name + "/"

            self.send(metric_base_name + 'info', metric=None,
                description=router['info'] or router['name'],
                snmp_ip=router['snmp_ip'],
                location=router['location'],
                netstream_mapping=router['netstream_mapping'],
                if_index_to_agg=router['if_index_to_agg'],
                client=client
            )

            self.send(metric_base_name + 'intfs', router['intfs'], client=client)
            self.send(metric_base_name + 'sysuptime', router['sysuptime'], client=client)

    def snapshot(self, client=None):
        self.send_snapshot('snmp')

class QueueMetric(Metric):
    '''
    XXX
    heartbeat_config:avg_queue_size_domain_seconds = 3600
        average the queue over the last hour
    '''

    tags = ['heartbeat', 'status', 'persist']

    def collect(self):

        return {
            'sqs_queue_size': deepy.aws.get_sqs_queue_count(deepy.cfg.deployment_id),
            'local_queue_size': deepy.local_queue.length(),
        }

    def publish(self, metric, client=None):
        prefix = 'metric'
        if metric is None:
            deepy.log.warning("Failed to collect QueueMetric")
            return
        self.send(prefix+'/deployment/queue/remote', metric['sqs_queue_size'], client=client)
        self.send(prefix+'/vm/queue/local', metric['local_queue_size'], client=client)

    def snapshot(self, client=None):
        self.send_snapshot('queue')


class GitStatusMetric(Metric):

    tags = ['heartbeat', 'status', 'git']

    def collect(self):
        code_root = deepy.deploy.get_code_root()
        output = {}
        if code_root:
            output['HEAD'] = subprocess.check_output(['git', 'rev-parse', 'HEAD'], cwd=code_root).strip()[:7]
            output['branch'] = subprocess.check_output(['git', 'rev-parse', '--abbrev-ref', 'HEAD'], cwd=code_root).strip()
        else:
            output['HEAD'] = 'Unknown'
            output['branch'] = 'Unknown'

        return output


    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect GitStatusMetric")
            return
        self.send('event/deployment/git/status',
            metric=None,
            description="On {HEAD} at {branch}".format(**metric),
            HEAD=metric['HEAD'], branch=metric['branch'],
            client=client
        )

    def snapshot(self, client=None):
        self.send_snapshot('git')

class SADFMetric(Metric):

    tags = ['sadf']

    def collect(self):
        now = arrow.get()
        start = now - datetime.timedelta(minutes=30)
        start_time = start.strftime("%H:%M:00")
        end_time = now.strftime("%H:%M:00")

        try:
            with open(os.devnull, 'w') as devnull:
                sadf_json_str = subprocess.check_output(['sadf', '-j', '--', '-d', '-p', '-s', start_time, '-e', end_time], stderr=devnull)
                if sadf_json_str:
                    return json.loads(sadf_json_str)
        except subprocess.CalledProcessError:
            deepy.log.exception('SADFMetric: Failed calling sadf')
        except OSError:
            deepy.log.error('SADFMetric: sysstat/sadf not installed')
        return None

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect SADFMetric")
            return
        self.send('system/sadf', description="Output of sadf", client=client, data=metric)

    def snapshot(self, client=None):
        self.send_snapshot('sadf')

class H5FlowMetric(Metric):

    tags = ['heartbeat', 'flowd']

    def load_router_dimension(self):
        dim_idx = deepy.dimensions.DimensionsIdx()
        dim_idx_elm = dim_idx.get_dim_by_name("router")
        dim_path = os.path.join(deepy.cfg.dimensions_dir, dim_idx_elm['file'])
        dims = deepy.dimensions.DimensionsDBFrag(dim_path)
        dim_routers = dims[str(dim_idx_elm['id'])]
        return dim_routers


    def collect(self):
        # Load h5flow stats
        if not is_vm_running_daemon("flowd"):
            deepy.log.warning('H5FlowMetric: Not running flowd')
            return None

        heartbeat_config = deepy.status.get_heartbeat_config()
        h5flow_status = None

        temp = deepy.util.recursive_default_dict()
        end = deepy.timerange.floor_timestamp_to(datetime.datetime.utcnow(), '5T')
        # was: hours=6
        begin = end - datetime.timedelta(hours=3)
        file_str = os.path.join(deepy.cfg.h5flow_dir, "flow.%Y-%m-%d-%H-%M.h5")
        files = deepy.timerange.generate_file_glob(file_str, begin, end, '5T')
        files.reverse()

        dim_routers = self.load_router_dimension()
        dim_ips = {v.get('attributes', {}).get('computed', {}).get('router', {}).get('flow_ip'): k
            for k, v in dim_routers.get('positions', {}).items()}

        for filename in files:
            if deepy.util.vm_or_slice_config_get("archive_h5flow"):
                 deepy.store.cache_load_from_remote(filename)
            try:
                h5file = tables.open_file(filename)
            except IOError:
                continue

            m = re.search("flow\.([0-9\-]+)\.h5", filename)
            if not m:
                continue

            if h5flow_status is None:
                h5flow_status = {}

            tstamp_ascii = m.group(1)
            tstamp = time.mktime(datetime.datetime.strptime(tstamp_ascii, "%Y-%m-%d-%H-%M").timetuple())

            h5flow_status['meta_data'] = {"duration": 300, "tstamp": tstamp, "tstamp_ascii": tstamp_ascii}

            try:
                h5flow_status["total_num_dns"] = h5file.root.flowgroup.onnet_v4.attrs.total_num_dns
                h5flow_status["num_with_dns"] = h5file.root.flowgroup.onnet_v4.attrs.num_with_dns
                h5flow_status["num_onnet_ip_v4"] = h5file.root.flowgroup.onnet_v4.nrows
            except:
                h5flow_status["total_num_dns"] = 0
                h5flow_status["num_with_dns"] = 0
                h5flow_status["num_onnet_ip_v4"] = 0

            h5flow_status["h5flow_ago"] = None
            if not heartbeat_config.get('ignore_h5flow_ago'):
                #XXX remove hb config
                ret = sample_latest_5min(file_str)
                if ret:
                    h5flow_status["h5flow_ago"] = ret
                else:
                    deepy.log.warning('H5FlowMetric: did not find h5flow for h5flow_ago')

            h5flow_status["routers"] = []


            for row in h5file.root.flowgroup.routers:
                ip = "%s" % ipaddr.IPv4Address(socket.ntohl(row['flowip']))
                fps = round (float(row['num_flows'] / 300.0), 0)
                bps = round ((float(row['bytes_flows'] * 8) / 300.0), 0)
                traffic_bps = round ((float(row['traffic_bytes'] * 8) / 300.0), 0)

                num_flows_missing_templates = row['num_flows_missing_templates']

                router = {"pos_id": row["router_pos"],
                          "ip": ip,
                          "fps": fps,
                          "bps": bps,
                          "traffic_bps": traffic_bps,
                          "version": row["version"],
                          "is_sflow": row["is_sflow"],
                          "nflows": row['num_flows'],
                          "nflows_missing": row["num_flows_missing"],
                          "sampling_rate": row["sampling_rate"],
                          "bytes_with_bad_duration": row["bytes_with_bad_duration"],
                          "garbage_sequence_numbers": row["garbage_sequence_numbers"]
                          }

                if ip in dim_ips.keys():
                    sample_rate = dim_routers.get('positions', {}).get(dim_ips[ip], {}).get('attributes', {}).get('computed', {}).get('router', {}).get('sampling_rate_override')
                    if sample_rate:
                        router["sampling_rate"] = sample_rate

                if num_flows_missing_templates:
                    router['num_flows_missing_templates'] = num_flows_missing_templates

                h5flow_status["routers"].append(router)

            h5file.close()
            break

        if h5flow_status is None:
            deepy.log.error('H5FlowMetric: No flows files found going back {} steps'.format(len(files)))
            return None

        file_str = os.path.join(deepy.cfg.flows_dir, "flow.%Y-%m-%d-%H-%M.pcap.gz")
        for filepath in deepy.timerange.generate_file_glob(file_str, begin, end, '5T'):
            filename = os.path.basename(filepath)
            try:
                size = os.path.getsize(filepath)
            except OSError:
                continue
            if size < 1500: #1.5 kB, unusually small
                temp["errors"]["bad_size"][filename] = size

        h5flow_status.update(temp)

        return h5flow_status

    def publish(self, metric, client=None):
        prefix = 'metric/deployment/flowd/'

        if 'routers' not in metric:
            deepy.log.warning('H5FlowMetric: no routers in metric')
            return

        summary_data = collections.defaultdict(list)
        try:
            for router in metric['routers']:
                router_prefix = prefix + 'routers/' + router['ip'] + '/'
                for k, v in router.iteritems():
                    if deepy.util.is_number(v):
                        self.send(router_prefix+k, metric=v)

                for k in ('fps', 'bps', 'traffic_bps', 'nflows'):
                    summary_data[k].append(router[k])

            # Summarize data
            data_frame = pd.DataFrame(summary_data)
            self.send(prefix+'total_fps', metric=data_frame['fps'].sum(), client=client, tags=['persist', 'status', 'heartbeat'])
            self.send(prefix+'total_bps', metric=data_frame['bps'].sum(), client=client, tags=['persist', 'status', 'heartbeat'])
            self.send(prefix+'traffic_bps', metric=data_frame['traffic_bps'].sum(), client=client, tags=['persist', 'status', 'heartbeat'])
        except:
            deepy.log.exception("Failed to collect H5FlowMetric")

    def snapshot(self, client=None):
        self.send_snapshot('flowd')

class H5BGPMetric(Metric):

    tags = ['h5bgp']

    def collect(self, filename=os.path.join(deepy.cfg.bgp_dir, "bgp.h5")):

        if not any(is_vm_running_daemon(d) for d in ['bird', 'bird6', 'bgpd']):
            deepy.log.warning('H5BGPMetric: Not running a bgp daemon')
            return None

        try:
            h5file = tables.open_file(filename)
        except IOError:
            return None

        seconds_since_file_mtime = int(time.mktime(time.gmtime()) -
            os.path.getmtime(filename))

        result = {}
        result["seconds_since"] = seconds_since_file_mtime
        result["ipv4_routes_count"] = h5file.root.routes.routesv4.nrows
        result["ipv6_routes_count"] = h5file.root.routes.routesv6.nrows
        result["ipv4_routers_count"] = h5file.root.routers.v4.nrows
        result["ipv6_routers_count"] = h5file.root.routers.v6.nrows

        h5file.close()

        return result

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect H5BGPMetric")
            return
        prefix = 'pipedream/h5bgp/'
        self.send(prefix + str(metric), data=metric, client=client)

    def snapshot(self, client=None):
        self.send_snapshot('h5bgp')

class SystemInfoMetric(Metric):
    tags = ['heartbeat', 'status']

    def collect(self):

        now = int(time.time())
        disk_iops = 0
        net_io_sent = 0
        net_io_recv = 0

        prev_system_state = deepy.store.simple_load_json(deepy.cfg.system_state)
        if prev_system_state: # we can get the stats for the last n minutes
            try:
                # disk iops is the amount of reads and writes in the last n seconds divided by n
                disk_iops = (psutil.disk_io_counters().read_count + psutil.disk_io_counters().write_count \
                    - (prev_system_state['disk_io_counters']['read_count'] + prev_system_state['disk_io_counters']['write_count'])) \
                    / (now - prev_system_state['current_time'])

                # net io bps is the bytes that were sent and recieved in the last n seconds divided by n
                net_io_sent = (psutil.net_io_counters().bytes_sent - prev_system_state['net_io_counters']['bytes_sent']) \
                    / (now - prev_system_state['current_time'])

                net_io_recv = (psutil.net_io_counters().bytes_recv - prev_system_state['net_io_counters']['bytes_recv']) \
                    / (now - prev_system_state['current_time'])

            except (TypeError, KeyError) as e:
                deepy.log.warn(e)

        # This happens when a vm is reset, the counters reset, so the delta is meaningless
        if disk_iops < 0 and net_io_sent < 0 and net_io_recv < 0:
            disk_iops = 0
            net_io_sent = 0
            net_io_recv = 0

        else: # we don't have the previous state, so we can only calculate the average over the lifetime of the box

            # disk iops is the amount of reads and writes in the last n seconds divided by n
            disk_iops = 1000*(psutil.disk_io_counters().read_count + psutil.disk_io_counters().write_count) \
                / (psutil.disk_io_counters().read_time + psutil.disk_io_counters().write_time)

            # we can't calculate net bps, because psutil has no time metric for network, leave bps as 0

        net_io_sent = int(net_io_sent)
        net_io_recv = int(net_io_recv)
        # find the max disk usage for all non-pipedream partitions
        max_other_disk_usage = 0
        other_disks = psutil.disk_partitions()
        for p in other_disks:
            if (p.mountpoint != deepy.cfg.home_dir):
                if (psutil.disk_usage(p.mountpoint).percent) > max_other_disk_usage:
                    max_other_disk_usage = psutil.disk_usage(p.mountpoint).percent


        result = \
        {
            "pipedream_disk_usage": psutil.disk_usage(deepy.cfg.home_dir).percent,
            "max_other_disk_usage": max_other_disk_usage,
            "net_io_sent": net_io_sent,
            "net_io_recv": net_io_recv,
            "disk_iops": disk_iops,
            "uptime": now - psutil.BOOT_TIME,
            "memory_per": psutil.virtual_memory().percent,
            "percent_swap_used": psutil.swap_memory().percent,
            "swap_disabled": psutil.swap_memory().total == 0
        }

        # Get the information & put it in the format we expect
        disk_io_counters = psutil.disk_io_counters()
        disk_io_counters = {
            "read_count": disk_io_counters.read_count,
            "write_count": disk_io_counters.write_count
        }

        net_io_counters = psutil.net_io_counters()
        net_io_counters = {
            "bytes_sent": net_io_counters.bytes_sent,
            "bytes_recv": net_io_counters.bytes_recv
        }

        net_io_counters_per_interface = psutil.net_io_counters(pernic=True)
        net_io_per_interface = {}
        for interface in net_io_counters_per_interface:
            net_io_per_interface[interface] = net_io_counters_per_interface[interface]

        current_state = \
        {
            "current_time": now,
            "disk_io_counters": disk_io_counters,
            "net_io_counters": net_io_counters,
            "net_io_per_interface": net_io_per_interface
        }

        deepy.store.simple_save_json(current_state, deepy.cfg.system_state)


        return result

    def publish(self, metric, client=None):
        prefix = 'metric/vm/system/'
        if metric is None:
            deepy.log.warning("Failed to collect SystemInfoMetric")
            return

        for k, v in metric.iteritems():
            name = prefix + k
            if k == 'swap_disabled':
                self.send('event/vm/system/' + k,
                    metric[k],
                    tags=['persist', 'heartbeat'],
                    client=client)
            elif k == 'uptime':
                self.send('metric/debug/vm/' + k,
                    metric[k],
                    tags=['persist', 'heartbeat', 'debug'],
                    client=client)
            else:
                self.send(
                    name,
                    metric[k],
                    tags=['persist', 'heartbeat'],
                    client=client
                )

    def snapshot(self, client=None):
        self.send_snapshot('sysinfo')

class TrafficMetric(Metric):

    tags = ['traffic']

    def collect(self):
        heartbeat_config = deepy.status.get_heartbeat_config()
        cube_type = heartbeat_config.get('traffic_metric_cube_type','drill_small')
        end = datetime.datetime.utcnow()
        begin = end - datetime.timedelta(hours=1)
        file_str = os.path.join(deepy.cfg.cubes_dir, cube_type, 'hours', "cube.%Y-%m-%d-%H.h5")
        frange = deepy.timerange.generate_file_glob(file_str, begin, end, '1H')

        for filepath in reversed(frange):
            try:
                return deepy.cube.CubeLoader(filepath).get_summary()
            except (TypeError, IOError):
                pass
        deepy.log.warning('TrafficMetric: Found no cubes for traffic metric')
        return None

    # XXX temporarily convert all numbers to floats (probably bad, but a temp solution)
    def publish(self, metric, client=None):
        prefix = 'system/traffic/'
        if metric is None:
            deepy.log.warning("Failed to collect TrafficMetric metric")
            return
        for k, v, in metric.iteritems():
            name = prefix + k
            if isinstance(v, list):
                for i in range(len(v)):
                    self.send(
                        name + '/' + str(i), #maybe this should be 'name + '/' + str(i)'?
                        float(v[i]),
                        client=client
                    )
            elif isinstance(v, dict):
                for i, j in v.iteritems():
                    self.send(
                        name + '/' + i,
                        float(j),
                        client=client
                    )
            else:
                self.send(
                    name,
                    float(metric[k]),
                    client=client
                )

    def snapshot(self, client=None):
        self.send_snapshot('gbps')

class BGPMetric(Metric):

    tags = ['bgp']

    def __init__(self, daemon):
        self.daemon = daemon

    def get_bgp_status_bgpd_once(self, timeout=20):
        deepy.log.debug("BGPMetric: Checking bgp")

        output = deepy.util.recursive_default_dict()

        for ip_version in ['ip', 'ipv6']:
            child = pexpect.spawn('nc 127.0.0.1 2605')
            child.setecho(False)

            try:
                child.expect('Password:', timeout=timeout)
            except:
                deepy.log.warning("BGPMetric: Timeout waiting for password")
                return
            child.sendline('default123')

            try:
                child.expect('> ', timeout=timeout)
            except:
                deepy.log.warning("BGPMetric: Timeout waiting for >")
                return

            cmd = 'show %s bgp summary' % ip_version
            child.sendline(cmd)
            child.sendline('quit')

            try:
                child.expect(pexpect.EOF, timeout=timeout)
            except:
                deepy.log.warning("BGPMetric: Timeout waiting EOF (%s)" % cmd)
                return
            buff = child.before

            deepy.log.debug(buff)

            rib_regex = re.compile(r"^RIB entries (?P<rib_entries>[0-9]+)", re.MULTILINE)
            match = rib_regex.search(buff)
            if match and ip_version == "ip":
                output["rib_entries"] = int(match.group("rib_entries"))

            ipv_regex = r"(?P<version>[\d]+)\s+" + \
                        r"(?P<as>[\d]+)\s+" + \
                        r"(?P<msg_recv>[\d]+)\s+" + \
                        r"(?P<msg_sent>[\d]+)\s+" + \
                        r"(?P<tbl_ver>[\d]+)\s+" + \
                        r"(?P<inq>[\d]+)\s+" + \
                        r"(?P<outq>[\d]+)\s+" + \
                        r"(?P<uptime>[\w]+|[\d:]+)\s+" + \
                        r"(?P<state>[\w]+|[\d]+)\s*$"

            ipv4_regex = re.compile(r"^(?P<ip_addr>[0-9\.]+)\s+" + ipv_regex, re.MULTILINE)
            ipv6_regex = re.compile(r"^(?P<ip_addr>[0-9a-fA-F:]+)\s+" + ipv_regex, re.MULTILINE)

            for regex in [ipv4_regex, ipv6_regex]:

                for match in regex.finditer(buff):
                    ip = match.group("ip_addr")

                    if ip_version == "ipv6":
                        if ip in output["neighbors"]:
                            output["neighbors"][ip]['ipv6'] = match.group("state")
                        continue

                    output["neighbors"][ip] = \
                    {
                        "uptime": match.group("uptime"),
                        "as": match.group("as"),
                        "state": match.group("state")
                    }

        return output

    def get_bgp_status_bird_once(self, timeout=20):
        deepy.log.debug("BGPMetric: Checking bgp")

        output = deepy.util.recursive_default_dict()

        pybird = deepy.pybird.PyBird("/usr/local/var/run/bird.ctl")

        status = None
        try:
            status = pybird.get_peer_status()
        except:
            deepy.log.exception("BGPMetric: Failed to get peer status (bird)")
            return None

        #{'router_id': '198.108.63.60', 'export_updates_received': 495136, 'protocol': 'BGP', 'export_updates_rejected': 495136, 'import_updates_ignored': 179, 'import_withdraws_rejected': 0, 'export_updates_accepted': 0, 'import_updates_accepted': 495136, 'import_updates_received': 495315, 'export_updates_filtered': 0, 'import_withdraws_ignored': 0, 'routes_imported': 492960, 'name': 'bgp1', 'import_updates_filtered': 0, 'export_withdraws_accepted': 0, 'import_withdraws_accepted': 694, 'export_withdraws_received': 694, 'up': True, 'state': 'Established', 'routes_exported': 0, 'import_updates_rejected': 0, 'last_change': '', 'import_withdraws_received': 694}

            #"198.164.200.234": {
            #"uptime": "02w5d05h",
            #"as": "855",
            #"state": "495376"
            #},

        #To keep heartbeat from alerting on zero rib entries  populate the number of routes as
        # the sum of all routes imported across all neighbors.
        #My understanding of the check is that we are checking if birdc recently restarted and has no routes.
        #This detects that case but there may be something I'm missing
        # Sandy 6/19/2014

        total = 0
        for line in status:
            try:
                ip = line['router_id']
            except:
                continue

            #State as output by pybird is the timestamp of
            # when the session was established
            #  state: 00:58:26
            #  last_change: 2014-06-19 00:58:25

            #State will still alert if the session
            # remains up for an entire day,
            # but it will only be for one 5min period

            last_change = datetime.datetime.strptime(line['last_change'], "%Y-%m-%d %H:%M:%S")

            diff = datetime.datetime.utcnow() - last_change
            uptime = datetime.datetime(1970, 1, 1) + diff
            uptime_str = uptime.strftime("%H:%M:%S")
            #That is the format heartbeat is checking. If uptime
            # is over a day we can append days on so the alert
            # doesn't trigger

            if diff.days > 0:
                uptime_str = "%iD " % (diff.days) + uptime_str

            routes_imported = line['routes_imported']
            total += routes_imported
            output["neighbors"][ip] = {
                "uptime": uptime_str,
                "as": line['router_as'],
                "state": routes_imported
            }

        output["rib_entries"] = total

        return output

    def collect(self):
        heartbeat_config = deepy.status.get_heartbeat_config()

        # old function arg defaults
        reps = heartbeat_config.get('bgp_metric_bgp_reps', 5)
        timeout = heartbeat_config.get('bgp_metric_timeout', 4)

        bgp_config = heartbeat_config.get('bgp', {})
        timeout = bgp_config.get('timeout', timeout)
        reps = bgp_config.get('reps', reps)

        now = datetime.datetime.utcnow()
        uuid = load_uuid()
        if uuid is None:
            deepy.log.error("BGPMetric: Could not determine UUID")
            return None

        prev_hb = load_previous_heartbeat(now, uuid)

        status = None
        for i in range(reps):
            deepy.log.debug('BGPMetric: get_bgp_status trying {}'.format(i))
            if self.daemon == "bgpd":
                status = self.get_bgp_status_bgpd_once(timeout)
            elif self.daemon == "bird":
                status = self.get_bgp_status_bird_once(timeout)
            else:
                break

            if status:
                if 'bgp' in status:
                    if not prev_hb:
                        prev_hb = {}
                    add_delta(status['bgp'], prev_hb.get('bgp', {}), 'rib_entries')
                break
        return status

    def publish(self, metric, client=None):
        prefix = 'system/bgp/'
        if metric is None:
            deepy.log.warning("BGPMetric: Failed to collect BGPMetric metric")
            return
        for k, v, in metric.iteritems():
            name = prefix + k
            if isinstance(v, list):
                for i in range(len(v)):
                    self.send(
                        name + '/' + str(i), #maybe this should be 'name + '/' + str(i)'?
                        float(v[i]),
                        client=client
                    )
            elif isinstance(v, dict):
                for i, j in v.iteritems():
                    if isinstance(j, dict):
                        # this is ridiculous
                        for x, y in j.iteritems():
                            if x == 'uptime':
                                continue
                            self.send(
                                '{}/{}/{}'.format(name, i, x),
                                float(y),
                                client=client
                            )
                    else:
                        self.send(
                            name + '/' + i,
                            float(j),
                            client=client
                        )
            else:
                self.send(
                    name,
                    float(metric[k]),
                    client=client
                )

    def snapshot(self, client=None):
        self.send_snapshot('bgp')

class DayTrafficMetric(Metric):
    tags = ['daytraffic']

    def __init__(self, dimensions_db=None):
        self.dimensions_db = dimensions_db if dimensions_db else deepy.dimensions.DimensionsDB()

    def collect(self):
        if not os.path.exists(deepy.cfg.cubes_dir):
            deepy.log.warning('DayTrafficMetric: Cubes dir does not exist: {}'.format(deepy.cfg.cubes_dir))
            return None

        now = datetime.datetime.utcnow()
        cube_template = "cube.%Y-%m-%d-%H.h5"
        template = os.path.join(deepy.cfg.cubes_dir, "drill1", "hours", cube_template)

        #XXX Sandy 06/20/2014
        # Start was set to floor(now, "1D") - timedelta(days=1)
        # This means we check close to 2 days during the end of the day and only a day near 0 UTC
        # I've changed this to always check a days worth. I think this was a bug, but feel free to change
        # if there is some reason that check was needed

        end = deepy.timerange.floor_timestamp_to(now, "1H")
        start = end - datetime.timedelta(days=1)
        input_files = deepy.timerange.generate_file_glob(template, start, end, 'H')
        input_files = [i for i in input_files if os.path.exists(i)] # Only use files that exist

        query = {
            "dimensions": [
                "timestamp"
            ],
            "applies": [
                {
                    "args": [
                        "days"
                    ],
                    "fn": "time_dist"
                }
            ],
            "input_files": input_files
        }

        try:
            cube_query = deepy.cube.CubeQuery(query, ddb=self.dimensions_db)
            query_cube = deepy.cube.cube_map_queries([cube_query], dimensions_db=self.dimensions_db)

            if len(query_cube) == 0:
                deepy.log.warning('DayTrafficMetric: Query of drill1 hours returned no results')
                return None

            unused, cube = query_cube.pop()
            cube = cube.get_dict()
        except deepy.cube.QueryError:
            deepy.log.exception('DayTrafficMetric: Query error for {}'.format(input_files))
            return None

        meta = cube["meta_data"]
        arr = meta["dimensions"] + meta["measures"]
        i_ts = arr.index("timestamp")
        i_sent = arr.index("max.sent.bps")
        i_recv = arr.index("max.recv.bps")
        days = [(x[i_ts], x[i_recv], x[i_sent]) for x in cube["cube"]]

        return days

    def publish(self, metric, client=None):
        prefix = 'system/daytraffic/cube'
        if metric is None:
            deepy.log.warning("Failed to collect DayTrafficMetric")
            return
        for k in range(len(metric)): #these might not actually be different cubes, investigate further XXX
            self.send(prefix + str(k) + '/i_ts', metric[k][0], client=client)
            self.send(prefix + str(k) + '/i_recv', metric[k][1], client=client)
            self.send(prefix + str(k) + '/i_sent', metric[k][2], client=client)

    def snapshot(self, client=None):
        self.send_snapshot('backbone_day_traffic')

class HourTrafficMetric(Metric):
    tags = ['hourtraffic']

    def __init__(self, dimensions_db=None):
        self.dimensions_db = dimensions_db if dimensions_db else deepy.dimensions.DimensionsDB()

    def collect(self):
        if not os.path.exists(deepy.cfg.cubes_dir):
            return None

        now = datetime.datetime.utcnow()
        cube_template = "cube.%Y-%m-%d-%H.h5"
        template = os.path.join(deepy.cfg.cubes_dir, "drill1", "hours", cube_template)

        # Get the last hour's traffic since it's pretty likely the current one hasn't been built yet
        end = deepy.timerange.floor_timestamp_to(now - datetime.timedelta(hours=1), "1H")
        start = end - datetime.timedelta(hours=6)
        input_files = deepy.timerange.generate_file_glob(template, start, end, 'H')

        input_files = [i for i in input_files if os.path.exists(i)]
        if len(input_files) == 0:
            deepy.log.warning('HourTrafficMetric: Could not find any drill1 hours')
            return None

        input_files = [input_files[-1]]

        query = {
            "dimensions": [
                "timestamp"
            ],
            "applies": [
                {
                    "args": [
                        "hours"
                    ],
                    "fn": "time_dist"
                }
            ],
            "input_files": input_files
        }

        try:
            cube_query = deepy.cube.CubeQuery(query, ddb=self.dimensions_db)
            query_cube = deepy.cube.cube_map_queries([cube_query], dimensions_db=self.dimensions_db)

            if len(query_cube) == 0:
                deepy.log.warning('HourTrafficMetric: Query of drill1 hours returned no results')
                return None

            unused, cube = query_cube[0] # Only one query, only one item should be returned
            cube = cube.get_dict()
        except deepy.cube.QueryError:
            deepy.log.exception('HourTrafficMetric: Querying {}'.format(input_files))
            return None

        meta = cube["meta_data"]
        arr = meta["dimensions"] + meta["measures"]
        i_ts = arr.index("timestamp")
        i_sent = arr.index("max.sent.bps")
        i_recv = arr.index("max.recv.bps")
        hours = [(x[i_ts], x[i_recv], x[i_sent]) for x in cube["cube"]]

        return hours

    def publish(self, metric, client=None):
        prefix = 'system/hourtraffic/cube'
        if metric is None:
            deepy.log.warning("Failed to collect HourTrafficMetric")
            return
        for k in range(len(metric)): #these might not actually be different cubes, investigate further XXX
            self.send(prefix + str(k) + '/i_ts', metric[k][0], client=client)
            self.send(prefix + str(k) + '/i_recv', metric[k][1], client=client)
            self.send(prefix + str(k) + '/i_sent', metric[k][2], client=client)

    def snapshot(self, client=None):
        self.send_snapshot('backbone_hour_traffic')

class RouterLookupMetric(Metric):

    tags = ['router']

    def recursive_parse(self, prefix, left, right, client):
        if not isinstance(right, dict):
            if isinstance(right, list):
                for i in range(len(right)):
                    self.send(prefix + '/' + str(i), client=client, data=right[i])
                    #print prefix, right
            else:
                self.send(prefix, client=client, data=right)
                #print prefix, right
        else:
            for i, j in right.iteritems():
                self.recursive_parse(prefix + '/' + i, i, j, client)

    def collect(self):
        router_dict = {}
        aws_path = "dimensions/router.json.gz"
        local_path =  deepy.cfg.cache_dir + "/" + aws_path
        dim_db = deepy.dimensions.DimensionsDBFrag(db_file=local_path)

        if not dim_db:
            deepy.log.warning('RouterLookupMetric: Could not load routers dimension')
            return None

        for pos_id, vals in dim_db["115"]["positions"].iteritems():
            try:
                flow_ip =  vals['attributes']['computed']['router']['flow_ip']
            except:
                continue

            #print flow_ip, vals['name']
            vals["pos_id"] = pos_id
            router_dict[flow_ip] = vals

        if not router_dict:
            deepy.log.warning('RouterLookupMetric: No (valid) router positions')
            return None
        return router_dict

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect RouterLookupMetric")
            return
        prefix = 'pipedream/router'
        for k, v in metric.iteritems():
            self.recursive_parse(prefix, k, v, client)

    def snapshot(self, client=None):
        self.send_snapshot('router_lookup')

class MissingBundlesMetric(Metric):

    tags = ['missingbundles']

    def format(self, data={}):
        if not data:
            return {}

        n = {}
        for k,v in data.iteritems():
            n["missing_bundles:{0}".format(k)] = ("gauge", 1)

        return n

    def _want_bundle(self, rule_name, make_rule):
        if make_rule.get('ignore_missing', False):
            deepy.log.debug("MissingBundlesMetric: ignore_missing: " + rule_name)
            return None

        if make_rule.get('type') != 'bundle':
            deepy.log.debug("MissingBundlesMetric: not bundle: " + rule_name)
            return None

        recipe = make_rule.get('recipe')
        if not recipe:
            return None

        # XXX: A somewhat arbitrary list of bundles Joe and I decided to alert
        # on. These are bundles that all deployments should be running and also
        # should be representative of bundles as a whole. I.e. if these bundles
        # are failing we're probably having issues with others as well. We
        # decided to do this to try and reduce the number of false positives on
        # the status dashboard.
        bundle_alert_list = ["cdn", "router", "service"]
        if not any([l in rule_name for l in bundle_alert_list]):
            return None

        for r in recipe:
            if 'marker' in r:
                return make_rule
        return None

    def collect(self):
        '''
        Finds all bundles that were not made within the specified number of hours.

        Return: {bundle_name: message}

        Returns an empty result if all bundles are up-to-date.
        '''
        heartbeat_config = deepy.status.get_heartbeat_config()
        hours = heartbeat_config.get('missing_bundles_metric_hours', 10)
        rules_db = deepy.build.deepy_util.construct_rules()
        bundles = []
        for rule_id, rule in rules_db.iteritems():
            if self._want_bundle(rule_id, rule):
                bundles.append((rule_id, rule))
        out = {}
        now = datetime.datetime.utcnow()
        for bundle_id, bundle in bundles:
            template = None
            for r in bundle.get('recipe'):
                if 'marker' in r:
                    args = parse_bundle2(r.split()[1:])
                    if args:
                        template = args.marker_file

            if not template:
                continue

            # Look for the current and previous step, in case we crossed the day/month boundary
            delta = {
                    '1d': datetime.timedelta(days=1),
                    'month': dateutil.relativedelta.relativedelta(months=1)
            }[bundle['file_step']]
            end = now
            timerange = [end, end - delta]

            found = False
            for t in timerange:
                almost = deepy.timerange.substitute_timestamp(template, t)
                filepath = almost.replace('$(cubes_dir)', deepy.cfg.cubes_dir)

                deepy.store.cache_load_from_remote(filepath)
                if not os.path.exists(filepath):
                    continue

                mtime = datetime.datetime.fromtimestamp(os.path.getmtime(filepath))
                name = filepath[len(deepy.cfg.cubes_dir):]
                name = '/'.join(name.split('/')[1:-2])
                if now - mtime > datetime.timedelta(hours=hours):
                    deepy.log.debug("MissingBundlesMetric: *** {} ({}) is {} old".format(name, bundle_id, now - mtime))
                    out[name] = "No %s bundle made for the last %d hours." % (name, hours)
                else:
                    deepy.log.debug("MissingBundlesMetric: {} ({}) is less than {} hours old".format(name, bundle_id, hours))
                found = True
                break

            if not found:
                deepy.log.debug("MissingBundlesMetric: *** ({}) not found".format(bundle_id))
                out[bundle_id] = "No %s bundle found." % (bundle_id)

        return out

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect MissingBundlesMetric")
            return
        prefix = 'pipedream/missingbundles/'
        for k, v in metric.iteritems():
            self.send(prefix + k, data=v, client=client)

    def snapshot(self, client=None):
        self.send_snapshot('bundles_missing')

class BundleStatusMetric(Metric):

    tags = ['bundlesstatus']

    def format(self, data={}):
        n = {}
        for k,v in data.iteritems():
            n["bundle_status:{0}".format(k)] = ("counter",v["last_modified"])
        return n

    def _want_bundle(self, rule_id, make_rule):
        ignore_bundles = set()

        if rule_id in ignore_bundles:
            return None

        if make_rule.get('type') != 'bundle':
            deepy.log.debug("BundleStatusMetric: not bundle: " + make_rule.rule_name)
            return None

        recipe = make_rule.get('recipe')
        if not recipe:
            return None

        for r in recipe:
            if 'marker' in r:
                return make_rule
        return None

    def collect(self):
        '''
        Returns status for all bundles. Returns a dictionary of dictionaries:

        {
          bundle_name: {
            "last_modified": ,
          }
        }
        '''
        rules_db = deepy.build.deepy_util.construct_rules()
        bundles = []
        for rule_id, rule in rules_db.iteritems():
            if self._want_bundle(rule_id, rule):
                bundles.append((rule_id, rule))
        hours = 24

        out = {}
        for rule_name, bundle in bundles:
            template = None
            for r in bundle.get('recipe'):
                if 'marker' in r:
                    args = parse_bundle2(r.split()[1:])
                    if args:
                        template = args.marker_file

            if not template:
                continue

            now = datetime.datetime.utcnow()
            end = deepy.timerange.floor_timestamp_to(now, 'D')
            begin = end - datetime.timedelta(hours=hours)
            timerange = arrow.range('D', begin, end)

            filepath = template.replace('$(cubes_dir)', deepy.cfg.cubes_dir)
            name = filepath[len(deepy.cfg.cubes_dir):]
            name = '/'.join(name.split('/')[1:-2])

            last_modified = None
            for t in reversed(timerange):
                almost = deepy.timerange.substitute_timestamp(template, t)
                filepath = almost.replace('$(cubes_dir)', deepy.cfg.cubes_dir)

                deepy.store.cache_load_from_remote(filepath)
                if not os.path.exists(filepath):
                    continue

                mtime = time.mktime(datetime.datetime.utcfromtimestamp(os.path.getmtime(filepath)).timetuple())
                last_modified = mtime
                break

            out[name] = {
                "last_modified": last_modified
            }
        return out

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect BundleStatusMetric")
            return
        prefix = 'pipedream/bundlesstatus/'
        for k, v in metric.iteritems():
            name = prefix + k + '/'
            for i, j in v.iteritems():
                self.send(name + i, j, client=client)

    def snapshot(self, client=None):
        self.send_snapshot('bundle_status')

class FlowsStatsMetric(Metric):

    tags = ['heartbeat', 'flows', 'persist', 'debug']

    def format(self, data={}):
        return {
            "flow_stats:flow_count": ("counter",data["count"]),
            "flow_stats:size": ("gauge", data["size"]),
            "flow_stats:avgerage_size": ("gauge", data["avg"])
        }

    def collect(self):
        count = len(os.listdir(deepy.cfg.flows_dir))
        size = get_dir_size(deepy.cfg.flows_dir)
        if count == 0:
            deepy.log.warning('FlowsStatsMetric: No flows found on vm')
            return {}
        return { "count": count, "size": size, "avg": size / float(count) }

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect FlowsStatsMetric")
            return
        prefix = 'metric/debug/deployment/flows/'
        for k, v in metric.iteritems():
            self.send(prefix + k, v, client=client)

    def snapshot(self, client=None):
        self.send_snapshot('flows')

class SyslogMetric(Metric):
    tags = ['heartbeat', 'syslog', 'debug']

    def collect(self):
        # I've changed this to only grab lines that match critical or traceback, and to abort faster.
        # The old algorithm was like n^2 on error count.

        max_lines = 500
        lines = []
        alarming = re.compile(r"(CRITICAL|Traceback)")

        with open('/var/log/syslog') as fd:
            for l in fd:
                if re.search(alarming, l):
                    lines.append(l)
                    if len(lines) > max_lines:
                        break;
        return [l.replace("#012", "\n") for l in lines]

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect SyslogMetric")
            return
        prefix = 'metric/debug/vm/syslog'
        self.send(prefix, data=len(metric), client=client)

    def snapshot(self, client=None):
        self.send('syslog')

class DirMetric(Metric):
    tags = ['dir']

    def collect(self):
        heartbeat_config = deepy.status.get_heartbeat_config()
        directory_size_threshold = heartbeat_config.get('dir_metric_directory_size_threshold', 0)
        dirs = [
            '/var',
            '/tmp',
            os.path.expanduser("~")
        ]
        err = []

        for d in dirs:
            size = (get_dir_size(d) / float(pow(1024, 3)))
            if size > directory_size_threshold: #default is 10 GB
                err.append({"directory": d, "size": size})

        return err if err else None

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect DirMetric")
            return
        prefix = 'system/dir/'
        for i in range(len(metric)):
            name = prefix + str(i) + '/'
            for k, v in metric[i].iteritems():
                if isinstance(v, basestring):
                    self.send(name + k, data=v, client=client)
                else:
                    self.send(name + k, v, client=client)

    def snapshot(self, client=None):
        self.send_snapshot('large_dirs')

class IPMetric(Metric):
    tags = ['ip']

    def collect(self):
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        try:
            s.connect(("8.8.8.8", 80))
        except:
            return "N/A"
        addr, port = s.getsockname()
        s.close()
        return addr.strip()

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect IPMetric")
            return
        prefix = 'system/ip'
        self.send(prefix, meta={'ip': metric}, client=client)

    def snapshot(self, client=None):
        self.send_snapshot('ip')

class HeartbeatMetric(Metric):
    tags = ['ping']

    def collect(self):
        return time.time()

    def publish(self, metric, client):
        self.send('heartbeat', metric, client=client)

class UILogMetric(Metric):

    tags = ['uilog']

    def format(self,data={}):
        return {
            "ui_failures" : ("counter", data["fails"] if data["fails"] is not None else 0),
        }

    def collect(self):
        ui_log = {"log": None, "fails": None, "use": None}

        deepy.log.debug("UILogMetric: Checking UI log")

        ui_login_lines = []
        ui_login_fails = []

        log_files = [
            deepy.cfg.ui_log,
            deepy.cfg.ui_log + ".1",
        ]

        metaDataLog = r"(?P<date>^[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2},[0-9]{3}) home.py\[\d+\]\[\S+\]: "
        user_regex = re.compile(metaDataLog + r"GET (?P<page>\S+) from \S+ \((?P<user>\S+)\) at (?P<ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})", re.MULTILINE)
        attempt_regex = re.compile(metaDataLog + r"Attempt to log in from IP (?P<ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})" + \
            " failed for username (?P<user>\S+)", re.MULTILINE)

        bad_strings = ['support', 'none', 'None']
        now = datetime.datetime.utcnow()
        for log_file in log_files:
            if os.path.isfile(log_file):
                file_buffer = open(log_file, 'r').read()
            elif os.path.isfile(log_file + ".gz"):
                file_buffer = gzip.open(log_file + ".gz").read()
            else:
                continue

            # Check for all users in last 5 days
            for match in user_regex.finditer(file_buffer):
                if any(string in match.group(0) for string in bad_strings):
                    continue
                if "dashboard" in match.group("page"):
                    # FIXME: Checking if user visited the dashboard is bad way to check
                    # for logging in, find a better way
                    continue
                date = datetime.datetime.strptime(match.group("date"), "%Y-%m-%d %H:%M:%S,%f")
                delta = now - date
                if delta.days <= 5:
                    ui_login_lines.append(match.group(0))

            #check for any login failures in last 5 days
            for match in attempt_regex.finditer(file_buffer):
                if any(string in match.group(0) for string in bad_strings):
                    continue
                date = datetime.datetime.strptime(match.group("date"), "%Y-%m-%d %H:%M:%S,%f")
                delta = now - date
                if delta.days <= 5:
                    ui_login_fails.append(match.group(0))

        if ui_login_lines:
            # Take only most recent 200
            ui_login_lines = ui_login_lines[-200:]
            ui_log['log'] = "".join(ui_login_lines)
        else:
            ui_log['use'] = "No Logins in 5 days"

        if ui_login_fails:
            if len(ui_login_fails) > 20:
                ui_log['fails'] = ui_login_fails

        return ui_log

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect UILogMetric")
            return
        prefix = 'pipedream/ui/'
        for k, v in metric.iteritems():
            self.send(prefix + k, data=v, client=client)

    def snapshot(self, client=None):
        self.send_snapshot('ui_log')

class FindMissingMapperFilesMetric(Metric):

    tags = ['missingmappers']

    def format(self, data={}):
        n = {}
        for k,v in data.iteritems():
            n["missing_mapper_files:exists_{0}".format(k)] = ("counter",int(not v["exists"]))
            n["missing_mapper_files:mtime_{0}".format(k)] = ("counter", v["last_modified"])
        return n

    def collect(self):
        deployment_id = deepy.cfg.deployment_id
        now = datetime.datetime.utcnow()
        filenames = \
        {
            # Don't do these for now, looks like they haven't been used in a while
            'benchmark': now.strftime('benchmarks_new2/benchmark.%Y-%m-%d.json.gz'),
            'localmap': 'maps/localmap.h5',
            'genome': 'maps/genome.h5'
        }

        good_ids = \
        [
            "ritter",
            "bca",
            "twc",
            "merit"
        ]

        result = {}
        ls_filenames = [os.path.join(deepy.cfg.cache_dir, filename) for filename in filenames.values()]
        temp = deepy.store.ls_files_remote(ls_filenames)

        for mapper, filename in filenames.iteritems():
            # Benchmark mapper doesn't run on all deployments
            if mapper == "benchmark" and deployment_id not in good_ids:
                continue
            local_path = os.path.join(deepy.cfg.cache_dir, filename)
            mtime = temp.get(local_path)
            if mtime:
                result[mapper] = {"exists": True, "last_modified": mtime}
            else:
                result[mapper] = {"exists": False}

        return result

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect MissingMapperFilesMetric")
            return
        prefix = 'pipedream/missingmappers/'
        for k, v in metric.iteritems():
            name = prefix + k + '/'
            for i, j in v.iteritems():
                self.send(name + i, j, client=client)

    def snapshot(self, client=None):
        self.send_snapshot('mapper_files')

class FindMissingMineFilesMetric(Metric):

    tags = ['missingmines']
    def collect(self):
        deployment_id = deepy.cfg.deployment_id
        heartbeat_config = deepy.status.get_heartbeat_config()
        hours = heartbeat_config.get('find_missing_missing_mine_files_metric_hours', 6)

        mine_files_config = heartbeat_config.get('mine_files', {})
        hours = mine_files_config.get('hours', hours)

        filenames = \
        {
            'https': 'mine/https/https.%Y-%m-%d-%H.txt.gz',
            'reverse': 'mine/reverse/reverse.%Y-%m-%d-%H.txt.gz',
            'webserver': 'mine/webserver/webserver.%Y-%m-%d-%H.txt.gz'
        }

        bucket = deepy.store.s3_get_bucket(deployment_id)

        # The files are labled 2 hours behind (also give a 1 hour grace period)
        now = datetime.datetime.utcnow() - datetime.timedelta(hours=3)
        end = deepy.timerange.floor_timestamp_to(now, 'H')
        start = end - datetime.timedelta(hours=hours)

        result = {}
        for mapper, filename in filenames.iteritems():
            files = deepy.timerange.generate_file_glob(filename, start, end, 'H')
            # Do any of the latest mine files exist?
            if not any(bucket.get_key(k) for k in files):
                result[mapper] = "No %s mine files for the last %d hours." % (mapper, hours)

        return result

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect MissingMineFilesMetric")
            return
        prefix = 'pipedream/missingmines/'
        for k, v in metric.iteritems():
            self.send(prefix + k, data=v, client=client)

    def snapshot(self, client=None):
        self.send_snapshot('mine_files')

# Thes are the *ago metrics
class DataPipelineMetric(Metric):
    tags = ['heartbeat', 'persist']
    def __init__(self, name, tmpl):
        self.name = name
        self.tmpl = tmpl
        self.tags += [self.name]

    def collect(self):
        ds_template = self.tmpl
        # check if the template is simply a filename first
        ret = check_file_mtime(ds_template)
        if ret:
            return ret
        # if not, then use a timerange check
        ret = sample_latest_5min(ds_template)
        if ret:
            return ret
        deepy.log.warning('{}: did not find {}'.format(self.name, self.tmpl))
        return None

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect {}".format(self.name))
            return
        prefix = 'metric/deployment/data/{}'.format(self.name)
        self.send(prefix, metric, client=client)

    def snapshot(self, client=None):
        self.send_snapshot(self.name)


class DrillSmallAgoMetric(DataPipelineMetric):
    def format(self,data):
        return {
            "drill_small_ago": ("counter", data)
        }
    def __init__(self):
        super(DrillSmallAgoMetric, self).__init__("drillsmallago", os.path.join(deepy.cfg.cubes_dir, 'drill_small', 'minutes', "cube.%Y-%m-%d-%H-%M.h5"))

class Drill1AgoMetric(DataPipelineMetric):
    def format(self,data):
        return {
            "drill_1_ago": ("counter", data)
        }
    def __init__(self):
        super(Drill1AgoMetric, self).__init__("drill1ago", os.path.join(deepy.cfg.cubes_dir, 'drill1', 'minutes', "cube.%Y-%m-%d-%H-%M.h5"))

class H5FlowAgoMetric(DataPipelineMetric):
    def format(self,data):
        return {
            "h5flow_ago": ("counter", data)
        }
    def __init__(self):
        super(H5FlowAgoMetric, self).__init__("h5flowago", os.path.join(deepy.cfg.h5flow_dir, "flow.%Y-%m-%d-%H-%M.h5"))

class H5BGPAgoMetric(DataPipelineMetric):
    def format(self,data):
        return {
            "h5gpg_ago": ("counter", data)
        }

    def __init__(self):
        super(H5BGPAgoMetric, self).__init__("h5bgpago", os.path.join(deepy.cfg.bgp_dir, "bgp.h5"))

class BackboneSmallAgoMetric(DataPipelineMetric):
    def format(self,data):
        return {
            "backbone_small_ago": ("counter", data)
        }
    def __init__(self):
        super(BackboneSmallAgoMetric, self).__init__("backbonesmallago", os.path.join(deepy.cfg.cubes_dir, 'backbone_small', 'minutes', "cube.%Y-%m-%d-%H-%M.h5"))

# These are the *gaps metrics
class DataPipelineGapsMetric(Metric):
    tags = ['heartbeat', 'persist']
    def __init__(self, name, cube, step, seconds_back=86400):
        self.name = name
        self.tags += [self.name]
        self.cube = cube
        self.step = step
        self.seconds_back = seconds_back

    def collect(self):
        cube = self.cube
        step = self.step
        seconds_back = self.seconds_back
        ret = find_gaps(cube, step, seconds_back)
        if ret:
            return ret
        deepy.log.warning('{}: did not find {}'.format(self.name, self.cube))
        return None

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect {}".format(self.name))
            return
        prefix = 'metric/deployment/data/{}'.format(self.name)
        self.send(prefix, metric, client=client)

    def snapshot(self, client=None):
        self.send_snapshot(self.name)

class DrillSmallGapsMetric(DataPipelineGapsMetric):
    def __init__(self):
        super(DrillSmallGapsMetric, self).__init__("drillsmallgaps", 'drill_small', '5min')

class Drill1GapsMetric(DataPipelineGapsMetric):
    def __init__(self):
        super(Drill1GapsMetric, self).__init__("drill1gaps", 'drill1', '5min')

class H5FlowGapsMetric(DataPipelineGapsMetric):
    def __init__(self):
        super(H5FlowGapsMetric, self).__init__("h5flowgaps", 'classify_h5flow', '5min', 3600*4)

class BackboneSmallGapsMetric(DataPipelineGapsMetric):
    def __init__(self):
        super(BackboneSmallGapsMetric, self).__init__("backbonesmallgaps", 'backbone_small', '5min')

class DNSStatusMetric(Metric):

    tags = ['heartbeat', 'debug', 'status', 'dns']

    def collect(self):
        if not is_vm_running_daemon("flowd"):
            deepy.log.warning('DNSStatusMetric: flowd is not running on vm')
            return None

        dns_status = {}

        temp = deepy.util.recursive_default_dict()
        end = deepy.timerange.floor_timestamp_to(datetime.datetime.now(), '5T')
        begin = end - datetime.timedelta(hours=1)

        # Real-time
        if deepy.util.vm_or_slice_config_get("dnsflow_period_seconds"):
            file_str = os.path.join(deepy.cfg.h5dns_dir, 'dns.%Y-%m-%d-%H-%M-%S.h5')
            files = deepy.timerange.generate_file_glob(file_str, begin, end, '10 seconds')
            duration = 10.0
        # Five minutes
        else:
            file_str = os.path.join(deepy.cfg.h5dns_dir, 'dns.%Y-%m-%d-%H-%M.h5')
            files = deepy.timerange.generate_file_glob(file_str, begin, end, '5T')
            duration = 300.0

        files.reverse()
        for filename in files:

            if not os.path.exists(filename) or not os.stat(filename).st_size:
                continue

            try:
                h5file = tables.open_file(filename)
            except IOError:
                continue

            for row in h5file.root.dns.resolvers:
                ip = '%s' % ipaddr.IPv4Address(socket.ntohl(row['ip']))

                dns_status[ip] = {
                    'dns_flow': round(row['num_queries'], 0),
                    'dnsflow_fps': round(float(row['num_queries']) / duration, 0),
                    'bytes_queries': round(row['bytes_queries'], 0),
                    'num_clients': round(row['num_clients'], 0)
                }

            h5file.close()
            break

        dns_status.update(temp)
        dns_status['total'] = {
            'num_queries': 0,
            'dnsflow_fps': 0
        }
        for ip in dns_status:
            if ip == 'total':
                continue
            dns_status['total']['num_queries'] += dns_status[ip]['dns_flow']
            dns_status['total']['dnsflow_fps'] += dns_status[ip]['dnsflow_fps']
        return dns_status

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect DNSStatusMetric")
            return
        prefix = 'metric/debug/deployment/dns/'
        for k, v in metric.iteritems():
            name = prefix + k + '/'
            if k == 'total':
                name = 'metric/deployment/dns/'
            for i, j in v.iteritems():
                self.send(name + i, j, client=client)

    def snapshot(self, client=None):
        self.send_snapshot('dns')

class CoreFilesMetric(Metric):

    tags = ['corefiles']

    def format(self, data=[]):
        return {"core_dumps": ("counter", len(data))}

    def collect(self):
        dir = '/var/coredumps/'
        return glob.glob(os.path.join(dir, '*'))

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect CoreFilesMetric")
            return
        prefix = 'pipedream/coredumps/'
        for i in range(len(metric)):
            self.send(prefix + i, data = metric[i], client=client)

    def snapshot(self, client=None):
        self.send_snapshot('core_dumps')

class BackboneErrorMetric(Metric):

    tags = ['backboneerror']

    def __init__(self, dimensions_db=None, day_traffic=None):
        self.dimensions_db = dimensions_db if dimensions_db else deepy.dimensions.DimensionsDB()
        self.traffic = day_traffic[0] if day_traffic else None

    def collect(self):
        if not self.traffic:
            day_traffic = deepy.metrics.DayTrafficMetric(self.dimensions_db).collect()
            if not day_traffic:
                return None
            self.traffic = day_traffic[0]
        unused, today_recv, today_sent = self.traffic

        # Todays traffic
        uuid = load_uuid()
        if uuid is None:
            deepy.log.error("BackboneErrorMetric: Could not determine UUID")
            return None

        heartbeat_config = deepy.status.get_heartbeat_config()
        min_ratio = heartbeat_config.get('backbone_metric_min_ratio', 0.5)
        max_ratio = heartbeat_config.get('backbone_metric_max_ratio', 2)

        now = datetime.datetime.utcnow()
        end = deepy.timerange.floor_timestamp_to(now, "5T")
        begin = end - datetime.timedelta(days=7)
        file_str = os.path.join(deepy.cfg.heartbeat_dir, str(uuid), "vm", "vm.%Y-%m-%d-%H-%M.json.gz")
        timerange = deepy.timerange.generate_file_glob(file_str, begin, end, "1D")

        total_sent = []
        total_recv = []

        # Traffic for a week prior
        for path in timerange:
            stat = deepy.store.simple_load_json(path)
            if not stat or 'backbone_day_traffic' not in stat or not stat['backbone_day_traffic']:
                continue
            current_backbone = stat["backbone_day_traffic"][0]
            unused, current_recv, current_sent = current_backbone[:3]
            if current_recv < 0 or current_sent < 0: # Negative traffic...?
                continue
            total_sent.append(current_sent)
            total_recv.append(current_recv)

        if len(total_sent) == 0 or len(total_recv) == 0:
            deepy.log.warning('BackboneErrorMetric: sent or recv is zero')
            return None

        # Using the median so we can be more resistant to spikes in the past
        median_sent = sorted(total_sent)[len(total_sent) / 2]
        median_recv = sorted(total_recv)[len(total_recv) / 2]
        ratio_sent = today_sent / median_sent if median_sent else 0
        ratio_recv = today_recv / median_recv if median_recv else 0

        err = {}
        convert_to_gb = lambda x: int(x / pow(1024, 3))

        if not min_ratio < ratio_sent < max_ratio:
            err["Sent"] = "%.2fx normal traffic (%dGb today / %dGb median)" % \
                (ratio_sent, convert_to_gb(today_sent), convert_to_gb(median_sent))
        if not min_ratio < ratio_recv < max_ratio:
            err["Recv"] = "%.2fx normal traffic (%dGb today / %dGb median)" % \
                (ratio_recv, convert_to_gb(today_recv), convert_to_gb(median_recv))

        return err if err else {}

    def publish(self, metric, client=None):
        prefix = 'pipedream/backbone_error/'
        if metric is None:
            deepy.log.warning("Failed to collect BackboneErrorMetric metric")
            return
        for k, v in metric.iteritems():
            self.send(prefix + k, data=metric, client=client)

    def snapshot(self, client=None):
        self.send_snapshot('traffic_error')

class BackboneChangeMetric(Metric):
    tags = ['backbonchange']

    def __init__(self, dimensions_db=None, hour_traffic=None):
        self.dimensions_db = dimensions_db if dimensions_db else deepy.dimensions.DimensionsDB()
        self.traffic = hour_traffic[0] if hour_traffic else None

    def collect(self):
        uuid = load_uuid()
        if uuid is None:
            deepy.log.error("BackboneChangeMetric: Could not determine UUID")
            return None

        # Todays traffic
        if not self.traffic:
            hour_traffic = deepy.metrics.HourTrafficMetric(self.dimensions_db).collect()
            if not hour_traffic:
                return None
            self.traffic = hour_traffic[0]

        unused, today_recv, today_sent = self.traffic

        now = datetime.datetime.utcnow()
        end = deepy.timerange.floor_timestamp_to(now, "1H")
        begin = end - datetime.timedelta(hours=1)
        file_str = os.path.join(deepy.cfg.heartbeat_dir, str(uuid), "vm", "vm.%Y-%m-%d-%H-%M.json.gz")
        timerange = deepy.timerange.generate_file_glob(file_str, begin, end, "1H")

        previous_path, now_path = timerange

        if previous_path is None:
            deepy.log.warning('BackboneChangeMetric: failed timerange for previous heartbeat')
            return None

        previous_heartbeat = deepy.store.simple_load_json(previous_path)
        if not previous_heartbeat:
            deepy.log.warning('BackboneChangeMetric: no previous heartbeat')
            return None

        pbbht = previous_heartbeat.get('backbone_hour_traffic')
        if not pbbht:
            deepy.log.warning('BackboneChangeMetric: no previous backbone_hour_traffic')
            return None

        previous_backbone = pbbht[0]
        unused, previous_recv, previous_sent = previous_backbone[:3]

        return {
            "delta_recv": today_recv - previous_recv,
            "delta_sent": today_sent - previous_sent,
            "ratio_recv": today_recv/previous_recv if previous_recv != 0 else None,
            "ratio_sent": today_sent/previous_sent if previous_sent != 0 else None,
        }

    def publish(self, metric, client=None):
        prefix = 'pipedream/backbone/'
        if metric is None:
            deepy.log.warning("Failed to collect BackboneChangeMetric")
            return
        for k, v in metric.iteritems():
            name = prefix + k
            self.send(name, v, client=client)

    def snapshot(self, client=None):
        self.send_snapshot('traffic_change')

class ProcessesByMemoryMetric(Metric):
    tags = ['processes']

    def collect(self):
        heartbeat_config = deepy.status.get_heartbeat_config();
        count = heartbeat_config.get('process_by_memory_metric_count', 20)

        try:
            top_N = sorted(psutil.get_process_list(), key=lambda x: x.get_memory_info().vms, reverse=True)[:count]
            processes = [(' '.join(process.cmdline), process.get_memory_info().vms, process.pid) for process in top_N]
        except psutil.NoSuchProcess:
            processes = []
        return processes

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect ProcessesByMemoryMetric")
            return
        prefix = 'system/processes/'
        for i in range(len(metric)):
            self.send(prefix + str(i) + '/process', data=str(metric[i][0]), client=client)
            self.send(prefix + str(i) + '/memory.vms', metric[i][1], client=client)
            self.send(prefix + str(i) + '/process.pid', metric[i][2], client=client)

    def snapshot(self, client=None):
        self.send_snapshot('top_mem_procs')

class FlowdStatsMetric(Metric):
    tags = ['flowdstats']

    def combine_flowd_stats(self, names):
        '''
        { "eth0": {"name": "eth0", "ps_recv": 11424, "ps_drop": 0, "ps_ifdrop": 0} }
        '''

        parts =[deepy.store.simple_load_json(name) for name in names]

        if not parts:
            return None

        out = {}
        for part in parts:
            for key in part.keys():
                out[key] = {'name':key, 'ps_recv': 0, 'ps_drop': 0, 'ps_ifdrop': 0}

        for part in parts:
            for key, part_data in part.items():
                out[key]['ps_recv'] += part_data['ps_recv']
                out[key]['ps_drop'] += part_data['ps_drop']
                out[key]['ps_ifdrop'] += part_data['ps_ifdrop']
        return out

    def collect(self):
        '''
        Loads the contents of /pipedream/cache/status/flowd.json and returns it.

        None if the file doesn't exist.

        { "eth0": {"name": "eth0", "ps_recv": 11424, "ps_drop": 0, "ps_ifdrop": 0} }

        '''
        heartbeat_config = deepy.status.get_heartbeat_config()
        range_num = heartbeat_config.get('flowd_stats_metric_range_num', 6)
        flowd_stats = None

        now = int(time.time())
        now -= (now % 300)
        for i in range(range_num):
            now_tm = time.gmtime(now)
            out = deepy.h5flowutil.find_pcap_files(now_tm, 'status')
            if out:
                names, _ = out
                flowd_stats = self.combine_flowd_stats(names)
                break
            now -= 300

        if not flowd_stats:
            local_path = os.path.join(deepy.cfg.cache_status_dir, "flowd.json")
            flowd_stats = deepy.store.simple_load_json(local_path)

        return flowd_stats

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect FlowdStatsMetric")
            return
        prefix = 'pipedream/flowd/'
        for k, v in metric.iteritems():
            name = prefix + k
            self.send(name + '/name', data=v['name'], client=client)
            self.send(name + '/ps_recv', v['ps_recv'], client=client)
            self.send(name + '/ps_drop', v['ps_drop'], client=client)
            self.send(name + '/ps_ifdrop', v['ps_ifdrop'], client=client)

    def snapshot(self, client=None):
        self.send_snapshot('flowd_pcap')

class MissingCubesMetric(Metric):

    tags = ['missingcubes']

    def format(self, data={}):
        if not data:
            return {}
        n = {}
        for k,v in data.iteritems():
            n["missing_cube_{}".format(k)] = ("counter", 1)
        return n

    def collect(self):
        heartbeat_config = deepy.status.get_heartbeat_config()
        hours = heartbeat_config.get('missing_cubes_metric_hours', 6)

        # N hour interval starting now floored to nearest hour
        now = datetime.datetime.utcnow()
        end = deepy.timerange.floor_timestamp_to(now, '1H')
        begin = end - datetime.timedelta(hours=hours)
        timerange = arrow.range('H', begin, end)
        t0 = datetime.datetime.utcfromtimestamp(0)

        ignored_cubes = [
            "cube_ui_log",      # Not created if no user data
            "backbone_small",    # Can't tell if missing this is OK (backbone uses .cube.date.h5 placeholders)
            "cube_heartbeats"
        ]

        ignored_cubes += heartbeat_config.get("ignored_cubes") or []

        _want_rule_partial = functools.partial(want_rule, ignored_cubes, ('1h', '5min'))
        rules_db = deepy.build.deepy_util.construct_rules()
        make_rules = []
        for rule_id, rule in rules_db.iteritems():
            if _want_rule_partial(rule_id, rule):
                make_rules.append((rule_id, rule))

        out = {}

        config = deepy.cfg.slice_config
        config.update(deepy.cfg.vm_config)

        for rule_id, make_rule in make_rules:
            if make_rule.get('ignore_missing', False):
                continue

            cube_id = make_rule.get('meta').get('cube_id')

            target_expander = deepy.build.deepy_jobs.DeepyDictJob(
                    rule_id, rules_db, config=config)
            targets = []
            for ts in timerange:
                expanded_targets = target_expander.expand(
                    {
                            "start_time": ts,
                            "end_time": ts,
                    }
                )
                for expanded_target in expanded_targets:
                    targets.append(expanded_target.unique_id)
            def generate_dotted_name(filepath):
                fp, filename = os.path.split(filepath)
                return os.path.join(fp, '.'+filename)
            acceptable_missing_targets = [generate_dotted_name(target) for target in targets]
            all_targets = targets + acceptable_missing_targets
            fileset = deepy.store.ls_files_remote(all_targets)

            # Do any of the latest cube files exist?
            if len(fileset) == 0:
                out[cube_id] = "No %s cubes for the last %d hours." % (cube_id, hours)

        return out if out else None

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect MissingCubesMetric")
            return
        prefix = 'pipedream/missingcubes/'
        for k, v in metric.iteritems():
            self.send(prefix + k, data=v, client=client)

    def snapshot(self, client=None):
        self.send_snapshot('cubes_missing')

class UIPageMetric(Metric):
    tags = ['uipage']

    def collect(self):
        heartbeat_config = deepy.status.get_heartbeat_config()
        max_timeout = heartbeat_config.get('ui_page_metric_max_timeout', 60)
        daemons = deepy.cfg.vm_config.get("daemons")
        err = {}
        if not daemons or 'home.py' not in daemons:
            return
        for p in ["https://", "http://"]:
            addr = p + "127.0.0.1/login"
            code = 0
            try:
                code = urllib2.urlopen(addr, timeout=max_timeout).code
            except socket.timeout, e:
                err["timeout"] = max_timeout
            except:
                err["bad_addr"] = addr

            if code >= 400:
                err["bad_code"] = code

        return err

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect UIPageMetric")
            return
        prefix = 'pipedream/uipage/'
        for k, v in metric.iteritems():
            self.send(prefix + k, data = v, client=client)

    def snapshot(self, client=None):
        self.send_snapshot('ui_page')

class CarpMasterMetric(Metric):
    tags = ['carpmaster']

    def __init__(self, carp_ip):
        self.carp_ip = carp_ip

    def collect(self):
        return carp_master(self.carp_ip)

    def publish(self, metric, client=None):
        if metric is None:
            deepy.log.warning("Failed to collect CarpMasterMetric")
            return
        prefix = 'pipedream/carpmaster/'
        self.send(prefix + str(metric), data=metric, client=client)

    def snapshot(self, client=None):
        self.send_snapshot('carp_master')

