import abc
import collections
import copy
import datetime
import dateutil
import multiprocessing
import os
import re
import time
import subprocess
import shlex
import sys

import arrow
import functools
import networkx as nx

import deepy.timerange
arrow = deepy.timerange.arrow_factory
import deepy.make as make
import deepy.log as log
import deepy.store

# depends-one-or-more acts like depends until this much time passes.
DEPENDS_ONE_OR_MORE_TIMEOUT = '12 hours'

def get_now():
    """Returns what is considered now by the job system"""
    return datetime.datetime.utcnow() - datetime.timedelta(minutes=5)


def exist_wrapper(path):
    """A wrapper for the built in for easier testing"""
    return os.path.exists(path)


def path_exists(path):
    """Checks locally for existance of the path if false checks s3 and caches
    the file if it exists"""
    if exist_wrapper(path):
        return True
    if path in deepy.store.ls_files_remote([path]):
        return True
    else:
        return False


def mtime_wrapper(path):
    """A wrapper for the built in for easier testing"""
    return os.stat(path).st_mtime

def getmtime(path):
    """Checks locally for a mtime, if the file doesn't exist it checks s3 and
    caches it locally if it exists"""
    try:
        mtime = mtime_wrapper(path)
        return mtime
    except:
        ls_files = deepy.store.ls_files_remote([path])
        if len(ls_files) == 0:
            raise OSError(2, "No such file or directory", path)
        mtime = ls_files[path]
        return mtime


def get_rules_database():
    """A wrapper for the database construction function"""
    return make.construct_rules()


def benchmark(function):
    """A decorator function that records how long it takes to run a function"""
    def wrapper(*args, **kwargs):
        """The inner function for the decorator"""
        start_time = time.time()
        retval = function(*args, **kwargs)
        end_time = time.time()
        log.info("Call to {} took {}s".format(
                 function.func_name, end_time - start_time))
        return retval
    return wrapper


def number_to_time_string(number):
    """Takes the old style of time and returns the new style

    By passing the number you get back the string of that number. Has protection
    to return then new style if the new style is passed

    Args:
        number: the number you wish to get a string for
    """
    if number == 300 or number == "5min":
        return "5min"
    elif number == 3600 or number == "1h":
        return "1h"
    elif number == 86400 or number == "1d":
        return "1d"
    else:
        return "month"


def parse_target(target):
    """Extracts the flag from a drake style string

    Takes the flag off the front of a drake style sting.

    Args:
        target: single drake style string to extract the
    """
    pattern = re.compile(r'([\^\=\+><\/]*)(.*)')
    match = pattern.match(target.strip())
    flag_glob = match.groups()[0]

    target_name = match.groups()[1]
    if not flag_glob:
        flag_glob = ""
    flags = []
    if "+" in flag_glob:
        flags.append('force')
    if "^" in flag_glob or "<" in flag_glob:
        flags.append('downtree')
    if "=" in flag_glob:
        flags.append('exact')
    if ">" in flag_glob:
        flags.append('uptree')
    if "/" in flag_glob:
        flags.append('exclude')

    log.info("Flags for target {} are {}".format(target, flags))
    return target_name, set(flags)


def get_targets(targets, rule_dependency_graph):
    """Pass and array of drake style targets and get their graph

    Using the rule_dependency_graph the new graph is constructed from the drak
    style targets

    Args:
        targets: Array of drake style targets for the new graph
        rule_dependency_graph: Base graph to base the target off of
    """
    include_targets = set()
    exclude_targets = set()
    for target in targets:
        target_name, flags = parse_target(target)

        if 'exclude' in flags:
            exclude_targets.add(target_name)
            continue
        if target_name == "...":
            targets_with_no_dependents = \
                deepy.scheduler.get_targets_with_no_dependents(
                    rule_dependency_graph)
            for target_with_no_dependents in targets_with_no_dependents:
                include_targets.add((target_with_no_dependents, tuple(flags)))
            continue
        include_targets.add((target_name, tuple(flags)))

    log.debug("Including: " + str(include_targets))
    log.debug("Excluding: " + str(exclude_targets))

    return include_targets, exclude_targets


def run_command(command_string):
    """Fork a command

    The command passed should be in a standard string style. This function then
    breaks it up into something that Popen can understand and then forks it.

    Args:
        command_stirng: standard text form of the command to run
    """
    log.info("running job {}".format(command_string))
    command = shlex.split(command_string)
    job = subprocess.call(command)

    return job


def parallel_process(queue, parallel):
    """Takes a queue of jobs and runs them in parallel with the maximum of
    parallel
    """
    p = multiprocessing.Pool(processes=parallel)
    p.map(run_command, queue)
    p.close()

    return queue


def parse_time(time_string):
    """Takes a string time and turns it into a datetime. Basically just a
    wrapper on the deepy.timerange.parse_datetime_timeperiod
    """
    timestamp = deepy.timerange.parse_datetime_timeperiod(
        time_string, time_sep="-")[0]

    return arrow.get(timestamp).datetime


def get_largest_file_delta(node_ids, rule_dependency_graph):
    """Returns the largest file_step as a timedelta in the nodes given

    Finds the largets file_step in the list of node_ids and returns it as a
    timedelta. If the job is a meta target then the targets that it points to
    are added to the jobs list and checked also

    Args:
        nodes: Acceptable nodes are anything with a file_step or a meta node.
            These nodes or the nodes the meta refers to will have their
            file_steps checked.
        rule_dependency_graph: The graph that holds these nodes and will have
            a file step for them
    """
    largest_file_delta = dateutil.relativedelta.relativedelta(minutes=5)
    current_time = datetime.datetime.now()
    oldest_time = current_time - largest_file_delta

    new_list = node_ids

    while new_list:
        node_ids = new_list
        new_list = []

        for node_id in node_ids:
            node = rule_dependency_graph.node[node_id]

            if node.get("type") == "meta":
                parent_ids = rule_dependency_graph.predecessors(node_id)
                new_list = new_list + parent_ids
                continue

            node_file_step = node.get("file_step", "5min")
            node_file_delta = deepy.timerange.convert_to_timedelta(
                    node_file_step)

            if type(node_file_delta) == datetime.timedelta:
                node_file_delta = deepy.timerange.time_to_relative_delta(
                        node_file_delta)

            time_in_past = current_time - node_file_delta
            if time_in_past < oldest_time:
                oldest_time = time_in_past
                largest_file_delta = node_file_delta

    return largest_file_delta


def get_time_range(start_time, end_time, range_num, target_ids,
        rule_dependency_graph):
    """Get the datetimes for the time range

    start_time and end_time will be turned into datetimes or they will be
    computed by the default values setup, or they will be computed by the
    range_num and the list of targets.

    Args:
        start_time: the user inputed start time
        end_time: the user inputed end time
        range_num: If the time is to be determined by a range then range_num is
            the number of the largest file_steps you wish to be made
        target_ids: The targets that should be considered for the range num
        rule_dependency_graph: The graph that holds the file_steps for the
            targets passed.

    Return:
        start_time, end_time both in datetime form
        if both are None the time is now to now
        if end_time is present then both are end_time if end_time is in the past
            otherwise start_time is now
        if start_time is present then both are start_time if start_time is in
            future otherwise end_time is now
        if both are present then they both take their own value
    """
    now = get_now()
    now = arrow.get(now).datetime
    if not start_time and not end_time:
        start_time = end_time = now
    elif not start_time and end_time:
        end_time = parse_time(end_time)
        if end_time > now:
            start_time = now
        else:
            start_time = end_time
    elif start_time and not end_time:
        start_time = parse_time(start_time)
        if start_time < now:
            end_time = now
        else:
            end_time = start_time
    else:
        start_time = parse_time(start_time)
        end_time = parse_time(end_time)

    if range_num is not None:
        largest_file_delta = get_largest_file_delta(
                target_ids, rule_dependency_graph)
        range_num = range_num - 1
        time_delta = range_num*largest_file_delta
        start_time = start_time - time_delta

    return start_time, end_time


def make_and_execute(targets, rule_dependency_graph, start_time, end_time):
    """Takes in the targets and runs everything through the execution of the
    build plan"""
    build_plan = make_build_plan(targets, rule_dependency_graph, start_time,
            end_time, range_num=None)
    return execute_build_plan(build_plan)


def make_build_plan(targets, rule_dependency_graph,
                    start_time=None, end_time=None,
                    patch=False, range_num=None):
    """Takes in drake strings, start and end times and creates a build plan

    The targets are all interpretted as drake strings and by using the
    rule_dependency_graph a build plan is generated. The build plan is created
    with the times given in start_time and end_time and the rules in
    get_time_range. Patch attempts to make the whole graph

    Args:
        targets: array of drake targets
        rule_dependency_graph: the graph to base the plan off of
        start_time: a start time string of the form YYYY-MM-DD-HH-MM
        end_time: a end time string of the same form
        patch: a boolean to force create the whole tree
    """
    # Sanitize args
    if len(targets) == 0 and not patch:
        log.exception("Must specify either one or more target specifies or "
                      "--patch. Exiting.")
        return


    # Generate the list of targets
    target_tuple = get_targets(targets, rule_dependency_graph)
    include_targets, exclude_targets = target_tuple

    joined_targets = [x[0] for x in include_targets]
    targets_str = " ".join(sorted(joined_targets))

    start_time, end_time = get_time_range(start_time, end_time, range_num,
            joined_targets, rule_dependency_graph)

    log.info("Running targets from {} to {}: {}".format(
             start_time, end_time, targets_str))

    run_time = time.time()
    log.info("Planning build. This may take a few seconds if there are many "
             "dependencies to resolve.")
    build_plan = deepy.scheduler.BuildPlan(rule_dependency_graph)

    # Add all targets to the build plan
    for target, flags in include_targets:
        log.info("adding job {} with start_time {} and end_time {}".format(
                 target, start_time, end_time))
        build_plan.add_job(target, start_time, end_time, flags=flags,
                           exclusions=exclude_targets)

    stop_time = time.time()
    log.info("Took {} seconds to plan build".format(stop_time - run_time))

    return build_plan


def follow_quit_strategy(quit_strategy, build_plan, execution_plan,
                         failed_node_id, command):
    """Takes in the flags and the nodes and decides how to handle a failed job

    There are multiple ways to handle a failed job. To quit. To update the
    scheduler accordingly or to just ignore it every happend. The flags hold the
    way we should quit. The specific flag is passed in quit_strategy. We update
    the build_plan and execution_plan based on this flag

    Args:
        quit_strategy: The way to handle the failed job. (quit, ignore,
            workaround) default is workaround
        build_plan: The build_plan that needs to be updated
        execution_plan: The execution_plan that should be updated
        failed_node_id: The node that failed to run
        command: The command for the node that failed
    """
    log.error("Command {} returned non-zero exit status".format(
              failed_node_id, command))
    if quit_strategy == "quit":
        sys.exit(1)
        return

    elif quit_strategy == "workaround" or quit_strategy is None:
        remove_failed_nodes(execution_plan, failed_node_id, build_plan)
        return

    elif quit_strategy == "ignore":
        return


def all_unbuildable(build_plan):
    """Checks if the initial_jobs are buildable and explains why they aren't

    Takes the build plan and looks at each initial job. Checks if the job is
    buildable. If the the job isn't it walks up the graph until it finds a node
    that doesn't exists that must exists to be built and logs each node.

    Args:
        build_plan: The plan that needs to be checked

    Returns:
        Returns True if none of the initial jobs are buildable and False
        otherwise
    """
    all_buildable = []
    initial_jobs = build_plan.get_initial_targets()
    for initial_job_id in initial_jobs:
        if not build_plan.is_buildable(initial_job_id):
            log.info("Job {} is unbuildable due to missing "
                     "dependencies".format(initial_job_id))
            log.info("example failure path")
            build_plan.why_unbuildable(initial_job_id)
            all_buildable.append(False)
        else:
            all_buildable.append(True)

    if initial_jobs and not any(all_buildable):
        log.warn("None of the targets are buildable. Now quiting")
        return True
    return False


def execute_build_plan(build_plan, dry_run=False, parallel=None,
                       quit_strategy=None):
    """pass a build_plan and it will be executed"""
    execution_plan = build_plan.get_execution_plan()

    if all_unbuildable(build_plan):
        return

    for step, buildset in enumerate(execution_plan):
        if len(buildset) == 0:
            continue
        log.info("Step {}:".format(step+1))

        if dry_run:
            for build in buildset:
                command = build["makerule"].get_command(build["timestamp"])
                log.info('{}: "{}"'.format(build['id'], command))
        else:
            if parallel is not None:
                queue = []
                for build in buildset:
                    timestamp = build["timestamp"]
                    command = build["makerule"].get_command(timestamp)
                    queue.append(command)
                parallel_process(queue, parallel)
                continue

            for build in buildset:
                if build.get("unbuildable"):
                    continue
                timestamp = build["timestamp"]
                command = build["makerule"].get_command(timestamp)
                command = shlex.split(command)
                command = [command[0]] + ["-f"] + command[1:]
                log.info(" ".join(command))
                retval = subprocess.call(command)
                if not retval == 0:
                    log.info("error")
                    follow_quit_strategy(quit_strategy, build_plan,
                            execution_plan, build["resolved_id"], command)

    if len(execution_plan) == 0:
        log.info("All targets up to date, nothing to do.")

def remove_from_execution_plan(execution_plan, job_id):
    """Removes the job_id from the execution_plan"""
    for step_list in execution_plan:
        for rule in step_list:
            if job_id == rule["resolved_id"]:
                rule["unbuildable"] = True


def _remove_failed_jobs_recurse(execution_plan, failed_resolved_job_id,
        build_plan):
    """The recursive helper function for remove_failed_jobs

    This is used since the first time taht remove_failed_jobs is called it needs
    to remove the node and mark it as unbuildable no matter what
    """
    del build_plan.node[failed_resolved_job_id]["unbuildable"]

    if build_plan.is_buildable(failed_resolved_job_id):
        return

    # mark the node as unbuildable and all it's targets, then remove all the
    # targets neighbors
    remove_from_execution_plan(execution_plan, failed_resolved_job_id)

    build_plan.node[failed_resolved_job_id]["unbuildable"] = True

    target_ids = build_plan.neighbors(failed_resolved_job_id)

    for target_id in target_ids:
        build_plan.node[target_id]["unbuildable"] = True
        target_neighbor_ids = build_plan.neighbors(target_id)
        for target_neighbor_id in target_neighbor_ids:
            _remove_failed_jobs_recurse(execution_plan, target_neighbor_id,
                                        build_plan)


def remove_failed_nodes(execution_plan, failed_resolved_node_id, build_plan):
    """Recursevly removes failed nodes from the build_plan

    Takes in a failed_node_id and removes it from the execution plan. It then
    checks the build_plan and calls the removal of aything that no longer has
    the ability to be built

    Args:
        execution_plan: the currently running execution_plan
        failed_resolved_node_id: the id of the node that failed, form
            job_YYYY-MM-DDTHH:MM:SS or a target of a job, form
            target.YYYY-MM-DDTHH:MM:SS
        build_plan: the build_plan that was used to create the execution plan
    """
    failed_resolved_node = build_plan.node[failed_resolved_node_id]

    # remove the job and all the targets for that job
    if failed_resolved_node.get("type") == "job":
        remove_from_execution_plan(execution_plan,
                failed_resolved_node_id)

        build_plan.node[failed_resolved_node_id]["unbuildable"] = True

        target_ids = build_plan.neighbors(failed_resolved_node_id)
        for target_id in target_ids:
            remove_failed_nodes(execution_plan, target_id, build_plan)

    # if it is a target then recurse through everything that depends on it
    if failed_resolved_node.get("type") == "target":
        build_plan.node[failed_resolved_node_id]["unbuildable"] = True

        child_node_ids = build_plan.neighbors(failed_resolved_node_id)

        for child_node_id in child_node_ids:
            _remove_failed_jobs_recurse(execution_plan, child_node_id,
                                        build_plan)


class RuleDependencyGraph(nx.DiGraph):

    def __init__(self, rules_db=None):
        super(RuleDependencyGraph, self).__init__(self)
        self.name = "Rule Dependency Graph"
        if not rules_db:
            rules_db = get_rules_database()
        self.rules_db = rules_db
        for rule in rules_db:
            self.add_rule(rule)
        self.prune_orphan_nodes()
        if nx.is_directed_acyclic_graph(self) != True:
            log.critical("The graph is not acyclic")
            log.warn("attempting better cycle algorithm")
            try:
                simple_cycles = nx.simple_cycles(self)
                log.warn("list of cycles {}".format(list(simple_cycles)))
            except:
                log.warn("failed due to a dumb recursion error")
                log.warn("now running the worse algorithm")
                self.why_acyclic()

    def get_starting_node_ids(self):
        starting_nodes = []
        for node in self.node:
            if not self.predecessors(node):
                starting_nodes.append(node)

        return starting_nodes

    def _why_acyclic_recurse(self, node_id, sub_tree):
        previous_sub_tree = copy.deepcopy(sub_tree)
        for neighbor_id in self.neighbors(node_id):
            sub_tree = copy.deepcopy(previous_sub_tree)
            try:
                node = sub_tree.node[neighbor_id]
                sub_tree.add_edge(node_id, neighbor_id)
                log.warn("found cycle")
                log.warn((list(nx.simple_cycles(sub_tree))))
                sys.exit()
            except KeyError:
                sub_tree.add_node(neighbor_id)
                sub_tree.add_edge(node_id, neighbor_id)
                self._why_acyclic_recurse(neighbor_id, sub_tree)

    def why_acyclic(self):
        starting_node_ids = self.get_starting_node_ids()
        for node_id in starting_node_ids:
            sub_tree = nx.DiGraph()
            sub_tree.add_node(node_id)
            self._why_acyclic_recurse(node_id, sub_tree)

    def add_rule(self, rule_id):
        """Add a rule to the tree

        using an id from the rules database add that rule to the tree. Adds the
        targets to the tree then the rule itself and then connects them to each
        other and connects the rule to it's dependencies

        Args:
            rule_id: an id from the rules database to add
        """
        rule = self.rules_db.get(rule_id)
        if not rule:
            raise KeyError("Rule with ID {} not found".format(rule_id))

        # if rule.get("type") == "bundle" and rule.get("recipe") == None:
        #     return

        self._make_target_nodes_for_rule(rule_id)
        self._make_job_nodes_for_rule(rule_id)
        self._make_edges_for_rule(rule_id)

    def prune_orphan_nodes(self):
        """Remove nodes with no connections"""
        for orphan in self._get_orphans():
            log.debug("Pruning orphan node {} from rule dependency "
                      "graph".format(orphan))
            self.remove_node(orphan)

    def prune_bundles(self):
        """Removes all bundles from the graph

        Removes uneccessary clutter of bundles when trying to view the graph
        """
        # keys to prevent changing dictionary size
        for node_id in self.node.keys():
            node = self.node[node_id]
            if node.get("bundle") == True:
                self.remove_node(node_id)

    def prune_excluded_targets(self, excluded_targets):
        """Removes all excluded targets

        Removes unnecessary clutter of excluded targets when trying to view the
        graph
        """
        for node_id in excluded_targets:
            self.remove_node(node_id)

    def tree_walk(self, node_id, tree, direction="down"):
        """walks self and returns the tree that was walked

        Used for debugging purposes. Walks the tree in the given direction and
        puts the walked tree into tree.

        Args:
            node_id: the node to start on
            tree: this is a copy of the tree that will be produced
            direction: the direction to move in the graph
        """
        node = self.node[node_id]
        if len(tree.node) == 0:
            tree.add_node(node_id, node)
        if direction == "down":
            next_nodes = self.predecessors(node_id)
        if direction == "up":
            next_nodes = self.neighbors(node_id)
        for next_node_id in next_nodes:
            if tree.node.get(next_node_id):
                continue
            next_node = self.node[next_node_id]
            tree.add_node(next_node_id, next_node)
            if direction == "down":
                tree.add_edge(next_node_id, node_id)
            elif direction == "up":
                tree.add_edge(node_id, next_node_id)
            self.tree_walk(next_node_id, tree, direction=direction)

    def intersect(self, tree):
        """removes all nodes in self not in tree

        Requires that tree is a sub graph of self if this were to be a true
        intersect. Removes all nodes that are in self that are not in the tree
        passed so self becomes tree. Usefull when you have a nx.DiGraph taking
        on the values in self and need self to become those values

        Args:
            tree: the tree containing the nodes and edges you wish to remain in
                self
        """
        for node_id in self.node.keys():
            if node_id not in tree.node:
                self.remove_node(node_id)

    def _get_orphans(self):
        """Get a list of all nodes with no edges """
        orphans = []
        for node in self:
            edges = self.in_edges(node) + self.out_edges(node)
            if len(edges) == 0:
                orphans.append(node)

        return orphans

    def get_target_node_id_for_rule(self, rule_id):
        """Returns the target node id for a rule id

        Given the rule_id move to it's target and return the id of that target.
        This requires that the node is not a bundle as bundles are treated
        differently. Do not use this for abstract node operations

        Args:
            rule_id: the rule you wish to get the target id for
        """
        rule = self.rules_db.get(rule_id)
        if rule is None:
            return rule_id
        target = rule.get("target")
        node_id = target if target else rule_id

        return node_id

    def get_all_prerequisites(self, rule_or_target_id, types=None):
        """Returns all the prerequisites for the node

        Returns a list of node ids that are prerequisites for the node given
        that the prereq has the type given by types

        Args:
            rule_or_target_id: The node to get the prereqs for
            types: a list of types of returnable nodes
        """
        if types == None:
            types = ["job", "meta", "target"]
        all_ancestors = nx.algorithms.ancestors(self, rule_or_target_id)

        filtered_ancestors = []
        for ancestor in all_ancestors:
            if self.node[ancestor].get("type") in types:
                filtered_ancestors.append(ancestor)

        return set(filtered_ancestors)

    def get_rules_db(self):
        """A getter function of the rules_d"""
        return self.rules_db

    def get_rule(self, rule_id):
        """A getter function for the rule_id """
        return self.get_rules_db().get(rule_id)

    @staticmethod
    def _default_bundle_target(rule_id, target_id, time_step, file_step):
        """Returns a target dict with the given values to clear up code"""
        target_data = {
            "id": rule_id,
            "target": target_id,
            "time_step": time_step,
            "file_step": file_step,
            "style": "filled",
            "type": "target",
            "color": "green",
            "fillcolor": "#F0F0F0",
            "bundle": True
        }

        return target_data

    def _add_target_nodes_for_bundle(self, rule_id):
        """Adds target nodes for all the queries in the bundle

        adds the nodes to self where the key of the node is the target and the
        contents carry the parents bundle info through

        Args:
            rule: must be a bundle and and all the target nodes will be
                attached to it.
        """
        rule = self.rules_db.get(rule_id)
        queries = rule.get("queries")

        target_id = rule.get("target")
        time_step = rule.get("make_time_step")
        time_step = number_to_time_string(time_step)
        file_step = time_step
        target_data = self._default_bundle_target(rule_id, target_id,
                                                  time_step, file_step)

        recipe = rule.get("recipe")
        if not recipe:
            target_data["target"] = None
            target_data["type"] = "meta"
            self.add_node(rule_id, target_data)
        else:
            self.add_node(target_id, target_data)

        if queries is None:
            return


        for _, query in queries.iteritems():
            target_id = query.get("target")
            target_data = self._default_bundle_target(rule_id, target_id,
                                                      time_step, file_step)
            self.add_node(target_id, target_data)

    def _make_target_nodes_for_rule(self, rule_id):
        """Adds a target node for a rule or bundle

        Bundles are treated differently than rules and are passed onto
        _add_target_nodes_for_bundle. Everyother rule is given a target node
        which holds its target name and other important information for a target
        to have. The target nodes are found by a rules database lookup.

        Args:
            rule_id: The id to add a target for
        """
        rule = self.rules_db.get(rule_id)
        target = rule.get("target")
        recipe = rule.get("recipe")

        if rule.get("type") == "bundle":
            self._add_target_nodes_for_bundle(rule_id)
            return

        if not target and recipe:
            return

        node_id = self.get_target_node_id_for_rule(rule_id)
        node_data = {
            'id': rule_id,
            'target': target,
            'time_step': rule.get('time_step'),
            'file_step': rule.get('file_step'),
            'style': 'filled'
        }

        if target:
            node_data['type'] = 'target'
            node_data['color'] = 'blue'
            node_data['fillcolor'] = '#C2FFFF'
        else:
            node_data['type'] = 'meta'
            node_data['color'] = 'purple'
            node_data['fillcolor'] = '#FFE0FF'
        self.add_node(node_id, node_data)

    def _make_job_nodes_for_rule(self, rule_id):
        """Given that the rule_id is a job make a node for it

        A node will be made for any rule_id provided that it is in the rules
        database. Errors if there isn't that rule in the database. If the
        rule_id does not have a recipe then it more than likely is a target only
        rule and will not be added.

        Args:
            rule_id: The rule to add to the graph
        """
        rule = self.rules_db.get(rule_id)
        if rule is None:
            raise KeyError("The following rule was not found in rules "
                           "database {}".format(rule_id))

        # If no output and no recipe, not a job; skip it
        recipe = rule.get('recipe')
        if not recipe:
            return

        node_id = rule_id

        node_data = {}
        node_data.update(rule)
        node_data.update({
            'id': rule_id,
            'type': 'job',
        })
        if rule.get("type") == "bundle":
            time_step = rule.get("make_time_step")
            time_step = number_to_time_string(time_step)
            file_step = time_step
            node_data.update({
                "time_step": time_step,
                "file_step": file_step
            })
        self.add_node(node_id, node_data)

    def _make_edges_for_bundle(self, rule_id):
        """Connects bundles to their tagets

        using rule_id, the bulde will connect with all the query targets

        Args:
            rule_id: The bundles id to connect to targets
        """
        rule = self.rules_db.get(rule_id)

        queries = rule.get("queries")

        if queries is None:
            return

        for _, query in queries.iteritems():
            target_id = query.get("target")
            self.add_edge(
                rule_id,
                target_id,
                kind="depends_one_or_more",
                label="depends_one_or_more"
            )

    def _add_depends_to_bundle(self, rule_id):
        """Adds to the bundles dependencies

        using the queries in the bundle, add the dependency for the queries to
        the node

        Args:
            rule_id: The bundles id to add dependencies to
        """
        rule = self.rules_db.get(rule_id)

        if "depends_one_or_more" not in rule:
            rule["depends_one_or_more"] = {}

        for dependency in rule["depends_one_or_more"].keys():
            del rule["depends_one_or_more"][dependency]
            if dependency in self.rules_db:
                depends_rule = self.rules_db[dependency]
                queries = depends_rule.get("queries", {})
                for _, query in queries.iteritems():
                    target = query.get("target")
                    timestep = query.get("timestep")
                    rule["depends_one_or_more"][target] = timestep

        queries = rule.get("queries")

        if queries is None:
            return

        for _, query in queries.iteritems():
            dependency = query.get("input_file_glob")
            if dependency is None:
                continue
            timestep = query.get("timestep")
            rule["depends_one_or_more"][dependency] = timestep

    def _make_edges_for_rule(self, rule_id):
        """Adds edges to the rule i.e. rule connect to target

        Connects the rule node to all of its dependency nodes and target nodes
        as well as its meta targets

        Args:
            rule_id: the rule you would like to connect to its targets and
                prereqs
        """
        rule = self.rules_db.get(rule_id)

        if rule is None:
            raise KeyError("rule was not found in the database "
                           "{}".format(rule_id))
        target = rule.get('target')
        recipe = rule.get('recipe')
        target_id = self.get_target_node_id_for_rule(rule_id)
        if target and not recipe:
            # Target definition only
            node_id = target_id
        elif target and recipe:
            # Job definition with target included
            node_id = rule_id
        elif recipe and not target:
            # Bundle definition with targets in the queries
            node_id = rule_id
            self._make_edges_for_bundle(rule_id)
        else:
            # Job definition only, no target (targets are
            # dependent on this rule)
            node_id = rule_id

        if rule.get("type") == "bundle":
            # Bundle needs to have many dependencies, add them now
            self._add_depends_to_bundle(rule_id)

        for dep_set in ('depends', 'depends_one_or_more', 'optional'):
            for dependency_id in rule.get(dep_set, {}):
                dependency = self.rules_db.get(dependency_id)
                if dep_set == "optional" and dependency is None:
                    # We don't consider it an error if an optional
                    # dependency is not defined
                    continue
                if self._is_meta(rule_id):
                    self.add_edge(dependency_id, node_id, kind="dep_set")
                    continue

                dependency_target_id = self.get_target_node_id_for_rule(
                    dependency_id)

                self.add_edge(dependency_target_id, node_id, kind=dep_set,
                              label=dep_set)

        # Connect target to job
        if target and recipe:
            self.add_edge(rule_id, target_id, kind="depends", label="depends")

    def _is_meta(self, rule_id):
        """Returns wether the rule_id of the meta type or not"""
        rule = self.rules_db.get(rule_id)
        if rule.get('target'):
            return False
        if rule.get('recipe'):
            return False
        else:
            return True

    def get_resolved_dependencies_for_rule(self, rule_id, timestamp,
                                           existing_only=False):
        """Returns a dict of the resolved dependencies for a node_id

        Takes in a node_id and a timestamp and returns a dict of all the things
        that it depends on. All returned values are resolved with the timestamp

        Args:
            node_id: the node you would like to get resolved dependencies for
            timestamp: the time at which you would like to resolve the node
            existing_only: TODO

        Returns:
            A dictionary where the rule is the key and the return value is an
            array of tuples that are (resolved_rule_id, dependency_type)
            Everything in this tuple is a dependency of node_id
            ex: {rule_id: [(resolved_rule_id, dependency_type)]}
        """
        rule = self.rules_db[rule_id]

        dependencies = dict(
            rule.get("depends", {}).items() +
            rule.get("depends_one_or_more", {}).items())
        if rule.get("target") and not dependencies:
            # top node
            return dependencies

        file_step = rule.get('file_step')
        # Get the floored time stamp for the node
        if file_step:
            timestamp = deepy.timerange.floor_timestamp_given_time_step(
                timestamp, file_step)
        # Start a list of dependencies
        resolved_dependencies = {}
        # Iterate through each in edge
        for dependency_id, _, data in self.in_edges(rule_id, data=True):
            dependency_type = data.get('kind')
            dependency = self.node[dependency_id]
            # Get the timestamps that the dependencies should have
            timestamps = deepy.timerange.subset_timerange(
                timestamp, parent_time_step=file_step,
                child_time_step=dependency.get('file_step'))

            # For the dependency type create a list of all the nodes
            if not dependency["id"] in resolved_dependencies:
                resolved_dependencies[dependency['id']] = []
            for sub_timestamp in timestamps:
                resolved_dependency_id = deepy.make.substitute_target(
                    dependency_id, sub_timestamp)

                if existing_only:
                    # TODO: This should use the Target abstraction, but it
                    # doesn't have access to it in the
                    # Rule Dependency graph. The fundamental problem here is
                    # that Makerule farms out this
                    # resolution to the RuleDependencyGraph when it should
                    # really be doing it to the BuildPlan.
                    # This needs to change in the next refactor cycle
                    if not path_exists(resolved_dependency_id):
                        continue
                resolved_dependencies[dependency['id']].append(
                    (resolved_dependency_id, dependency_type))

        return resolved_dependencies

    def get_resolved_targets_for_job(self, rule_id, timestamp):
        """Returns a resolved target for a node

        Much simpilar than dependencies because most will only have one target

        Args:
            job_id: the job you would like to get a resolved dependency for
            timestamp: the time at which you would like to resolve

        Returns:
            A list of all the resolved targets for a node
            [resolved_target_id, ...]
        """
        rule = self.rules_db.get(rule_id)
        assert rule.get('recipe')
        assert rule.get('file_step')

        file_step = rule.get('file_step')
        timestamp = deepy.timerange.floor_timestamp_given_time_step(
                timestamp,
                file_step)
        targets = []
        for _, target_id, _ in self.out_edges(rule_id, data=True):
            target = self.node[target_id]
            timestamps = deepy.timerange.subset_timerange(
                timestamp, parent_time_step=file_step,
                child_time_step=target.get('file_step'))
            for sub_timestamp in timestamps:
                sub_timestamp = arrow.get(sub_timestamp)
                targets.append(
                    deepy.make.substitute_target(target_id, sub_timestamp))

        return targets

    def get_target_ids_for_node(self, node_id):
        """returns a list of all target_ids

        If a job is provided, then target_ids will be returned
        If a target is provided, then itself will be returned
        If a meta is provided, then a value error is raised

        Args:
            node_id: The node you want targets for

        Returns:
            A list of the targets for the given node using the rules stated
            above

        Raises:
            ValueError if a meta target is passed
        """
        node = self.node[node_id]
        if node.get('type') == "meta":
            raise ValueError("Meta target {} has no concrete "
                             "dependents".format(node_id))
        if node.get('type') == 'target':

            return [node_id]
        else:
            retval = []

            for neighbor in self.neighbors(node_id):
                if self.node[neighbor].get('type') == "meta":
                    continue
                retval.append(neighbor)

            return retval

    def get_dependency_ids_for_node(self, node_id):
        """returns a list of all dependency ids

        If a job is provided, then dependency ids will be returned
        If a target is provided, then it's jobs dependencies will be returned
        If a meta is provided, then a value error is raised

        Args:
            node_id: The node you want targets for

        Returns:
            A list of the targets for the given node using the rules stated
            above

        Raises:
            ValueError if a meta target is passed
        """
        parents = []
        for parent in self.predecessors(node_id):
            parents.append(parent)

        node = self.node[node_id]
        if node.get('type') == "meta":
            raise ValueError("Meta target {} has no concrete "
                             "dependents".format(node_id))
        if node.get('type') == 'target':

            return self.get_dependency_ids_for_node(parents[0])
        else:
            retval = []

            for parent in parents:
                if self.node[parent].get('type') == "meta":
                    continue
                retval.append(parent)

            return retval

    def is_job_immediatly_buildable(self, job_id, timestamp):
        """Returns whether the job at the timestamp can be built without
        walking up the tree

        New, behavior to fix issues with hour cubes (see DS-79):
            If the job is an hour cube and has all its depends (including
            depends_one_or_more) then it is immediately buildable. If it does
            not have all but it depends_one_or_more, *and* a sufficient amount
            of time has passed, then it is immediately buildable.

        Old behavior:
            If the job has all it's depends and at least one or more depends
            then it is immediately buildable.

        Args:
            job_id: The node you wish to check the buildable status of
            timestamp: The timestamp you wish to build the node at

        Returns:
            True if the job is buildable at the timestamp without walking up
            the tree. False otherwise
        """
        job = self.node[job_id]
        job_file_step = job.get("file_step")
        if job_file_step is None:
            log.debug("{} does not have a file_step, assining it 5min"
                .format(job_id))
            job_file_step = "5min"
        job_timestamp = deepy.timerange.floor_timestamp_given_time_step(
            timestamp,
            job_file_step)

        edge_iter = self.in_edges_iter(job_id, data=True)

        for edge in edge_iter:
            source_id, _, data = edge
            if data.get("kind") == "depends":
                source_path, source_node = resolve_node(
                        self,
                        source_id,
                        job_timestamp)

                if source_node["exists"]:
                    continue
                else:
                    log.debug("{} must exist to build {}"
                        .format(source_path, job_id))
                    return False
            elif data.get("kind") == "depends_one_or_more":
                source = self.node[source_id]

                timedelta = deepy.timerange.convert_to_timedelta(job_file_step)
                start = job_timestamp
                end = start + timedelta
                source_file_step = source.get("file_step")

                timestamps = arrow.range(
                        source_file_step,
                        start,
                        end)

                exist_count = 0
                missing = []
                for job_sub_timestamp in timestamps:
                    source_path, source_node = resolve_node(
                            self,
                            source_id,
                            job_sub_timestamp)

                    if source_node["exists"]:
                        exist_count += 1
                    else:
                        missing.append(source_path)

                if exist_count < len(timestamps):
                    log.debug("{} is missing {}".format(job_id, missing))

                    if job_file_step not in ['hour', '1h', '3600', 3600]:
                        # The only restriction to the non-hour cubes is that any prereq exists
                        if exist_count < 1:
                            log.debug("{} must exist for time {} in the range {} to build {}"
                                .format(source_id, job_timestamp, job_file_step, job_id))
                            return False
                    else:
                        # Hour cubes wait for a timeout to expire before building an incomplete cube
                        now = arrow.get()
                        td = deepy.timerange.convert_to_timedelta(DEPENDS_ONE_OR_MORE_TIMEOUT)
                        if job_timestamp < now - td:
                            if exist_count < 1:
                                log.debug("{} must exist for time {} in the range {} to build {}"
                                    .format(source_id, job_timestamp, job_file_step, job_id))
                                return False
                            else:
                                log.info("reluctantly satisfied building {} {} with incomplete dependency after timeout of {}"
                                    .format(job_id, job_timestamp, DEPENDS_ONE_OR_MORE_TIMEOUT))
                        else:
                            log.debug("all {} must exist for time {} in the range {} to build {} until {} pass"
                                .format(source_id, job_timestamp, job_file_step, job_id, DEPENDS_ONE_OR_MORE_TIMEOUT))
                            return False
        return True

    def is_job_stale(self, job_id, timestamp, flags=None):
        """Returns whether a job is stale or not

        If the jobs dependencies are newer than it's target then it is stale,
        otherwise it is not stale.

        Note: there is special-case code in here for h5flow for compatibility
        with the old jobs.py scheduler. This function will return False if
        either .h5.raw exists or if .h5 exists.

        Args:
            job_id: the job you would like to check the stale staus of
            timestamp: the time at which you would like to evaluate the job

        Returns:
            If any mtime of a dependency target is newer than the job_id's
            target it returns True. Otherwise it returns False
        """
        if flags == None:
            flags = []

        if 'force' in flags:
            return True

        job = self.node[job_id]
        if job.get("type") == "target":
            parents = self.predecessors(job_id)
            if len(parents) == 0:
                return False
            job_id = parents[0]
            job = self.node[job_id]
        file_step = job["file_step"]

        resolved_target_mtime_list = []
        target_ids = self.get_target_ids_for_node(job_id)
        for target_id in target_ids:
            try:
                target_file_step = self.node[target_id]["file_step"]
            except KeyError, e:
                if target_id not in self.node:
                    log.warn('target_id "{}" missing from node for job_id ' \
                             '"{}"'.format(target_id, job_id))
                elif 'file_step' not in self.node[target_id]:
                    log.warn('target_id "{}" missing file_step for job_id ' \
                             '"{}"'.format(target_id, job_id))
                raise e
            timestamps = deepy.timerange.subset_timerange(
                    timestamp, file_step, target_file_step)
            for sub_timestamp in timestamps:
                resolved_path, resolved_target = resolve_node(self, target_id, sub_timestamp)

                target_exists = resolved_target.get("exists")
                if not target_exists:
                    # XXX special case hack so we queue fewer jobs during transition
                    if job_id == 'h5flow':
                        if os.path.exists(resolved_path[:-4]):
                            log.debug('h5flow-special-case-not-stale')
                            return False
                    return True

                target_mtime = resolved_target.get("mtime", 0)
                resolved_target_mtime_list.append(target_mtime)

                cache_time = resolved_target.get("cache_time")
                if cache_time:
                    td = deepy.timerange.convert_to_timedelta(cache_time)
                    sub_timestamp = arrow.get(target_mtime)
                    now = arrow.get()
                    if (sub_timestamp + td) < now:
                        return True

        if len(resolved_target_mtime_list) == 0:
            resolved_target_mtime_list.append(0)
        min_resolved_target_mtime = min(resolved_target_mtime_list)

        for dependency_id, _ in self.in_edges_iter(job_id):
            dependency = self.node[dependency_id]
            dependency_file_step = dependency.get("file_step")
            timestamps = deepy.timerange.subset_timerange(
                timestamp, file_step, dependency_file_step)
            for sub_timestamp in timestamps:
                resolved_dependency_id, resolved_dependency = resolve_node(self, dependency_id, sub_timestamp)
                if resolved_dependency.get("exists"):
                    resolved_dependency_mtime = resolved_dependency.get("mtime")
                    if resolved_dependency_mtime >= min_resolved_target_mtime:
                        return True


        return False


class BuildPlan(nx.DiGraph):

    def __init__(self, rule_dependency_graph):
        super(BuildPlan, self).__init__(self)
        self.name = "Build Plan"
        self.rule_dependency_graph = rule_dependency_graph
        self.initial_targets = []

    def write_dot(self, file_name):
        """Write the build plan to a dot file

        Sanatizes the build plan of all the crap that the dot writer can't
        handle and then writes it to a dot file

        Args:
            file_name: the file to write the dot file to
        """

        sanatized = nx.DiGraph()
        for node_id, node in self.node.iteritems():
            if self.is_buildable(node_id):
                node = copy.deepcopy(node)

                sanatized.add_node(node_id, attr_dict=node)
                for node1, node2, data in self.out_edges(node_id, data=True):
                    if self.is_buildable(node2):
                        data = copy.deepcopy(data)
                        sanatized.add_edge(node1, node2, attr_dict=data)

        for node in sanatized.node:
            try:
                sanatized.node[node].pop("target", None)
            except KeyError:
                pass
            try:
                sanatized.node[node].pop("makerule", None)
            except KeyError:
                pass
            try:
                sanatized.node[node].pop("depends", None)
            except KeyError:
                pass
            try:
                sanatized.node[node].pop("depends_one_or_more", None)
            except KeyError:
                pass
            try:
                sanatized.node[node].pop("meta", None)
            except KeyError:
                pass
            try:
                sanatized.node[node].pop("recipe", None)
            except KeyError:
                pass
            try:
                sanatized.node[node].pop("timestamp", None)
            except KeyError:
                pass
        nx.write_dot(sanatized, file_name)


    @benchmark
    def add_job(self, job_id, start_time, end_time, flags=set(),
                exclusions=set()):
        """Takes a job_id and times and adds them to the build plan

        Using the start_time and end_time, adds the job_id with the proper times
        to the build_plan. Then walks up the tree and adds all dependencies. It
        also follows standard darke flags

        Args:
            job_id: the job you want to add
            start_time: the beginning of the time range to add the job through
            end_time: the end of the time range to add the job through
            flags: standard drake flags in word form for how to add the job
            exclusions: and node that should be skipped

        Raises:
            ValueError if the job provided does not have a file_step
        """
        # Ignore jobs in exclusion set
        if job_id in exclusions:
            return

        job = self.rule_dependency_graph.node[job_id]
        if job.get('file_step'):
            start_time = deepy.timerange.floor_timestamp_given_time_step(
                start_time, job.get('file_step'))
            end_time = deepy.timerange.floor_timestamp_given_time_step(
                end_time, job.get('file_step'))

        # Non-meta targets
        if job.get('target') or job.get('recipe'):
            if not job.get('file_step'):
                raise ValueError("Build targets must specify a file_step")

            timestamps = arrow.range(job.get('file_step'),
                start_time, end_time)
            for sub_timestamp in timestamps:
                node_is_stale = self.rule_dependency_graph.is_job_stale(
                        job_id, sub_timestamp, flags)
                if not node_is_stale:
                    continue
                for target in self._resolve_dependency(job_id,
                        sub_timestamp,
                        job.get('file_step'),
                        flags=flags,
                        exclusions=exclusions,
                        dependency_type="depends"):
                    self.initial_targets.append(target)

        # Meta targets
        else:
            for parent_id, _ in self.rule_dependency_graph.in_edges(job_id):
                self.add_job(parent_id, start_time, end_time, flags, exclusions)

    def get_initial_targets(self):
        """Getter function for the initial_targets"""
        return self.initial_targets

    def _resolve_exact_dependency(self, dependency_id, direction,
                                  current_timestamp, expanded_node_id,
                                  exclusions, dependency_type, node):
        """Finishes up the resolve dependency for below but for exacts"""
        dependency_node = self.rule_dependency_graph.node[dependency_id]
        if dependency_node.get('type') == "meta" or dependency_id in exclusions:
            return False

        if direction == "up":
            dependency_timestamps = deepy.timerange.subset_timerange(
                    current_timestamp, node.get('file_step'),
                    dependency_node.get('file_step'))

            for dependency_timestamp in dependency_timestamps:
                expanded_dependency_id, expanded_dependency_content = \
                    resolve_node(self.rule_dependency_graph, dependency_id,
                                 dependency_timestamp)

                if not self.node.get(expanded_dependency_id):
                    self.add_node(expanded_dependency_id,
                                  expanded_dependency_content)

                self.add_edge(expanded_dependency_id, expanded_node_id,
                              kind=dependency_type, label=dependency_type)
        else:
            expanded_dependency_id, expanded_dependency_content = \
                resolve_node(self.rule_dependency_graph, dependency_id,
                             current_timestamp)

            if not self.node.get(expanded_dependency_id):
                self.add_node(expanded_dependency_id,
                              expanded_dependency_content)

            self.add_edge(expanded_node_id, expanded_dependency_id,
                          kind=dependency_type, label=dependency_type)

    def _resolve_dependency(self, node_id, timestamp, parent_filestep, flags,
                            exclusions, dependency_type, directions={'up'}):
        """Returns a list of all the resolved dependency IDs

        Takes in all the properties of the node and gets the dependencies for
        that node and extends the build_plan along the way

        It also is a excellent function for making max cry

        Args:
            node_id: the node you would like dependencies for
            timestamp: the time at which this node should be resoled for
            parent_filestep: the file step for the node that depends on this
            flags: the full length text of the drake style flags to operate on
            exclusions: nodes to avoid
            dependency_type: the way that the previous node depends on this node
            directions: the direction in the tree to traverse

        returns:
            A list of the resolved node ids, so every node id above the one that
            is passed
        """
        # We'll return a list of the resolved node ids
        retval = []
        node = self.rule_dependency_graph.node[node_id]
        if node.get('type') == "meta" or node_id in exclusions:
            return []

        # Expand out this dependency in the directions provided
        edge_sets = []
        dependencies = self.rule_dependency_graph.in_edges(node_id, data=True)
        dependents = self.rule_dependency_graph.out_edges(node_id, data=True)
        # Exact always gets both flags to help with tree attaching
        if 'uptree' in flags or 'up' in directions or 'exact' in flags:
            edge_sets.append(('up', dependencies))
        if 'downtree' in flags or 'down' in directions or 'exact' in flags:
            edge_sets.append(('down', dependents))

        def get_dependency_from_edge(edge, direction):
            """returns a tuple with the next node in the traversal and the
            information about the edge

            Returns:
                (next_node_id, data)
            """
            if direction == 'up':
                return edge[0], edge[2]
            else:
                return edge[1], edge[2]

        timestamps = deepy.timerange.subset_timerange(
            timestamp, parent_filestep, node.get('file_step'))

        for current_timestamp in timestamps:
            # Resolve the node
            expanded_node_id, expanded_content = resolve_node(
                self.rule_dependency_graph, node_id, current_timestamp)

            # Adds it to the list of dependencies
            retval.append(expanded_node_id)

            # If it's already in the build plan, no need to re-add it
            if self.node.get(expanded_node_id):
                continue

            # Add the node to the build_plan
            self.add_node(expanded_node_id, expanded_content)

            # If the node isn't stale, nothing to do
            node_is_stale = self.rule_dependency_graph.is_job_stale(
                    node_id, current_timestamp, flags)
            if not node_is_stale:
                continue

            # Expand out dependencies unless explicitly requested not to
            for direction, edges in edge_sets:
                converter = functools.partial(get_dependency_from_edge,
                                              direction=direction)
                edges = [converter(edge) for edge in edges]

                for dependency_id, dependency in edges:
                    dependency_type = dependency.get('kind')
                    directions_to_recurse = set()
                    if "downtree" in flags:
                        directions_to_recurse.add("up")
                    if not 'exact' in flags:
                        directions_to_recurse.add(direction)
                    else:
                        # Exact gets treated differently, it adds both it's
                        # dependencies and it's targets and that's it
                        if self._resolve_exact_dependency(
                                dependency_id, direction, current_timestamp,
                                expanded_node_id, exclusions, dependency_type,
                                node) == False:
                            continue

                    for expanded_dependency_id in self._resolve_dependency(
                            dependency_id, current_timestamp,
                            node.get('file_step'),
                            flags=get_next_job_flags(flags,
                                                     direction=direction),
                            exclusions=exclusions,
                            dependency_type=dependency_type,
                            directions=directions_to_recurse):
                        if direction == "up":
                            self.add_edge(expanded_dependency_id,
                                          expanded_node_id,
                                          kind=dependency_type,
                                          label=dependency_type)
                        else:
                            self.add_edge(
                                expanded_node_id, expanded_dependency_id,
                                kind=dependency_type, label=dependency_type)

        return retval


    def _check_for_job_nodes_in_parents(self, node_id):
        node = self.node[node_id]
        predecessors = self.predecessors(node_id)
        if len(predecessors) == 0:
            return False
        if node.get('type') == 'job':
            return True
        for predecessor in predecessors:
            if self._check_for_job_nodes_in_parents(predecessor):
                return True

        return False

    def _is_entry_job(self, node_id):
        """Returns the truth value of node_id being an entry job"""
        node = self.node[node_id]
        if node.get('type') != 'job':
            return False
        for predecessor in self.predecessors(node_id):
            if self._check_for_job_nodes_in_parents(predecessor):
                return False
        return True

    def _get_next_jobs(self, job_id, visited):
        """Using the job id of a previously run node and all the visited nodes
        find the next job to run

        Looks at the last run job and all the visited jobs, moves down the tree
        to the next jobs and if any of these jobs have all their dependencies
        fulfilled, return them as the next jobs

        Args:
            job_id: the id of the previously run job
            visited: a list of all ids that have been run already

        Returns:
            A list of job_ids that can be run next
        """
        job = self.node[job_id]
        assert job.get('type') == 'job'

        targets = []
        for neighbor_id in self.neighbors(job_id):
            neighbor = self.node[neighbor_id]
            if neighbor.get('type') == 'target':
                targets.append(neighbor_id)
                visited.add(neighbor_id)

        jobs = []
        def job_is_ready(job_id):
            dependencies = set(self.predecessors(job_id))
            dependencies = [dependency in visited
                            for dependency in dependencies]
            return all(dependencies)

        for target_id in targets:
            for next_job_id in self.neighbors(target_id):
                if not self.is_buildable(next_job_id):
                    continue
                if job_is_ready(next_job_id):
                    jobs.append(next_job_id)

        return jobs


    def get_starting_jobs(self):
        '''
        Returns all jobs that have no dependencies; i.e., the jobs that will create the initial targets
        '''
        # Get the initial targets
        starting_jobs = []
        for node in self:
            if self._is_entry_job(node):
                starting_jobs.append(node)

        return starting_jobs

    def is_job_runnable(self, job_node_id):
        '''
        Returns True if all dependencies for this job are satisifed, False otherwise

        '''
        job_node = self.node[job_node_id]

        assert job_node.get('type') == 'job'

        exists = collections.defaultdict(list) # target_id -> [exists]
        unbuildable = collections.defaultdict(list)
        dependency_types = {}
        for dependency_id, _,  data in self.in_edges(job_node_id, data=True):
            dependency = self.node[dependency_id]
            if not dependency.get('type') == 'target':
                continue
            target = dependency.get('target')
            dependency_type = data.get('kind')
            source_id = dependency.get('source_id')
            dependency_types[source_id] = dependency_type
            exists[source_id].append(target.exists())

            buildable = not self.is_buildable(dependency_id)
            unbuildable[source_id].append(buildable)

        for dependency_id in dependency_types.keys():
            dependency_type = dependency_types[dependency_id]
            dependency_exists = exists[dependency_id]
            dependency_unbuildable = exists[dependency_id]
            if dependency_type == "depends":
                return all(dependency_exists)
            if dependency_type == "depends_one_or_more":
                return any(dependency_exists) and not all(dependency_unbuildable)

        return True

    def is_buildable(self, node_id):
        """Returns true if node_id can be built, false otherwise

        recursively checks if a node is buildable, caches the value for each
        node that is checked.

        Args:
            node_id: The id of the node that is wished to be checked

        Returns:
            True is the node is buildable, Flase otherwise
        """
        in_edges = self.in_edges(node_id, data=True)

        node = self.node[node_id]

        # cache case
        if "unbuildable" in node:
            if node["unbuildable"] == True:
                return False
            elif node["unbuildable"] == False:
                return True

        # base case
        if len(in_edges) == 0:
            node_type = node.get("type")
            if node_type == "job":
                return True

            if not node['exists']:
                node['unbuildable'] = True
                return False
            node["unbuildable"] = False
            return True

        # creates a dict of dependency types for the rule_id
        # also creates a dict of buildables for th rule_id
        buildables = collections.defaultdict(list)
        dependency_types = {}
        for dependency_id, _, data in in_edges:
            dependency_type = data.get("kind")
            dependency = self.node[dependency_id]
            rule_id = dependency.get("id")
            dependency_types[rule_id] = dependency_type
            is_buildable = self.is_buildable(dependency_id)
            buildables[rule_id].append(is_buildable)

        # Now check that the buildable list is good for the dependency type
        for rule_id in buildables:
            dependency_type = dependency_types[rule_id]
            buildable = buildables[rule_id]
            if dependency_type == "depends" and not all(buildable):
                node["unbuildable"] = True
                return False
            elif dependency_type == "depends_one_or_more" and not any(buildable):
                node["unbuildable"] = True
                return False

        node["unbuildable"] = False
        return True

    def why_unbuildable(self, node_id):
        """Returns a unbuildable path to the top

        Takes in a node_id and follows a path to the top and announces each
        unbuildable node it crosses. This is recursive.

        Args:
            node_id: the node that you wish to now why it is unbuildable
        """
        if self.is_buildable(node_id):
            log.info("node {} is buildable".format(node_id))
            return
        log.info("node {} is unbuildable".format(node_id))

        buildables = collections.defaultdict(list)
        example = {}
        for other_node_id, _, data in self.in_edges(node_id, data=True):
            if not self.is_buildable(other_node_id):
                if data["kind"] == "depends":
                    self.why_unbuildable(other_node_id)
                    return
                elif data["kind"] == "depends_one_or_more":
                    dependency = self.node[other_node_id]
                    rule_id = dependency.get("id")
                    is_buildable = self.is_buildable(other_node_id)
                    buildables[rule_id].append(is_buildable)
                    example[rule_id] = other_node_id

        for rule_id in buildables:
            if not any(buildables[rule_id]):
                self.why_unbuildable(example[rule_id])
                return

        log.info("top most unbuildable {}".format(node_id))

    def get_input_targets(self):
        """returns all the initial targets in a tree"""
        input_targets = []
        start_time = time.time()
        for node_id in self:
            node = self.node[node_id]
            if node.get('type') == 'target' and len(self.predecessors(node_id)) == 0:
                input_targets.append(node_id)

        end_time = time.time()
        log.info("Took {}s to detect input targets".format(end_time - start_time))
        return input_targets

    def get_execution_plan(self):
        """Returns a list of lists of steps to take to execture the build plan

        Looks for all the initial jobs. Adds them to a list, then with those
        removed looks for the next jobs that can run and adds them to a list. In
        the end you have a list of lists where each interior list can be
        executed in parallel

        Returns:
            [[job1, job2, ... (all run in parallel)], [job1, job2, ...], ...]

        """
        start_jobs = self.get_starting_jobs()
        execution_plan = []
        next_step = start_jobs
        visited = set()
        for input_target_id in self.get_input_targets():
            input_target = self.node[input_target_id]
            if input_target['exists']:
                visited.add(input_target_id)

        # We'll also consider all unbuildables to be visited
        for node_id in self:
            if not self.is_buildable(node_id):
                visited.add(node_id)

        while len(next_step) > 0:
            runlist = []
            current_step = next_step
            next_step = []
            for job_id in current_step:
                if not self.is_buildable(job_id):
                    continue
                visited.add(job_id)
                runlist.append(self.node[job_id])
                next_step += self._get_next_jobs(job_id, visited)
            execution_plan.append(runlist)

        return execution_plan

def get_resolved_node_id(rule_dependency_graph, node_id, timestamp):
    node = rule_dependency_graph.node[node_id]
    if node.get('type') == "job":
        return "{}_{}".format(node_id, str(timestamp))
    elif node.get('type') == "target":
        return deepy.make.substitute_target(node_id, timestamp)
    else:
        return node_id

def resolve_node(rule_dependency_graph, node_id, timestamp):
    """Resolves the node with the timestamp given

    Takes the node and following standard naming rules it resolves the node with
    the timestamp. Also resolves the targets path

    Args:
        node_id: the node you would like resolved
        timestamp: the time that the node should be resolved for

    Returns:
        The resolved node name. Time stamps are resolved and so are the paths.
        Extra values like makerule and target and exists are added
    """
    node = rule_dependency_graph.node[node_id]
    node_type = node.get('type')
    newnode = copy.copy(node)
    newnode['timestamp'] = timestamp
    newnode['source_id'] = node_id
    new_node_id = get_resolved_node_id(rule_dependency_graph,
                                       node_id, timestamp)

    if node_type == 'job':
        makerule = deepy.make.Makerule(
            node['id'], rule_dependency_graph,
            rules=rule_dependency_graph.get_rules_db())
        newnode['makerule'] = makerule
        new_node_id = "{}_{}".format(node_id, str(timestamp))
        newnode["resolved_id"] = new_node_id
    elif node_type == "bundle":
        makerule = deepy.make.Makerule(
            node["id"], rule_dependency_graph,
            rules=rule_dependency_graph.get_rules_db())
        newnode["makerule"] = makerule
        new_node_id = "{}_{}".format(node_id, str(timestamp))
        newnode["resolved_id"] = new_node_id
    elif node_type == 'target':
        new_node_id = deepy.make.substitute_target(node_id, timestamp)
        newnode['target'] = LocalTarget(new_node_id)
        if timestamp.float_timestamp < time.time():
            newnode['exists'] = newnode['target'].exists()
        else:
            newnode["exists"] = False
        if newnode['exists']:
            newnode['mtime'] = newnode['target'].mtime()
    else:
        raise ValueError("Node type {} in node {} not "
                         "recognized".format(node_type, node_id))

    return new_node_id, newnode


def is_node_stale(rule_dependency_graph, node_id, timestamp, flags):
    '''Node is stale if one of the following is true:
        * Targets associated with node don't exist
        * mtime for a target is less than a dependency's mtime
        * mtime for a target is further in the past than the specified timeout (cache_time)
        * force flag is specified
    '''
    if 'force' in flags:
        return True

    target_ids = rule_dependency_graph.get_target_ids_for_node(node_id)
    mtime_min = None
    for target_id in target_ids:
        _, resolved_node = resolve_node(rule_dependency_graph, target_id, timestamp)

        node_exists = resolved_node.get('exists')
        if not node_exists:
            return True

        mtime = resolved_node.get('mtime', 0)

        if mtime_min == None:
            mtime_min = mtime
        mtime_min = min(mtime, mtime_min)

        # Check for auto-rebuild due to cache time override
        cache_time = resolved_node.get('cache_time')
        if cache_time:
            td = deepy.timerange.convert_to_timedelta(cache_time)
            timestamp = arrow.get(mtime)
            now = arrow.get()
            if (timestamp + td) < now:
                return True

    return False


def get_next_job_flags(flags, direction):
    '''Get the set of flags that should be present in the next iteration of the graph walk.

    For example, if flags contains "downtree", then each node after the initial one should also have
    the "downtree" flag. If it contains "exact" then the flag set will be empty and the recursion will
    halt.
    '''

    new_flags = set()
    if 'exact' in flags:
        return set()
    if 'downtree' in flags and direction == "down":
        new_flags.add('downtree')
    if 'uptree' in flags and direction == "up":
        new_flags.add('uptree')
    if 'force' in flags and ('downtree' in flags or 'uptree' in flags):
        new_flags.add('force')

    return new_flags


def get_targets_with_no_dependents(rule_dependency_graph):
    ''' '''
    targets = []
    for node_id in rule_dependency_graph:
        dependents = rule_dependency_graph.out_edges(node_id)
        if len(dependents) == 0:
            targets.append(node_id)

    return targets

class Target(object):
    """A Target is a resource generated by a job.

    For example, a Target might correspond to a file in HDFS or data in a database. The Target
    interface defines one method that must be overridden: :py:meth:`exists`, which signifies if the
    Target has been created or not.

    Typically, a job will define one or more Targets as output, and the Task
    is considered complete if and only if each of its output Targets exist.
    """
    __metaclass__ = abc.ABCMeta

    @abc.abstractmethod
    def exists(self):
        """Returns ``True`` if the :py:class:`Target` exists and ``False`` otherwise.
        """
        pass


class FileSystemException(Exception):
    """Base class for generic file system exceptions. """
    pass


class FileAlreadyExists(FileSystemException):
    """Raised when a file system operation can't be performed because a directory exists but is
    required to not exist.
    """
    pass


class FileSystem(object):
    """FileSystem abstraction used in conjunction with :py:class:`FileSystemTarget`.

    Typically, a FileSystem is associated with instances of a :py:class:`FileSystemTarget`. The
    instances of the py:class:`FileSystemTarget` will delegate methods such as
    :py:meth:`FileSystemTarget.exists` and :py:meth:`FileSystemTarget.remove` to the FileSystem.

    Methods of FileSystem raise :py:class:`FileSystemException` if there is a problem completing the
    operation.
    """
    __metaclass__ = abc.ABCMeta

    @abc.abstractmethod
    def exists(self, path):
        """ Return ``True`` if file or directory at ``path`` exist, ``False`` otherwise

        :param str path: a path within the FileSystem to check for existence.
        """
        pass

    @abc.abstractmethod
    def remove(self, path, recursive=True):
        """ Remove file or directory at location ``path``
deepy.scheduler.FileSystem.exists") as mock_exists
        :param str path: a path within the FileSystem to remove.
        :param bool recursive: if the path is a directory, recursively remove the directory and all
                               of its descendants. Defaults to ``True``.
        """
        pass

    def mkdir(self, path):
        """ Create directory at location ``path``

        Creates the directory at ``path`` and implicitly create parent directories if they do not
        already exist.

        :param str path: a path within the FileSystem to create as a directory.

        *Note*: This method is optional, not all FileSystem subclasses implements it.

        """
        raise NotImplementedError("mkdir() not implemented on {0}".format(self.__class__.__name__))

    def isdir(self, path):
        """Return ``True`` if the location at ``path`` is a directory. If not, return ``False``.

        :param str path: a path within the FileSystem to check as a directory.

        *Note*: This method is optional, not all FileSystem subclasses implements it.
        """
        raise NotImplementedError("isdir() not implemented on {0}".format(self.__class__.__name__))


class FileSystemTarget(Target):
    """Base class for FileSystem Targets like LocalTarget and HdfsTarget.

    A FileSystemTarget has an associated :py:class:`FileSystem` to which certain operations can be
    delegated. By default, :py:meth:`exists` and :py:meth:`remove` are delegated to the
    :py:class:`FileSystem`, which is determined by the :py:meth:`fs` property.

    Methods of FileSystemTarget raise :py:class:`FileSystemException` if there is a problem
    completing the operation.
    """

    def __init__(self, path):
        """
        :param str path: the path associated with this FileSystemTarget.
        """
        self.path = path

    @abc.abstractproperty
    def fs(self):
        """The :py:class:`FileSystem` associated with this FileSystemTarget."""
        raise

    @abc.abstractmethod
    def open(self, mode):
        """Open the FileSystem target.

        This method returns a file-like object which can either be read from or written to depending
        on the specified mode.

        :param str mode: the mode `r` opens the FileSystemTarget in read-only mode, whereas `w` will
                         open the FileSystemTarget in write mode. Subclasses can implement
                         additional options.
        """
        pass

    def exists(self):
        """Returns ``True`` if the path for this FileSystemTarget exists and ``False`` otherwise.

        This method is implemented by using :py:meth:`fs`.
        """
        return self.fs.exists(self.path)

    def remove(self):
        """Remove the resource at the path specified by this FileSystemTarget.

        This method is implemented by using :py:meth:`fs`.
        """
        self.fs.remove(self.path)

class LocalFileSystem(FileSystem):
    """ Wrapper for access to file system operations

    Work in progress - add things as needed
    """
    def exists(self, path):
        return path_exists(path)

    def mkdir(self, path):
        os.makedirs(path)

    def isdir(self, path):
        return os.path.isdir(path)

    def remove(self, path, recursive=True):
        if recursive and self.isdir(path):
            shutil.rmtree(path)
        else:
            os.remove(path)
    def mtime(self, path):
        return getmtime(path)


class File(FileSystemTarget):
    fs = LocalFileSystem()

    def __init__(self, path=None, format=None, is_tmp=False):
        if not path:
            if not is_tmp:
                raise Exception('path or is_tmp must be set')
            path = os.path.join(tempfile.gettempdir(), 'make-tmp-%09d' % random.randint(0, 999999999))
        super(File, self).__init__(path)
        self.format = format
        self.is_tmp = is_tmp

    def open(self, mode='r'):
        if mode == 'w':
            # Create folder if it does not exist
            normpath = os.path.normpath(self.path)
            parentfolder = os.path.dirname(normpath)
            if parentfolder and not path_exists(parentfolder):
                os.makedirs(parentfolder)

            if self.format:
                return self.format.pipe_writer(atomic_file(self.path))
            else:
                return atomic_file(self.path)

        elif mode == 'r':
            fileobj = FileWrapper(open(self.path, 'r'))
            if self.format:
                return self.format.pipe_reader(fileobj)
            return fileobj
        else:
            raise Exception('mode must be r/w')

    def move(self, new_path, fail_if_exists=False):
        if fail_if_exists and path_exists(new_path):
            raise RuntimeError('Destination exists: %s' % new_path)
        d = os.path.dirname(new_path)
        if not path_exists(d):
            self.fs.mkdir(d)
        os.rename(self.path, new_path)

    def move_dir(self, new_path):
        self.move(new_path)

    def remove(self):
        self.fs.remove(self.path)

    def copy(self, new_path, fail_if_exists=False):
        if fail_if_exists and path_exists(new_path):
            raise RuntimeError('Destination exists: %s' % new_path)
        tmp = File(is_tmp=True)
        tmp.open('w')
        shutil.copy(self.path, tmp.fn)
        tmp.move(new_path)

    def mtime(self):
        return self.fs.mtime(self.path)

    @property
    def fn(self):
        return self.path

    def __del__(self):
        if self.is_tmp and self.exists():
            self.remove()

LocalTarget = File
