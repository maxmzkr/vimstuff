import os
import pwd
import abc
import uuid
import json
import tempfile
import time
import socket
import subprocess
import itertools
import shutil
import funcy
import collections
import re
import sys
from StringIO import StringIO

import arrow
import boto
import boto.iam, boto.ec2, boto.s3, boto.route53
from boto.s3.connection import Location
from boto.s3.key import Key
from boto.ec2.blockdevicemapping import BlockDeviceMapping, BlockDeviceType, EBSBlockDeviceType
from fabric.api import get, put, sudo, run, cd, execute, env, hide, open_shell, settings, parallel
from fabric.contrib.files import append
from Crypto.PublicKey import RSA

import deepy.util
import deepy.log as log
import deepy.cfg
import deepy.store
import deepy.dimensions

# This is to speed up testing with fab
# see https://github.com/paramiko/paramiko/pull/192
env.disable_known_hosts = True

DEEPFIELD_NET_ZONE_ID = 'Z2Y384M9BH34ES'
DEEPFIELD_ZONE_NAME = 'deepfield.net.'
DEFAULT_REGION = "us-east-1"
DEFAULT_INSTANCE_TYPE = "m3.xlarge"
DEFAULT_VOLUME_SIZE_GB = 1000

# http://aws.amazon.com/amazon-linux-ami/instance-type-matrix/
HVM_INSTANCE_TYPES = sorted([
    'c4.large',
    'c4.xlarge',
    'c4.2xlarge',
    'c4.4xlarge',
    'c4.8xlarge',
    'c3.xlarge',
    'c3.4xlarge',
    'c3.8xlarge',
    'c3.large',
    'c3.xlarge',
    'cc2.8xlarge',
    'cg1.4xlarge',
    'cr1.8xlarge',
    'g2.2xlarge',
    'hi1.4xlarge',
    'hs1.8xlarge',
    'i2.2xlarge',
    'i2.4xlarge',
    'i2.8xlarge',
    'i2.xlarge',
    'm3.2xlarge',
    'm3.xlarge',
    'm3.large',
    'm3.medium',
    'm3.xlarge',
    'r3.2xlarge',
    'r3.4xlarge',
    'r3.8xlarge',
    'r3.large',
    'r3.xlarge',
    't2.medium',
    't2.micro',
    't2.small'
])
PV_INSTANCE_TYPES = sorted([
    'c1.medium',
    'c1.xlarge',
    'm1.large',
    'm1.medium',
    'm1.small',
    'm1.xlarge',
    'm2.2xlarge',
    'm2.4xlarge',
    'm2.xlarge',
    't1.micro'
])
INSTANCE_TYPES = sorted(funcy.distinct(PV_INSTANCE_TYPES + HVM_INSTANCE_TYPES))

# http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html#StorageOnInstanceTypes
INSTANCE_STORE_COUNTS = {
    'c1.medium': 1,
    'c1.xlarge': 4,
    'c3.2xlarge': 2,
    'c3.4xlarge': 2,
    'c3.8xlarge': 2,
    'c3.large': 2,
    'c3.xlarge': 2,
    'cc2.8xlarge': 4,
    'cr1.8xlarge': 2,
    'hi1.4xlarge': 2,
    'his1.8xlarge': 24,
    'i2.2xlarge': 2,
    'i2.4xlarge': 4,
    'i2.8xlarge': 8,
    'i2.xlarge': 1,
    'm1.small': 1,
    'm1.medium': 1,
    'm1.large': 2,
    'm1.xlarge': 4,
    'm2.xlarge': 1,
    'm2.2xlarge': 1,
    'm2.4xlarge': 2,
    'm3.medium': 1,
    'm3.large': 1,
    'm3.xlarge': 2,
    'm3.2xlarge': 2,
    'r3.large': 1,
    'r3.xlarge': 1,
    'r3.2xlarge': 1,
    'r3.4xlarge': 1,
    'r3.8xlarge': 2,
    't1.micro': 0,
    't2.medium': 0,
    't2.micro': 0,
    't2.small': 0
}

get_bucket_name_for_deployment = lambda x: '{}.pdrlabs.net'.format(x)
get_deployment_bucket = lambda deployment_id, s3: s3.get_bucket(get_bucket_name_for_deployment(deployment_id))
get_id_login = pwd.getpwuid(os.getuid())[0] if sys.stdout.isatty() else 'none'
get_id_hostname = socket.gethostname() if sys.stdout.isatty() else 'none'
get_id = lambda: '{}@{}'.format(get_id_login, get_id_hostname)

def get_bucket_count():
    s3 = boto.connect_s3()
    return len(s3.get_all_buckets())

def get_ami_type(instance_type):
    '''
    Return the ami type (pv or hvm) for a given instance type
    '''
    if instance_type and instance_type in HVM_INSTANCE_TYPES:
        return "hvm"
    else:
        return "pv"

def load_json(deployment_id, key_name, cache_age=0):
    '''
    Loads a json file from the specified deployment's bucket

    Parameters:
        deployment_id (str): Deployment ID
        key_name (str): The key name containing the JSON data
        s3 (boto.S3Connection): The boto connection to s3 to use
    '''
    deepy.cfg.init(deployment_id)
    data = deepy.store.simple_load_json(
        os.path.join(deepy.cfg.cache_dir,key_name),
        cache_age=cache_age
    )

    if not data:
        log.warn("unable-to-load-json {}".format(key_name))
    return data or {}

def save_json(deployment_id, key_name, data):
    '''
    Saves a json file from the specified deployment's bucket

    Parameters:
        deployment_id (str): Deployment ID
        key_name (str): The key name containing the JSON data
        data (dict): The dictionary containing the data to save
    '''
    deepy.cfg.init(deployment_id)
    return deepy.store.simple_save_json(
        data,
        os.path.join(deepy.cfg.cache_dir, key_name)
    )

def update_json(deployment_id, key_name, update_data):
    '''
    Updates the specified key with the provided data. Uses
    dict.update to do the merge, so be aware that keys may be overwritten
    '''
    content = load_json(deployment_id, key_name)
    content.update(update_data)
    save_json(deployment_id, key_name, content)

def add_connector(deployment_id, name, author, filename, args, description, local):
    deepy.cfg.init(deployment_id)
    basefn = os.path.basename(filename)
    base, ext = os.path.splitext(basefn)
    if not local:
        script_fname = '%s/scripts/%s' % (deepy.cfg.cache_connectors_dir,
                basefn)
        deepy.store.cache_save_to_remote(script_fname)

    connector = {'name':name,
                 'author':author,
                 'filename':basefn,
                 'args':args,
                 'description':description}

    #override and use installed local file
    if local:
        connector['local'] = True

    connector_fname = '%s/%s.json' % (deepy.cfg.cache_connectors_dir, base)
    deepy.store.simple_save_json(connector, connector_fname)

def setup_connectors(deployment_id, local=True, args=''):
    '''
    default connectors for deployments
    '''
    add_connector(deployment_id, "Peers", 'support@deepfield.net', 'peers.py', args, "Automatically build peers dimension from BGP data", local)
    add_connector(deployment_id, "Interfaces", 'support@deepfield.net', 'interfaces.py', args, "Router Interfaces", local)
    add_connector(deployment_id, "Routers", 'support@deepfield.net', 'routers.py', args, 'Router Connector', local)
    add_connector(deployment_id, "BGP Routers", 'support@deepfield.net', 'bgp_routers.py', args, 'BGP Routers.  Routers which the Deepfield system will peer with to collect BGP data.', local)
    add_connector(deployment_id, "BGP Communities", 'support@deepfield.net', 'community.py', args, 'BGP Communities.', local)
    add_connector(deployment_id, "Prefixes", 'support@deepfield.net', 'bgproute.py', args, 'Create top /24 prefixes.', local)
    add_connector(deployment_id, "DNSFlow", 'support@deepfield.net', 'dnsflow_connector.py', args, 'DNSFlow Connector', local)
    add_connector(deployment_id, "ASPaths", 'support@deepfield.net', 'aspaths.py', args, 'AS Paths Connector', local)
    add_connector(deployment_id, "DNS Resolvers", 'support@deepfield.net', 'dns_resolvers.py', args, 'DNS Resolvers Connector', local)
    add_connector(deployment_id, "PoPs", 'support@deepfield.net', 'pops.py', args, 'Points of Presence Connector', local)
    add_connector(deployment_id, "DSCP", 'support@deepfield.net', 'dscp.py', args, 'DSCP Connector', local)

# XXX Refactor to used s3 connection that is passed in. Requires touching deepy.store
def load_dimensions(deployment_id, dimensions_id='dimensions', map_id='map'):
    dimension_fns = deepy.store.ls_dir_remote(dimensions_id, deployment_id=map_id, force_remote='s3')
    map_bucket = deepy.store.s3_get_bucket(deployment_id=map_id)
    deploy_bucket = deepy.store.s3_get_bucket(deployment_id=deployment_id)
    for dfn in dimension_fns:
        mkey = map_bucket.get_key(dfn)
        s = mkey.get_contents_as_string()

        dkey = deploy_bucket.new_key(dfn)
        dkey.set_contents_from_string(s)

    udb = deepy.dimensions.DimensionsDBFrag(db_file=deepy.cfg.dimensions_db_file)
    udb.save()

    deepy.dimensions.merge_dimensions()
    deepy.dimensions.build_dimension_idx()

def update_notices(deployment_ids, copy_eula=True, copy_notice=True, s3=None, dry_run=False):
    '''
    Copy EULA and notice from s3://map/notices to deployments
    Deployment-specific notices are stored under notices/deployment,
    e.g. s3://map/notices/merit/eula.json.gz
    '''

    if not s3:
        raise ValueError("Must provide s3 connection")
    map_bucket = s3.get_bucket('map.pdrlabs.net')
    if not map_bucket:
        raise ValueError('Failed to get map bucket')

    def make_copy_tuple_fn(map_bucket, content_file):
        def fn(deployment_id):
            notices_dir = 'notices/'

            source_key = map_bucket.get_key(notices_dir + deployment_id + '/' + content_file)
            if not source_key:
                source_key = map_bucket.get_key(notices_dir + content_file)
                if not source_key:
                    log.warn('Cannot find notice source file for'.format(deployment_id))
                    return (None, None, None)
            dest_bucket = deployment_id + '.pdrlabs.net'
            dest_key_string = notices_dir + content_file
            return (source_key, dest_bucket, dest_key_string)
        return fn

    eula_tasks = []
    if copy_eula:
        eula_file = os.path.basename(deepy.cfg.eula_file)
        eula_tasks = map(make_copy_tuple_fn(map_bucket, eula_file), deployment_ids)

    notice_tasks = []
    if copy_notice:
        # notice package unpacked by notice_sync.py
        notice_package_file = os.path.basename(deepy.cfg.notice_package_file)
        notice_tasks = map(make_copy_tuple_fn(map_bucket, notice_package_file), deployment_ids)

    tasks = eula_tasks + notice_tasks
    copied = 0
    def pretty_key(key):
        return 's3://' + key.bucket.name + '/' + key.key
    for source_key, dest_bucket, dest_key_string in tasks:
        if not source_key:
            continue

        # This doesn't reliably work for some reason...
        #if not s3.lookup(dest_bucket):
        #    raise ValueError('{} does not seem to be a bucket'.format(dest_bucket))

        if dry_run:
            print 'Would copy',
        else:
            source_key.copy(dest_bucket, dest_key_string)
            print 'Copied',
            copied += 1

        print pretty_key(source_key), '->', 's3://' + dest_bucket + '/' + dest_key_string

    return copied


# XXX Refactor to used s3 connection that is passed in. Requires touching deepy.store
def load_map_files(deployment_id, map_id='map'):
    mfns = [
        #'maps/cloudmap.dfm',
        #'geoip/metrocodes.json.gz',
        #'geoip/geoipmap_new.json.gz',
        'maps/genome.h5',
        'maps/geoip.h5',
        'mine/master_names.json.gz'
    ]
    map_bucket = deepy.store.s3_get_bucket(deployment_id=map_id)
    deploy_bucket = deepy.store.s3_get_bucket(deployment_id=deployment_id)
    for mfn in mfns:
        mkey = map_bucket.get_key(mfn)
        s = mkey.get_contents_as_string()

        dkey = deploy_bucket.new_key(mfn)
        dkey.set_contents_from_string(s)

    deepy.dimensions.build_dimension_idx()

def generate_keypair():
    '''
    Generate a private/public key pair.

    Returns:
        key pair (tuple): public key is in ssh format (private_key:str, public_key:str, key instance)
    '''
    key = RSA.generate(2048, os.urandom)
    private_key = key.exportKey()
    public_key = key.publickey().exportKey("OpenSSH")
    return (private_key, public_key, key)

def get_key_pair_name(deployment_id):
    '''
    Returns the key pair name for the specified deployment
    '''
    key_pair_filename = 'config/key-pair-name'
    key_pair_data = load_json(deployment_id, key_pair_filename)
    key_pair_name = None
    if key_pair_data:
        key_pair_name = key_pair_data['key_pair_name']
    else:
        key_pair_name = str(uuid.uuid4())
        save_json(
            deployment_id,
            key_pair_filename,
            {'key_pair_name': key_pair_name}
        )

    return key_pair_name

def ensure_iam_user(deployment_id, deployment_config, iam):
    '''
    Ensures IAM user for this deployment exists
    '''
    # Create user
    user, group = get_deployment_user_and_group(deployment_id, deployment_config)
    try:
        iam.create_user(user)
    except Exception as e:
        pass

def ensure_iam_group(deployment_id, deployment_config, iam):
    '''
    Ensures IAM group for this deployment exists
    '''
    # Create user
    user, group = get_deployment_user_and_group(deployment_id, deployment_config)
    try:
        iam.create_group(group)
    except Exception as e:
        pass
    iam.add_user_to_group(group, user)
    iam.add_user_to_group('deployment', user)
    iam.put_group_policy(group, 'S3Policy', json.dumps(generate_s3_policy(deployment_id)))
    iam.put_group_policy(group, 'SQSPolicy', json.dumps(generate_sqs_policy(deployment_id)))
    iam.put_group_policy(group, 'EC2Policy', json.dumps(generate_ec2_policy(deployment_id)))

    if deployment_id == "status":
        iam.put_group_policy(group, 'S3Policy', deepy.deploy.get_s3_policy_status() )

def delete_all_user_access_keys(username, iam):
    '''Deletes all access keys for the specified user'''
    response = iam.get_all_access_keys(username)
    key_list = response['list_access_keys_response']['list_access_keys_result']['access_key_metadata']
    for key in key_list:
        iam.delete_access_key(key['access_key_id'], user_name=username)

def ensure_credentials(deployment_id, deployment_config, s3, iam):
    '''Ensure valid credentials are in deployment descriptor'''

    slice_config = get_slice_config(deployment_id)
    if deployment_config is None:
        deployment_config = {}
    if slice_config is None:
        slice_config = {}

    username, group = get_deployment_user_and_group(deployment_id, deployment_config)
    if deployment_config.get('credentials'):
        credentials = deployment_config['credentials']
        access_key_id, secret_access_key = credentials
        response = iam.get_all_access_keys(username)
        key_list = response['list_access_keys_response']['list_access_keys_result']['access_key_metadata']
        for key in key_list:
            if access_key_id == key['access_key_id']:
                return [access_key_id, secret_access_key]

    # Key not found, making a new one, after removing the old ones

    # Delete all old access keys
    delete_all_user_access_keys(username, iam)

    # Create a new one
    response = iam.create_access_key(username)
    key = response['create_access_key_response']['create_access_key_result']['access_key']

    # Save it off
    credentials = key['access_key_id'], key['secret_access_key']
    deployment_config['credentials'] = credentials
    slice_config['credentials'] = credentials

    save_deployment_config(deployment_id, deployment_config)
    save_slice_config(deployment_id, slice_config)

    return credentials

def ensure_deployment_key_pair(deployment_id, ec2=None):
    '''
    Ensures that
    a) There is a key pair in ec2 of the same name as the deployment
        and
    b) There is a private key in <deployment_id>.pdrlabs.net/config/deployment.key
    '''
    deepy.cfg.init(deployment_id)
    deployment_key_filename = os.path.join(
        deepy.cfg.cache_dir,
        'config/deployment.key'
    )
    deployment_pub_key_filename = os.path.join(
        deepy.cfg.cache_dir,
        'config/deployment.key.pub'
    )
    deepy.store.cache_load_from_remote(deployment_key_filename)
    deployment_key_exists = os.path.exists(deployment_key_filename)

    public_key = private_key = None
    if not deployment_key_exists:
        private_key, public_key, key = generate_keypair()
        deepy.util.ensure_directory(os.path.dirname(deployment_key_filename))
        with open(deployment_key_filename, 'w') as f:
            f.write(private_key)
        with open(deployment_pub_key_filename, 'w') as f:
            f.write(public_key)
        deepy.store.cache_save_to_remote(deployment_key_filename)
        deepy.store.cache_save_to_remote(deployment_pub_key_filename)
    else:
        with open(deployment_key_filename) as f:
            private_key = f.read()

    if os.path.exists(deployment_pub_key_filename) and not public_key:
        with open(deployment_pub_key_filename) as f:
            public_key = f.read()

    if not public_key:
        key_pair = RSA.importKey(private_key)
        public_key = key_pair.publickey().exportKey("OpenSSH")
        with open(deployment_pub_key_filename, 'w') as f:
            f.write(public_key)

    key_pair_name = get_key_pair_name(deployment_id)
    if ec2 and ec2.get_key_pair(key_pair_name) is None:
        ec2.import_key_pair(key_pair_name, public_key)

    return private_key, public_key

def bucket_exists(bucket_name, s3):
    '''Check to see if specified bucket exists

    Args:
        bucket_name (str): The bucket to create
    Kwargs:
        s3 (s3connection): The boto s3 connection to use. Will create if not
            specified.
    Returns:
        bool: True if bucket exists, false otherwise
    '''
    found_bucket = True if s3.lookup(bucket_name) is not None else False
    return found_bucket

def set_bucket_tags(bucket, tags):
    '''
    Set the key -> value tags for an S3 bucket.
    '''
    tag_set = boto.s3.tagging.TagSet()
    for k, v in tags.items():
        tag_set.add_tag(k, v)

    tags = boto.s3.tagging.Tags()
    tags.add_tag_set(tag_set)
    bucket.set_tags(tags)

def ensure_bucket(bucket_name, s3, location=Location.DEFAULT):
    '''Ensure specified bucket exists.

    If the specified bucket exists, do nothing. Otherwise, create the bucket.

    Args:
        bucket_name (str): The bucket to create
    Kwargs:
        s3 (s3connection): The boto s3 connection to use. Will create if not
            specified.
        location (boto.s3.connection.Location): The location to use for bucket
            creation.
    Returns:
        bool: True if created, False otherwise
    '''

    found_bucket = bucket_exists(bucket_name, s3=s3)

    if not found_bucket:
        s3.create_bucket(bucket_name, location=location)

        deployment_id = bucket_name.replace('.pdrlabs.net', '')
        tags = {
            'Name': deployment_id,
            'deployment': deployment_id,
            'creator': get_id()
        }
        bucket = s3.get_bucket(bucket_name)
        set_bucket_tags(bucket, tags)

        return True

    return False

def deployment_bootstrapped(deployment_id):
    '''
    Returns True if deployment with specified ID has already been bootstrapped,
    false otherwise.

    Args:
        deployment_id (str): The deployment ID to check
    Kwargs:
        s3 (s3connection): The boto s3 connection to use. Will create if not
            specified.
    Returns:
        bool: True if deployment id bootstrapped, False otherwise
    '''
    deepy.cfg.init(deployment_id)
    deployment_json = deepy.store.simple_load_json(os.path.join(deepy.cfg.cache_dir, 'config/deployment.json'))
    return deployment_json is not None

def get_initial_slice(deployment_id):
    return {
        'deployment': deployment_id,
        'customer_id': deployment_id,
        'ignore_alerts': True,
        'roles': ["riemann.vm"]
    }

def get_initial_deployment_config(deployment_id, kind='aws', region=None):
    deployment_uuid = str(uuid.uuid4())

    result = {}
    result['uuid'] = deployment_uuid
    result['deployment'] = deployment_id
    result['master'] = None
    result['kind'] = kind

    if region:
        result['region'] = region

    return result

def find_staging_file(filename):
    '''
    Get path to "staged" file, based on VM role.  Search in "shared"
    directory, override w/ vm_role-specific file.

    @param[in] filename Path to file to search for, from "top level"
                        VM-specific/shared staging directory.
    @return Path to file
    '''
    staging_dir = deepy.cfg.staging_dir

    shared_path = staging_dir + '/shared/' + filename

    if os.path.exists(shared_path):
        return shared_path

    return None

def create_initial_config_files(deployment_id, kind, region, branch=None):
    '''
    Creates the initial config files
    '''
    deepy.cfg.init(deployment_id)

    # Write initial (blank) config files
    for filename in (
        'users.json',
        'ui.json',
        'tags.json',
        'tile_config.json',
        'interface_regexp.json',
        'setup.json',
        ):
        resolved_filename = find_staging_file(filename)
        if not resolved_filename:
            log.error("Failed to find staging file {}".format(filename))
            continue

        config_key = os.path.join(deepy.cfg.cache_dir, 'config', filename)
        deepy.store.cache_load_from_remote(config_key)

        if not os.path.exists(config_key):
            with open(resolved_filename, 'rb') as f:
                content = f.read()
                deepy.store.simple_save_content(content, config_key)
        else:
            log.error("Not creating config file {} because it already exists".format('config/'+filename))
            deepy.store.cache_save_to_remote(config_key)


    # Create initial slice
    initial_slice = get_initial_slice(deployment_id)
    if kind == "isolated":
        initial_slice['isolated_deployment'] = True

    if branch is not None:
        initial_slice['build_updates'] = {'revision': branch}

    deepy.store.simple_save_json(initial_slice,
        os.path.join(deepy.cfg.cache_dir, 'config/slice.json'))

    # Create initial deployment json
    initial_deployment_config = get_initial_deployment_config(deployment_id,
        kind=kind, region=region)
    deepy.store.simple_save_json(initial_deployment_config,
        os.path.join(deepy.cfg.cache_dir, 'config/deployment.json'))

def get_provision_dest():
    if deepy.cfg.hood_dir == '/':
        provision_dest = '/pipedream/provision'
    else:
        provision_dest = os.path.join(deepy.cfg.hood_dir, 'provision')

    return provision_dest

def sync_provision_directory():
    '''Sync the local provision directory with the remote one'''

    # Sync our provisioning directory from S3
    os.chdir(deepy.util.get_code_root())
    provision_dest = get_provision_dest()
    deepy.util.ensure_directory(provision_dest)

    subprocess.call([
            'aws',
            's3',
            'sync',
            's3://download.deepfield.net/provision/',
            provision_dest
        ]
    )

def build_installer():
    '''
    Builds a tar file with all of the wheels and apt packages necessary to
install our software on a completely isolated deployment

    '''

    provision_dest = get_provision_dest()
    sync_provision_directory()

    # Override the bootstrap-salt script with our customized one
    shutil.copyfile(
        os.path.join(deepy.util.get_code_root(), 'deployment', 'installer', 'bootstrap-salt.sh'),
        os.path.join(provision_dest, 'bootstrap-salt.sh')
    )


    # Clone git repos into provision directory
    retval = subprocess.call([
            'git', 'clone',
            'git@github.com:deepfield/pipedream.git',
            os.path.join(provision_dest, 'pipedream')
        ]
    )
    if not retval == 0:
        os.chdir(os.path.join(provision_dest, 'pipedream'))
        subprocess.call([
                'git', 'pull',
            ]
        )

    # Tar the resulting directory
    subprocess.call("rm -rf deepfield-installer.tar.gz", shell=True)
    os.chdir(deepy.util.get_code_root())
    subprocess.call([
            'tar',
            '-czvf',
            'deepfield-installer.tar.gz',
            '-C',
            provision_dest,
            '.'
        ]
    )
    return os.path.abspath('deepfield-installer.tar.gz')

def build_ami():
    deployment = AWSDeployment('build-ami')
    deployment.bootstrap_deployment()
    try:
        deployment.create_master(make_mount=False)
        deployment.sudo('apt-get update; apt-get update') # That's right, do it twice, ubuntu AMI weirdness
        deployment.sudo('apt-get install -y python-pip git build-essential autoconf curl')
        deployment.sudo('pip install awscli')
        deployment.sudo('pip install GitPython==0.3.2.RC1 gitdb')
        deployment.sudo("curl -L https://bootstrap.saltstack.com -o bootstrap-salt.sh && sh bootstrap-salt.sh -X")
        instance = deployment.get_master_instance()
        image_id = deployment.ec2.create_image(instance.id, 'salt-base-{}'.format(str(uuid.uuid4())), description="Salt base image", no_reboot=False)
        log.info("Created image with id {}".format(image_id))
        tags = {
            'image_type': 'salt',
            'description': "Salt base image",
            'creator': get_id()
        }
        deployment.ec2.create_tags([image_id], tags)
        log.info("Created tags")

        salt_images = deployment.ec2.get_all_images(filters={'tag:image_type': 'salt'})

        for image in salt_images:
            if image.id != image_id:
                image.deregister()
    finally:
        deployment.destroy()

def copy_ami(ami_id, from_region, to_region, name=None):
    if name is None:
        name = '{}-{}'.format(to_region, ami_id)

    from_conn = boto.ec2.connect_to_region(from_region)
    from_image = from_conn.get_image(ami_id)
    from_tags = from_image.tags

    to_conn = boto.ec2.connect_to_region(to_region)
    to_image = to_conn.copy_image(from_region, ami_id, name=name)
    to_image_id = to_image.image_id
    to_conn.create_tags([to_image_id], from_tags)

    print "AMI in region {} and ID {} copied to region {} with new ID {}".format(
        from_region, ami_id, to_region, to_image_id)

def get_instance(deployment_id, minion_id=None):
    deployment = get_deployment(deployment_id)
    instance = (deployment.get_minion_instance(minion_id) if minion_id else
        deployment.get_master_instance())
    return instance, deployment

def get_route53_records(r53, zone_name=DEEPFIELD_ZONE_NAME):
    zone = r53.get_zone(zone_name)
    return zone.get_records()

def get_reservations(ec2):
    reservations = ec2.get_all_instances()
    return reservations

def get_dns_ip_from_reservations(reservations):
    dns = {i.dns_name: (i.id, r.id) for r in reservations
        for i in r.instances}
    ip = {i.ip_address: (i.id, r.id) for r in reservations
        for i in r.instances}
    return dns, ip

def get_resource_records_from_records(records):
    return {r.name: r.resource_records for r in records}

def list_all_route53_entries(r53, ec2):
    if r53 is None:
        raise ValueError("Route53 Connection not provided")
    if ec2:
        raise ValueError("EC2 Connection not provided")

    records = get_route53_records(r53)
    resource_records = get_resource_records_from_records(records)

    reservations = get_reservations(ec2)
    dns, ip = get_dns_ip_from_reservations(reservations)

    print_route53_entries(dns, ip, resource_records)

def print_route53_entries(dns, ip, records_dict):
    print "{: <40} {: <50} {: <35}".format("ROUTE53", "DNS", "INSTANCE/RESERVATION")
    for name, records in records_dict.items():
        for record in records:
            if dns.get(record):
                print "{: <40} {: <50} {: <35}".format(name, record,
                    dns[record])
            elif ip.get(record):
                print "{: <40} {: <50} {: <35}".format(name, record,
                    ip[record])
            else:
                print "{: <40} {: <50} {: <35}".format("", record, "")

def associate_new_address(instance_id, ec2):
    if not ec2:
        raise ValueError("EC2 Connection not provided")
    address = ec2.allocate_address()
    public_ip = address.public_ip
    ec2.associate_address(instance_id, public_ip)
    return public_ip

def create_elastic_ip(deployment_id, name=None, minion_id=None,
        record_type=None, ec2=None):
    instance, deployment = get_instance(deployment_id, minion_id=minion_id)
    if not isinstance(deployment, AWSDeployment):
        raise TypeError("Non-AWS deployment, no route53 {}".format(deployment_id))

    ip = associate_new_address(instance.id, ec2)

    route53_name = deployment.update_route53_entry(ip, name=name,
        minion_id=minion_id, record_type=record_type)

    kwargs = {
        "private_dns_address": instance.private_dns_name,
        "private_ip_address": instance.private_ip_address,
        "public_dns_address": instance.public_dns_name,
        "public_ip_address": ip,
        "ssh_address": ip,
        "r53_address": route53_name
    }
    if minion_id:
        kwargs['minion_id'] = minion_id
    deployment.set_connection_info(**kwargs)

def list_elastic_ips(deployment_id=None, minion_id=None, ec2=None, r53=None,
        name=None, all_ids=None, record_type=None):
    # If the {deployment, minion}_id or name isn't given, list all route53 entries
    if deployment_id or minion_id or name:
        instance, deployment = get_instance(deployment_id, minion_id=minion_id)
        if not isinstance(deployment, AWSDeployment):
            raise TypeError("Non-AWS deployment, no route53 {}".format(deployment_id))

        deployment.list_route53_entries(name=name, minion_id=minion_id,
            all_ids=all_ids, record_type=record_type)
    else:
        list_all_route53_entries(r53, ec2)

def migrate_deployment(deployment_id, master_host, kind, ssh_address=None,
        ssh_port=None, ui_address=None, ui_port=None, ec2=None):
    '''Migrates deployment from old slice-only based scheme
    to scheme with new deployment.json file

    Note: You can call this function multiple times without bad effects, UNLESS
    you have hand-edited deployment.json, in which case your changes will be
    lost.

    Side effects:
        - Create config/deployment.json in deployment's bucket
        - Ensure that a keypair with the same name as the deployment exists in
          s3, and that config/deployment.key is populated with the associated
          private key
    '''

    slice_config = get_slice_config(deployment_id)
    uuid = None

    if len(slice_config['vms']) > 1:
        for vm in slice_config['vms']:
            if 'home.py' in vm['daemons']:
                uuid = vm['uuid']
                break
        else:
            raise ValueError("Failed to find master VM")
    else:
        uuid = slice_config['vms'][0]['uuid']
    master_ip = socket.gethostbyname(master_host)

    deployment_config = get_initial_deployment_config(deployment_id,
        kind=kind)
    deployment_config['master'] = {
        'public_ip_address': master_ip,
        'public_dns_address': master_host,
        'initialized': True
    }

    # Set SSH info if it's present
    if ssh_port:
        deployment_config['master']['ssh_port'] = int(ssh_port)

    # Set UI info if it's present
    if ui_address:
        deployment_config['ui_address'] = ui_address
    if ui_port:
        deployment_config['ui_port'] = ui_port

    # Copy credentials from slice
    if 'credentials' in slice_config and len(slice_config['credentials']) == 2:
        credentials = slice_config['credentials']
        deployment_config['credentials'] = credentials

    save_deployment_config(deployment_id, deployment_config)
    print "New deployment config written"
    print json.dumps(deployment_config, indent=2)
    ensure_deployment_key_pair(deployment_id, ec2=ec2)

def get_deployment_config(deployment_id):
    '''Return a dictionary with the deployment configuration'''
    return load_json(deployment_id, 'config/deployment.json')

def get_slice_config(deployment_id):
    '''Return a dictionary with the deployment configuration'''
    return load_json(deployment_id, 'config/slice.json')

def save_deployment_config(deployment_id, deployment_config):
    '''Save deployment config for specified eployment

    Args:
        deployment_id (string)
        deployment_config (dict)
        s3 (boto s3 connection)

    '''
    return save_json(deployment_id, 'config/deployment.json', deployment_config)

def save_slice_config(deployment_id, slice_config):
    '''Save slice_config config for specified deployment

    Args:
        deployment_id (string)
        slice_config (dict)
        s3 (boto s3 connection)

    '''
    return save_json(deployment_id, 'config/slice.json', slice_config)

def generate_policy_statement(actions, effect, resource):
    return {
        'Action': actions,
        'Effect': effect,
        'Resource': resource
    }


def generate_s3_policy(deployment_id):
    '''
    Generate s3 policy for specified deployment
    '''
    policy = {'Statement':[]}
    statements = policy['Statement']
    slice_config = get_slice_config(deployment_id)

    deployment_master = deepy.util.recursive_get(slice_config, "deployment_master", "deployment_id")

    # Deployment has full control over its own bucket
    statements.append(
        generate_policy_statement(
            actions=['s3:*'],
            effect='Allow',
            resource='arn:aws:s3:::{}.pdrlabs.net*'.format(deployment_id)
        )
    )

    # List-only access to these
    list_only_bucket_paths = (
        'map.pdrlabs.net',
    )

    for specifier in list_only_bucket_paths:
        statements.append(
            generate_policy_statement(
                actions=['s3:List*'],
                effect='Allow',
                resource='arn:aws:s3:::'+specifier
            )
        )

    # Read-only access to these
    read_only_bucket_paths = (
        'map.pdrlabs.net/maps*',
        'map.pdrlabs.net/rocketfuel/rocketfuel.json.gz*',
        'map.pdrlabs.net/domains*',
        'map.pdrlabs.net/ssl*',
        'map.pdrlabs.net/dimensions*',
        'map.pdrlabs.net/whois*',
        'map.pdrlabs.net/sites_db*',
        'map.pdrlabs.net/linkedin*',
        'map.pdrlabs.net/geoip*',
        'download.deepfield.net/*',
    )
    for specifier in read_only_bucket_paths:
         statements.append(
            generate_policy_statement(
                actions=['s3:Get*'],
                effect='Allow',
                resource='arn:aws:s3:::'+specifier
            )
        )

    # Read and list access to these
    read_list_bucket_paths = (
        'map.pdrlabs.net/routeviews*',
        'map.pdrlabs.net/logos*',
        'map.pdrlabs.net/components*',
        'subscription.pdrlabs.net*',
    )
    if deployment_master:
        read_list_bucket_paths += tuple(["{}*".format(get_bucket_name_for_deployment(deployment_master))])

    for specifier in read_list_bucket_paths:
         statements.append(
            generate_policy_statement(
                actions=['s3:Get*', 's3:List*'],
                effect='Allow',
                resource='arn:aws:s3:::'+specifier
            )
        )

    return policy

def generate_sqs_policy(deployment_id):
    '''
    Generate SQS policy for specified deployment
    '''
    policy = {'Statement':[]}
    statements = policy['Statement']

    # Deployment has full access to its own jobs queue
    statements.append(
        generate_policy_statement(
            actions=['sqs:*'],
            effect='Allow',
            resource='arn:aws:sqs:*:*:jobs_{}'.format(deployment_id)
        )
    )

    # Deployment has full access to any other deployment queues
    statements.append(
        generate_policy_statement(
            actions=['sqs:*'],
            effect='Allow',
            resource='arn:aws:sqs:*:*:{}*'.format(deployment_id)
        )
    )

    # Deployment can list other queues
    statements.append(
        generate_policy_statement(
            actions=['sqs:ListQueues'],
            effect='Allow',
            resource='arn:aws:sqs:*:*:*'
        )
    )

    # Deployment can send messages to mining queue
    statements.append(
        generate_policy_statement(
            actions=['sqs:SendMessage', 'sqs:GetQueue'],
            effect='Allow',
            resource='arn:aws:sqs:*:*:mining*',
        )
    )

    return policy

def generate_ec2_policy(deployment_id):
    '''
    Generate EC2 policy for specified deployment
    '''

    policy = {'Statement':[]}
    statements = policy['Statement']
    actions = (
        'ec2:RebootInstances',
        'ec2:RequestSpotInstances',
        'ec2:ResetInstanceAttribute',
        'ec2:RunInstances',
        'ec2:StartInstances',
        'ec2:StopInstances',
       # 'ec2:TerminateInstances',
        'ec2:AssignPrivateIpAddresses',
        'ec2:AssociateAddress',
        'ec2:AttachVolume',
        'ec2:CreateKeyPair',
        'ec2:CreateNetworkInterface',
        'ec2:CreatePlacementGroup',
    )

    statement = generate_policy_statement(
            actions=actions,
            effect='Allow',
            resource='arn:aws:ec2:*:*:*'
    )
    statement['Condition'] = {
        "ForAnyValue:StringEquals": {
          "ec2:ResourceTag/deployment": deployment_id
        }
    }
    statements.append(statement)
    return policy

def get_deployment(deployment_id, kind=None, s3=None, ec2=None, r53=None, iam=None, region=None):
    '''
    Gets Deployment implementation for specified deployment_id
    '''

    if deepy.cfg.isolated_deployment:
        kind = 'isolated'
        region = DEFAULT_REGION

    # Discover kind and region if not specified
    deployment_config = {}
    if not region or not kind:
        deployment_config = get_deployment_config(deployment_id)

    if not kind:
        kind = deployment_config.get('kind')
        if not kind:
            raise ValueError("Could not determine 'kind' for deployment '{}'".format(deployment_id))

    if not region:
        region = deployment_config.get('region') or DEFAULT_REGION


    # Connect to AWS
    if not s3:
        s3 = boto.s3.connect_to_region(region)
    if not ec2:
        ec2 = boto.ec2.connect_to_region(region)
    if not r53:
        r53 = boto.connect_route53()
    if not iam:
        iam = boto.connect_iam()

    if kind == 'aws':
        return AWSDeployment(deployment_id, s3=s3, ec2=ec2, iam=iam, r53=r53)
    elif kind == 'private':
        return PrivateDeployment(deployment_id, s3=s3, r53=r53, iam=iam)
    elif kind == 'isolated':
        return IsolatedDeployment(deployment_id)
    elif kind == 'shadow':
        return ShadowDeployment(deployment_id, s3=s3, ec2=ec2, iam=iam, r53=r53)
    elif kind == 'temporary':
        return TemporaryDeployment(deployment_id, {}, s3=s3, ec2=ec2, iam=iam, r53=r53)
    else:
        raise ValueError('Deployment of type {} is not recognized'.format(kind))

def get_deployment_user_and_group(deployment_id, deployment_config):
    '''Return tuple (user, group)'''
    if deployment_config is None:
        deployment_config = {}
    override_user = deployment_config.get('aws_user')
    override_group = deployment_config.get('aws_group')
    user = override_user or ("%s_user" % deployment_id)
    group = override_group or ("%s_group" % deployment_id)
    return user, group

class Deployment(object):

    def __init__(self, deployment_id):
        self.deployment_id = deployment_id

    def get_ui_address(self):
        deployment_config = self.get_deployment_config()
        ui_address = (deployment_config.get('ui_address')
            or deployment_config.get('master', {}).get('ui_address')
            or deployment_config.get('master', {}).get('public_ip_address')
        )
        return ui_address

    def get_ui_port(self):
        deployment_config = self.get_deployment_config()
        ui_port = (deployment_config.get('ui_port')
            or deployment_config.get('master', {}).get('ui_port')
            or 443
        )
        return ui_port
    @abc.abstractmethod
    def create_master(self, **kwargs):
        '''Create the master for this deployment'''
        raise NotImplementedError()

    @abc.abstractmethod
    def initialize_master(self, **kwargs):
        '''Initialize the master for this deployment'''

    @abc.abstractmethod
    def destroy(self, preserve_bucket=False):
        '''Destroy this deployment'''
        raise NotImplementedError()

    @abc.abstractmethod
    def check(self):
        '''Return a dictionary of status checks'''
        raise NotImplementedError()

    @abc.abstractmethod
    def get_master_instance(self):
        '''
        Return the boto.Instance object that is associated with the master. Raises NotImplementedError
        if we don't have an AWS Instance
        '''
        raise NotImplementedError()

    def get_minion_instance(self, minion_id):
        raise NotImplementedError

    def get_aws_command(self):
        '''
        Returns the appropriate AWS command, sometimes HTTPS_PROXY=blah aws, sometimes aws
        '''
        deepy.cfg.init(self.deployment_id)
        proxy_connection_info = deepy.cfg.proxy_info
        if deepy.cfg.proxy_deployment:
            prefix = 'HTTPS_PROXY=http://{ip}:{port}'.format(ip=proxy_connection_info['ip'], port=proxy_connection_info['port'])
        else:
            prefix = ''

        return prefix + ' aws'

    def setup_fabric(self, **kwargs):
        '''Set up fabric'''
        env.forward_agent = True

        # Overrides
        host = kwargs.get('public_ip_address') or kwargs.get('host')
        port = kwargs.get('port')
        user = kwargs.get('ssh_user') or kwargs.get('user')

        minion_ids = kwargs.get('minion_ids')
        master_definition = self.get_master_definition()

        deployment_config = self.get_deployment_config()

        if port:
            env.port = port

        # Get appropriate host name and port
        if host:
            env.hosts = ['{}@{}'.format('support', host)]

        elif not minion_ids:
            if master_definition is None:
                raise ValueError("Failed to retrieve master definition for deployment {}".format(self.deployment_id))

            if master_definition.get('public_ip_address'):
                ip = master_definition.get('public_ip_address')
            else:
                ip = master_definition.get('host')

            if not port:
                env.port = master_definition.get('ssh_port') or 22

            if not ip:
                raise ValueError("Do not know how to connect to master {}".format(self.deployment_id))

            username = self.get_ssh_user()
            hostname = '{}@{}:{}'.format(username, ip, master_definition.get('ssh_port') or 22)
            env.hosts = [hostname]

        else:
            minions = deployment_config.get('minions') or {}
            hosts = []

            for min_id in minion_ids:
                minion_definition = minions.get(min_id)

                if not minion_definition:
                    raise ValueError("Failed to retrieve minion definition for deployment {}".format(min_id))

                if minion_definition.get('public_ip_address'):
                    ip = minion_definition.get('public_ip_address')
                else:
                    ip = minion_definition.get('host')

                if not ip:
                    raise ValueError("Do not know how to connect to minion {}".format(min_id))

                username = self.get_ssh_user(minion_id=min_id)
                hostname = '{}@{}:{}'.format(username, ip, minion_definition.get('ssh_port') or 22)

                hosts.append(hostname)

            env.hosts = hosts


        # Get username and setup keys
        if not master_definition.get('initialized') or minion_ids:
            private_key, public_key = self.ensure_deployment_keypair()
            tempfile_directory = os.path.join(deepy.cfg.hood_dir, 'tmp')
            deepy.util.ensure_directory(tempfile_directory)
            with tempfile.NamedTemporaryFile(delete=False, dir=tempfile_directory, prefix='tmpkey') as fp:
                fp.write(private_key)
                log.debug("Using key pair at {}".format(fp.name))
                env.key_filename = fp.name



    def _get_uuid(self, minion_id=None):
        '''
        Returns a string representing the UUID
        '''

        # First check our configuration
        if minion_id:
            config = deepy.util.recursive_get(self.get_deployment_config(),
                "minions", minion_id, default={})
        else:
            config = self.get_master_definition()
        if config.get("uuid"):
            return config.get("uuid")

        minion_ids = [] if minion_id is None else [minion_id]

        # Next try our AWS instance ID
        try:
            instance = (self.get_minion_instance(minion_id=minion_id) if
                minion_id else self.get_master_instance())
            if instance is None:
                raise ValueError("No instance is running for this deployment")
            uuid = instance.id
        # Finally check local system
        except NotImplementedError:
            uuid = self.sudo('dmidecode -s system-uuid', minion_ids=minion_ids).values()[0]
            # Need to trim out comments because of stupid output of dmidecode
            tokenized = uuid.split('\n')
            uuid = tokenized[-1]

        if not uuid:
            # Try dbus
            uuid = self.sudo('cat /var/lib/dbus/machine-id', minion_ids=minion_ids).values()[0] + '-' + (minion_id or 'master')

        # Error out
        if not uuid:
            raise ValueError("Failed to find UUID")

        return uuid

    def set_hostname(self, hostname, minion_ids=None):
        '''
        Sets the hostname by writing to /etc/hostname

        If minion_ids is none, set hostname on master, otherwise, set on specified minions
        '''

        def _set_hostname():
            put(StringIO(hostname), '/etc/hostname', use_sudo=True)
            sudo('hostname {}'.format(hostname))

        self.execute(_set_hostname, minion_ids=minion_ids)

    def get_master_uuid(self):
        '''
        Returns a string representing the UUID of the master
        '''
        return self._get_uuid()

    def get_minion_uuid(self, minion_id):
        '''
        Returns a string representing the UUID of the minion
        '''
        return self._get_uuid(minion_id=minion_id)

    def refresh_master_ip(self):
        '''
        Updates hosts files on all machines in the cluster to point to the current master IP
        '''
        master_definition = self.get_master_definition()
        master_ip = master_definition.get("private_ip_address", master_definition['public_ip_address'])
        hostnames = ['salt', 'master.{}.deepfield.net'.format(self.deployment_id),
            'master-{}.deepfield.net'.format(self.deployment_id),
            'salt-master', 'master']
        def update_commands():
            for hostname in hostnames:
                sudo('salt-call --local state.single host.present name={} ip={}'.format(hostname, master_ip) )

        deployment_config = self.get_deployment_config()
        minions = deployment_config.get('minions') or {}
        self.execute(update_commands, minion_ids=list(minions.keys()))

    def setup_connectors(self, local=True):
        '''Do initial set up of connectors'''
        setup_connectors(self.deployment_id, local=local)

    def finalize_master_creation(self):
        '''Finalize creation of deployment'''
        # Set up the VM section in slice
        slice_config = self.get_slice_config()
        deployment_config = self.get_deployment_config()
        master_config = deployment_config.get('master') or {}
        deployment_config['master'] = master_config

        # Get UUID
        master_uuid = self.get_master_uuid()

        vms = slice_config.get('vms', [])
        master_vms = filter(lambda x: x.get('uuid') == master_uuid, vms)
        if len(master_vms) == 0:
            vm = {}
            vms.append(vm)
            slice_config['vms'] = vms
        else:
            vm = master_vms[0]

        # Set UUID
        vm['uuid'] = master_uuid
        master_config['uuid'] = master_uuid

        # Set name
        vm['name'] = self.deployment_id + "-master"
        vm['daemons'] = ['home.py', 'redirect.py', 'jobs_daemon.py']
        self.save_slice_config(slice_config)
        self.save_deployment_config(deployment_config)

        # Set UUID
        self.sudo('mkdir -p /pipedream')
        self.execute(lambda: put(StringIO(master_uuid), '/pipedream/uuid.txt', use_sudo=True))

    def add_minion_to_config(self, minion_id, host=None, instance=None,
            private_dns_address=None, public_ip_address=None,
            private_ip_address=None, ssh_address=None, ssh_port=None,
            ssh_user=None, ui_address=None, ui_port=None):
        '''
        Adds a minion to the deployment.

        If instance is passed, it will set most of the parameters for you
        '''
        public_dns_address = host
        if instance:
            public_dns_address = instance.public_dns_name
            private_dns_address = instance.private_dns_name
            public_ip_address = instance.ip_address
            private_ip_address = instance.private_ip_address
            ssh_address = instance.ip_address
            ssh_port = 22
            ui_port = 443
        else:
            if not public_dns_address:
                raise ValueError("Must specify at least a host name when adding a minion")
            if not private_dns_address:
                private_dns_address = public_dns_address
            if not public_ip_address:
                public_ip_address = socket.gethostbyname(public_dns_address)
            if not private_ip_address:
                private_ip_address = public_ip_address
            if not ssh_port:
                ssh_port = 22
            if not ssh_address:
                ssh_address = public_ip_address
            if not ui_port:
                ui_port = 433

        # Add to deployment config
        deployment_config = self.get_deployment_config()
        minions = deployment_config.get('minions') or {}
        deployment_config['minions'] = minions

        # Make sure there are no minions with the same minion id already in the deployment config
        if minion_id in minions:
            raise ValueError("Minion with the specified ID already exists")
        minions[minion_id] = {
            'minion_id': minion_id,
            'public_dns_address': public_dns_address,
            'private_dns_address': private_dns_address,
            'public_ip_address': public_ip_address,
            'private_ip_address': private_ip_address,
            'ssh_address': ssh_address,
            'ssh_port': ssh_port,
            'ui_port': ui_port,
            'ssh_user': ssh_user

        }
        if ui_address:
            minions[minion_id]["ui_address"] = ui_address
        self.save_deployment_config(deployment_config)

        minion_uuid = self.update_minion_uuid(minion_id)
        self.create_minion_vm_section(minion_id, minion_uuid)


    def update_minion_uuid(self, minion_id):
        deployment_config = self.get_deployment_config()
        minions = deployment_config.get('minions') or {}
        deployment_config['minions'] = minions
        minion_uuid = self.get_minion_uuid(minion_id)
        minions[minion_id]['uuid'] = minion_uuid
        self.save_deployment_config(deployment_config)
        def _update_uuid():
            sudo('mkdir -p /pipedream')
            put(StringIO(minion_uuid), '/pipedream/uuid.txt', use_sudo=True)

        self.execute(_update_uuid, minion_ids=[minion_id])
        return minion_uuid

    def create_minion_vm_section(self, minion_id, minion_uuid):
        '''
        Creates the minion's vm section in slice.
        By default no cron jobs are running (cron_jobs: [])
            and 1 bundle is set to run
        '''
        slice_config = self.get_slice_config()
        existing_vm = None
        vms = slice_config.get('vms') or []
        slice_config['vms'] = vms
        for vm in slice_config.get('vms') or []:
            if vm['uuid'] == minion_uuid:
                log.warn("Updating VM instead of adding")
                existing_vm = vm
                break

        if not existing_vm:
            existing_vm = {}
            vms.append(existing_vm)


        vm = {
            "uuid": minion_uuid,
            "name": minion_id,
            "cron_jobs": [],
            "daemons": ["jobs_daemon.py"],
            "job_queue_config": {
                "queues": {
                    "slow": {
                        "max": 1,
                        "patterns": [
                            "bundle2.py"
                        ],
                        "queue_retention": 86400
                    }
                }
            }
        }
        existing_vm.update(vm)
        self.save_slice_config(slice_config)
        return slice_config

    def remove_minions(self, minion_ids):
        '''
        Remove a minion with the specified minion id
        '''
        deployment_config = self.get_deployment_config()
        minions = deployment_config.get('minions') or {}
        for minion_id in minion_ids:
            minion = minions.get(minion_id)
            if minion is None:
                raise KeyError("Minion with id {} not found in deployment config".format(minion_id))

            del minions[minion_id]

        self.save_deployment_config(deployment_config)

        slice_config = self.get_slice_config()
        vms = slice_config.get('vms') or []
        minion_id_set = set(minion_ids)
        vms = filter(lambda x: x.get('name') not in minion_id_set, vms)
        slice_config['vms'] = vms
        self.save_slice_config(slice_config)


    @parallel
    def initialize_minion(self, minion_ids):
        '''
        Install salt, set up config files, seed key
        '''
        # Get the specified minion
        deployment_config = self.get_deployment_config()
        minions = deployment_config.get('minions') or {}

        these_minions = []
        for minion_id in minion_ids:
            minion = minions.get(minion_id)
            if minion is None:
                raise KeyError("Minion with id {} not found in deployment config".format(minion_id))
            these_minions.append(minion)

        # Install boto.cfg
        self.configure_boto(minion_ids=minion_ids)

        # Provision system software
        self.install_base_packages(minion_ids=minion_ids)
        self.prepare_salt_installation(minion_ids=minion_ids)
        self.install_salt(minion_ids=minion_ids)
        ran_highstate = self.configure_salt(minion_ids=minion_ids)

        # Mark minions as initialized
        if ran_highstate:
            for minion in these_minions:
                minion['initialized'] = True
            self.save_deployment_config(deployment_config)

    def install_minion_keys(self, minion_ids):
        '''
        Install PKI for minion
        '''

        for minion_id in minion_ids:
            deployment_config = self.get_deployment_config()
            minions = deployment_config.get('minions') or {}
            minion = minions.get(minion_id)
            if minion is None:
                raise KeyError("Minion with id {} not found in deployment config".format(minion_id))

            # Generate keys
            def _generate_keys():
                with cd('/root'):
                    private_key = StringIO()
                    public_key = StringIO()
                    master_public_key = StringIO()
                    # Generate Keys on master
                    sudo('salt-key --gen-keys={minion_id}.{deployment_id}.deepfield.net'.format(minion_id=minion_id, deployment_id=self.deployment_id))
                    sudo('cp /etc/salt/pki/master/master.pub /root/master.pub')
                    sudo('rm -rf /tmp/keys && mkdir -p /tmp/keys && chmod a+rwX /tmp/keys')
                    sudo('mv /root/*.{pem,pub} /tmp/keys/ && chmod a+rw /tmp/keys/*')

                    # Fetch generated keys
                    get(remote_path='/tmp/keys/{minion_id}.{deployment_id}.deepfield.net.pem'.format(minion_id=minion_id, deployment_id=self.deployment_id),
                        local_path=private_key
                    )
                    get(remote_path='/tmp/keys/{minion_id}.{deployment_id}.deepfield.net.pub'.format(minion_id=minion_id, deployment_id=self.deployment_id),
                        local_path=public_key
                    )

                    get(remote_path='/tmp/keys/master.pub',
                        local_path=master_public_key
                    )

                    # Install them on master
                    sudo('cp /tmp/keys/{minion_id}.{deployment_id}.deepfield.net.pub /etc/salt/pki/master/minions/{minion_id}.{deployment_id}.deepfield.net'.format(minion_id=minion_id, deployment_id=self.deployment_id))
                    sudo('rm -rf /root/*.{pem,pub} /tmp/keys')
                    private_key.seek(0)
                    public_key.seek(0)
                    master_public_key.seek(0)

                    return private_key, public_key, master_public_key

            # Copy keys to minion
            def _install_minion_keys():
                sudo('mkdir -p /etc/salt/pki/minion')
                put(private_key, '/etc/salt/pki/minion/minion.pem', mode=600, use_sudo=True)
                put(public_key, '/etc/salt/pki/minion/minion.pub', mode=600, use_sudo=True)
                put(master_public_key, '/etc/salt/pki/minion/minion_master.pub', mode=600, use_sudo=True)

            generated_key_retval = self.execute(_generate_keys)
            private_key, public_key, master_public_key = generated_key_retval.values()[0]
            self.execute(_install_minion_keys, minion_ids=[minion_id])
            self.sudo('service salt-minion restart', minion_ids=[minion_id])

    def get_key_pair_name(self):
        '''Return the key pair name that corresponds to this deployment's key'''
        return get_key_pair_name(self.deployment_id)

    def ensure_deployment_keypair(self):
        ec2 = getattr(self, 'ec2', None)
        return ensure_deployment_key_pair(deployment_id=self.deployment_id, ec2=ec2)

    def set_snmp_community(self, snmp_community):
        '''
        Set the snmp community for this deployment


        Parameters:
            snmp_community (str): The snmp_community for this deployment
        '''
        slice_update = {
            'snmp': {
                'community': snmp_community
            }
        }
        tag_update = slice_update

        update_json(self.deployment_id,
            'config/slice.json',
            slice_update
        )
        update_json(self.deployment_id,
            'config/tags.json',
            tag_update
        )

    def set_license_expiration(self, expires):
        '''
        Sets the license configuration for the deployment

        Parameters:
           expires (arrow): The expiration date
        '''
        slice_config = self.get_slice_config()
        license = slice_config.get('license', {})
        license['expires'] = expires.strftime('%Y-%m-%d')
        slice_config['license'] = license
        self.save_slice_config(slice_config)

    def set_asns(self, asns):
        '''
        Sets the ASNs that we use to define on-net vs off-net

        Parameters:
            asns (list): A list of integers representing ASNs
        '''
        tags = load_json(self.deployment_id, 'config/tags.json')

        for tag in tags['tags']:
            desc = tag.get('description')
            if desc and desc == 'On-net':
                if "match" not in tag:
                    tag["match"] = {}
                if 'contains_as' in tag['match']:
                    contains_as = tag["match"].get('contains_as', [])
                    contains_as += asns
                else:
                    contains_as = asns
                tag["match"]['contains_as'] = contains_as

        save_json(self.deployment_id, 'config/tags.json', tags)

    def set_max_gbps(self, max_gbps):
        '''
        Sets the Max Gbps

        Parameters:
            max_gbps (int): An integer representing the max Gbps
        '''
        slice_config = self.get_slice_config()
        license = slice_config.get('license', {})
        license['max_gbps'] = max_gbps
        slice_config['license'] = license
        self.save_slice_config(slice_config)

    def set_contacts(self, contacts):
        '''
        Sets the contact information for the deployment

        Parameters:
            contacts (list): A list of strings containing email addresses
            of people associated with this deployment
        '''
        deployment_config = self.get_deployment_config()
        deployment_config['contacts'] = contacts
        self.save_deployment_config(deployment_config)

    def get_credentials(self):
        '''Return the boto credentials for the deployment'''
        deployment_config = self.get_deployment_config()
        return deployment_config.get('credentials')

    def wait_for_ssh(self, tries=100, host=None, port=None, ssh_user=None, minion_ids=None):
        '''
        Try {tries} times to connect to ssh. If not successful, return False,
        else return True
        '''
        log.info("Waiting for SSH connection")
        env.connection_attempts = tries

        try:
            result = self.run('true', host=host, port=port, ssh_user=ssh_user, minion_ids=minion_ids)
        except SystemExit as e:
            return False
        return True

    def run(self, command, host=None, port=None, ssh_user=None, minion_ids=None):
        '''Run command as support user on master'''
        def _run():
            return run(command)
        return self.execute(_run, host=host, port=port, ssh_user=ssh_user, minion_ids=minion_ids)

    def sudo(self, command, user=None, host=None, port=None, ssh_user=None, minion_ids=None):
        '''Run command as sudo on master'''

        def _run():
            return sudo(command, user=user)

        return self.execute(_run, host=host, port=port, ssh_user=ssh_user, minion_ids=minion_ids)

    def get_ssh_user(self, minion_id=None):

        deployment_config = self.get_deployment_config()
        if minion_id:
            minions = deployment_config.get('minions') or {}
            minion_definition = minions.get(minion_id) or {}
            definition = minion_definition
        else:
            definition = self.get_master_definition() or {}

        override_user = definition.get('ssh_user') or deployment_config.get('ssh_user')
        user = override_user or 'support'
        if not definition.get('initialized'):
            user = override_user or 'ubuntu'
        return user

    def get_shell_command(self, minion_ids=None, local_tunnels=None):
        '''
        Returns the command to run for opening a shell in the format [arg0, arg1, ...].
        '''
        if local_tunnels is None:
            local_tunnels = []

        if not minion_ids:
            master_definition = self.get_master_definition()
            if master_definition is None:
                raise ValueError("Failed to retrieve master definition for deployment {}".format(self.deployment_id))
            vm_definition = master_definition
        else:
            assert len(minion_ids) == 1
            minion_id = minion_ids[0]
            deployment_config = self.get_deployment_config()
            minions = deployment_config.get('minions') or {}
            minion_definition = minions.get(minion_id)
            if not minion_definition:
                raise ValueError("Failed to retrieve minion with id {} definition for deployment {}".format(minion_id, self.deployment_id))
            vm_definition = minion_definition



        if 'ssh_address' in vm_definition:
            host = vm_definition['ssh_address']
        else:
            host = vm_definition.get('ip') or vm_definition.get('public_ip_address')
        if not host:
            raise ValueError("Could not determine host")

        ssh_port = vm_definition.get('ssh_port') or 22

        key_filename = None
        if minion_ids:
            minion_id = minion_ids[0]
        else:
            minion_id = None
        user = self.get_ssh_user(minion_id=minion_id)
        if not vm_definition.get('initialized'):
            private_key, public_key = self.ensure_deployment_keypair()
            tempfile_directory = os.path.join(deepy.cfg.hood_dir, 'tmp')
            deepy.util.ensure_directory(tempfile_directory)
            with tempfile.NamedTemporaryFile(delete=False, dir=tempfile_directory, prefix='tmpkey') as fp:
                fp.write(private_key)
                log.debug("Using key pair at {}".format(fp.name))
                key_filename = fp.name

        cmd = ['ssh', '-A', '{}@{}'.format(user, host)]
        if ssh_port != 22:
            cmd += ['-p', str(ssh_port)]
        if key_filename:
            cmd += ['-i', key_filename]
        for local_tunnel in local_tunnels:
            cmd += ['-L', local_tunnel]
        return cmd

    def shell(self, minion_ids=None, local_tunnels=None):
        '''Open a shell to a vm'''
        cmd = self.get_shell_command(minion_ids, local_tunnels=local_tunnels)
        log.info(' '.join(cmd))
        subprocess.call(cmd)

    def execute(self, func, host=None, port=None, ssh_user=None, minion_ids=None):
        '''Execute fabric function remotely.

        If host or port are specified, override the values set in the setup_fabric method with the provided ones.
        '''
        self.setup_fabric(minion_ids=minion_ids, host=host, port=port, ssh_user=ssh_user)
        func = parallel(func)
        return execute(func)

    def get_deployment_config(self):
        '''Returns dictionary with deployment configuration'''
        return get_deployment_config(self.deployment_id)

    def save_deployment_config(self, deployment_config):
        return save_deployment_config(self.deployment_id, deployment_config)

    def get_slice_config(self):
        '''Returns dictionary with slice configuration'''
        return get_slice_config(self.deployment_id)

    def save_slice_config(self, slice_config):
        return save_slice_config(self.deployment_id, slice_config)

    def get_master_definition(self):
        '''Return definition for master node'''
        deployment_config = self.get_deployment_config()
        return deployment_config.get('master')

    def is_bootstrapped(self):
        '''
        Return true if deployment is bootstrapped, False otherwise
        '''
        return deployment_bootstrapped(self.deployment_id)


    def install_base_packages(self, minion_ids=None):
        '''Installs all the packages necessary for getting salt master up and
        going'''

        # XXX: Need to do this because otherwise it sometimes fails to connect
        # to the appropriate package repo (AWS problem)
        for i in xrange(2):
            self.sudo('apt-get update', minion_ids=minion_ids)
        self.sudo('apt-get install -y python-pip git build-essential autoconf swig', minion_ids=minion_ids)
        self.sudo('pip install awscli', minion_ids=minion_ids)
        self.sudo('pip install GitPython==0.3.2.RC1 gitdb', minion_ids=minion_ids)

    def prepare_salt_installation(self, minion_ids=None):
        '''Get everything set up to run the salt installation'''

        # Create initial directories
        dirs = [
            "/root/.ssh",
            "/etc/salt",
            "/usr/local/var/run",
            "/pipedream/salt",
            "/pipedream/pillar",
            "/pipedream/cache/config",
            "/pipedream/provision/wheels",
            "/pipedream/tmp/pids"
        ]
        dirs_str = ' '.join(dirs)
        self.sudo('mkdir -p {} || true'.format(dirs_str, minion_ids=minion_ids))
        self.sudo('chmod -R 777 /pipedream/tmp')

        # Install the pdrops.key
        src = os.path.join( deepy.util.get_code_root(), 'staging', 'shared', 'pdrops.key')
        if not os.path.exists(src):
            src = '/var/local/pipedream/staging/shared/pdrops.key'
        if not os.path.exists(src):
            raise ValueError("Failed to find pdrops.key")
        dest = '/pipedream/pdrops.key'
        self.execute(lambda: put(src, dest, use_sudo=True), minion_ids=minion_ids)
        self.execute(lambda: put(src, '/root/.ssh/id_rsa', use_sudo=True), minion_ids=minion_ids)


        # Setup hosts file
        if not minion_ids:
            # Point salt -> 127.0.0.1
            self.execute(lambda: append('/etc/hosts', '''
127.0.0.1  salt
''', use_sudo=True))
        else:
            master_definition = self.get_master_definition()
            master_ip = master_definition.get('private_ip_address') or master_definition.get('ip')
            if not master_ip:
                raise ValueError("Can't find master IP")

            # Point salt -> master ip
            self.execute(lambda: append('/etc/hosts', '''
    {master_ip}  salt
    '''.format(master_ip=master_ip), use_sudo=True), minion_ids=minion_ids)


    def install_salt(self, wait_time=5, minion_ids=None):
        '''Install salt
        '''
        # Install salt
        if minion_ids:
            self.sudo('{aws} s3 cp s3://download.deepfield.net/provision/bootstrap-salt.sh . && sh bootstrap-salt.sh -X'.format(aws=self.get_aws_command()), minion_ids=minion_ids)
        else:
            self.sudo('{aws} s3 cp s3://download.deepfield.net/provision/bootstrap-salt.sh . && sh bootstrap-salt.sh -M -X'.format(aws=self.get_aws_command()))

    def get_salt_master_config_file(self, with_git):
        '''
        Return string representing salt master config file
        '''
        master_config = '''
file_roots:
    base:
        - /home/support/pipedream/salt-states

pillar_roots:
    base:
        - /pipedream/pillar
        - /home/support/pipedream/salt-states/pillar-data

ext_pillar:
  - s3_credentials: True
  - deepfield_config: True

log_level: debug

'''
        master_config += '''
fileserver_backend:
  - roots

extension_modules: /home/support/pipedream/salt-states
'''
        return master_config

    def get_salt_minion_config(self, name):
        '''
        Return string representing salt minion config
        '''
        minion_config = '''
id: {name}.{deployment_id}.deepfield.net
auth_timeout: 50
auth_tries: 5
'''.format(name=name, deployment_id=self.deployment_id)

        return minion_config

    def install_salt_config_files(self, with_git=True, minion_ids=None):
        '''
        Sets up /etc/salt/minion and /etc/salt/master (the latter only when appropriate)
        '''

        # Master
        if not minion_ids:
            # Make master config
            master_config = self.get_salt_master_config_file(with_git=with_git)

            # Install it
            self.execute(lambda: put(StringIO(master_config), '/etc/salt/master', use_sudo=True))

            # Make minion config
            minion_config = self.get_salt_minion_config(name='master')

            # Install it
            self.execute(lambda: put(StringIO(minion_config), '/etc/salt/minion', use_sudo=True))


        # Minion Config
        else:

            # Install minion configuration
            for minion_id in minion_ids:
                # Make minion config
                minion_config = self.get_salt_minion_config(name=minion_id)

                # Install it
                self.execute(lambda: put(StringIO(minion_config), '/etc/salt/minion', use_sudo=True), minion_ids=[minion_id])

    def add_support_to_sudoers(self, minion_ids=None):
        '''
        Adds the support user to the sudoers file
        '''
        self.execute(lambda: append('/etc/sudoers', 'support  ALL = NOPASSWD:ALL', use_sudo=True), minion_ids=minion_ids)

    def install_salt_keys(self, minion_ids=None):
        '''
        Installs all of the appropriate keys for salt masters/minions
        '''

        # Generate master-minion keys
        if not minion_ids:
            minion_name = "master.{}.deepfield.net".format(self.deployment_id)
            self.sudo('salt-key --gen-keys={}'.format(minion_name))
            self.sudo('mkdir -p /etc/salt/pki/master/minions /etc/salt/pki/minion/ || true')
            self.sudo('cp {minion}.pub /etc/salt/pki/master/minions/{minion}'.format(minion=minion_name))
            self.sudo('cp {minion}.pub /etc/salt/pki/minion/minion.pub'.format(minion=minion_name))
            self.sudo('cp {minion}.pem /etc/salt/pki/minion/minion.pem'.format(minion=minion_name))

            self.sudo('service salt-master restart; service salt-minion restart')
            self.sudo('salt-run fileserver.update')
            self.sudo('salt-key -A -y')
        else:
            self.install_minion_keys(minion_ids=minion_ids)

    def restart_salt(self, minion_ids=None):
        '''
        Restart the salt server processess
        '''
        if not minion_ids:
            self.sudo('service salt-master restart')
        self.sudo('service salt-minion restart')

    def configure_salt(self, wait_time=5, with_git=True, minion_ids=None):
        '''Configure salt

        Wait time is how many seconds to wait for salt master/minion to come up.
Defaults to 5 seconds

        Returns True if high state successfully completed, False otherwise

        '''

        # Create initial directories
        self.sudo('mkdir -p /root/.ssh /etc/salt /pipedream/salt /pipedream/pillar /pipedream/cache/config /pipedream/provision/wheels || true', minion_ids=minion_ids)

        # Add support to sudoers
        self.add_support_to_sudoers(minion_ids=minion_ids)

        # Add github to root's known hosts
        self.execute(lambda: append('/root/.ssh/known_hosts', '''
github.com,192.30.252.131 ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAq2A7hRGmdnm9tUDbO9IDSwBK6TbQa+PXYPCPy6rbTrTtw7PHkccKrpp0yVhp5HdEIcKr6pLlVDBfOLX9QUsyCOV0wzfjIJNlGEYsdlLJizHhbn2mUjvSAHQqZETYP81eFzLQNnPHt4EVVUh7VfDESU84KezmD5QlWpXLmvU31/yMf+Se8xhHTvKSCZIFImWwoG6mbUoWf9nzpIoaSjB+weqqUUmpaaasXVal72J+UX2B+2RPW3RcT0eOzQgqlJL3RKrTJvdsjE3JEAvGq3lGHSZXy28G3skua2SmVi/w4yCE6gbODqnTWlg7+wC604ydGXA8VJiS5ap43JXiUFFAaQ==
''', use_sudo=True), minion_ids=minion_ids)

        # Create salt config files
        self.install_salt_config_files(with_git=with_git, minion_ids=minion_ids)

        # Copy deployment.json and slice.json
        self.execute(lambda: put(StringIO(json.dumps(self.get_deployment_config(), indent=2)),
            '/pipedream/cache/config/deployment.json',
            use_sudo=True), minion_ids=minion_ids)
        self.execute(lambda: put(StringIO(json.dumps(self.get_slice_config(), indent=2)),
            '/pipedream/cache/config/slice.json',
            use_sudo=True), minion_ids=minion_ids)

        if with_git and not minion_ids:
            self.sudo('mkdir -p /home/support')
            self.sudo('git clone https://pdropsdaemon:1Oct37Twecim@github.com/deepfield/pipedream.git /home/support/pipedream || true')

        # Generate/install salt keys
        self.install_salt_keys(minion_ids=minion_ids)

        # Reboot servers and bring up high state
        self.restart_salt(minion_ids=minion_ids)
        return self.run_salt_highstate(minion_ids=minion_ids)

    def run_salt_highstate(self, minion_ids=None, try_hard=False, wait_time=15):
        '''
        Bring up salt high state
        '''
        ran_highstate = False
        for i in xrange(2):

            for j in xrange(10):
                try:
                    time.sleep(wait_time)
                    self.sudo("salt-call test.ping", minion_ids=minion_ids)
                    self.sudo("salt-call state.highstate -l debug", minion_ids=minion_ids)
                    ran_highstate = True
                    log.info("Machine is in high state")
                    break
                except SystemExit as e:
                    log.error("Failed to bring up salt high state, trying again")

            # Try again
            if not ran_highstate and try_hard:
                log.error("Failed to bring up salt high state, cleaning house and retrying")
                self.sudo('rm -rf /var/cache/salt')
                if not minion_ids:
                    self.sudo('service salt-master restart')
                    time.sleep(wait_time)
                self.sudo('service salt-minion restart', minion_ids=minion_ids)
                time.sleep(wait_time)
            else:
                # Restart servers for good measure, to prevent this bug from showing up: https://github.com/saltstack/salt/issues/13535
                self.restart_salt(minion_ids=minion_ids)
                break

        if not ran_highstate:
            log.error("Failed to bring up high state. Run pdcloud.py salt highstate manually to bring it up yourself. It is likely that you are running into one of the Salt bugs, e.g. https://github.com/saltstack/salt/issues/13535 . This usually only happens when bringing up masters; if it happens for minions, let Matt H know so he can investigate further and file a bug.")
        return ran_highstate

    def mark_master_as_initialized(self):
        '''Marks master as initialized
        '''

        deployment_config = self.get_deployment_config()
        master_definition = self.get_master_definition()
        master_definition['initialized'] = True
        deployment_config['master'] = master_definition
        self.save_deployment_config(deployment_config)

    def mark_deployment_as_configured(self):
        '''Marks deployment as configured'''
        deployment_config = self.get_deployment_config()
        deployment_config['configured'] = True
        self.save_deployment_config(deployment_config)

    def set_connection_info(self, minion_id=None,
            private_dns_address=None, private_ip_address=None,
            public_dns_address=None, public_ip_address=None, r53_address=None,
            ssh_address=None, ui_address=None, ssh_port=22, ssh_user=None):
        '''
        Private_dns_address, public_dns_address, private_ip_address,
        and public_ip_address are used to distinguish between a deployments
        internal network and public facing network. This distiction will mostly
        be made on AWS deployments, but may also be used when we're behind a
        firewall within someones network. (e.g. ip-10-73-22-129.ec2.internal,
        ec2-107-22-149-255.compute-1.amazonaws.com, and 10.73.22.129,
        54.235.177.202, respectively)

        r53_address is a AWS deployment only concept, this would be the actual
        webpage address of the deployment (e.g. deepfield-proxy.deepfield.net).

        ssh_address will be used within internal networks for salt, fabric, etc.

        ssh_port will be used as the port to connect to for ssh

        ui_address will be used if we've set up a deployment to use a non-
        standard home.py UI address.

        '''

        deployment_config = self.get_deployment_config()
        result = {}

        if private_dns_address:
            result['private_dns_address'] = private_dns_address
        if private_ip_address:
            result['private_ip_address'] = private_ip_address
        if public_dns_address:
            result['public_dns_address'] = public_dns_address
        if public_ip_address:
            result['public_ip_address'] = public_ip_address
        if r53_address:
            result['r53_address'] = r53_address
        if ssh_address:
            result['ssh_address'] = ssh_address
        if ui_address:
            result['ui_address'] = ui_address
        if ssh_user:
            result['ssh_user'] = ssh_user

        result['ssh_port'] = ssh_port

        if minion_id:
            deployment_config['minions'].setdefault(minion_id, {}).update(result)
        else:
            deployment_config['master'] = deployment_config.get('master') or {}
            deployment_config['master'].update(result)

        self.save_deployment_config(deployment_config)

    def unset_connection_info(self, minion_id=None,
            private_dns_address=None, private_ip_address=None,
            public_dns_address=None, public_ip_address=None, r53_address=None,
            ssh_address=None, ui_address=None):

        deployment_config = self.get_deployment_config()
        result = (deployment_config['minions'][minion_id] if minion_id else
            deployment_config['master'])

        if private_dns_address:
            result.pop('private_dns_address', private_dns_address)
        if private_ip_address:
            result.pop('private_ip_address', private_ip_address)
        if public_dns_address:
            result.pop('public_dns_address', public_dns_address)
        if public_ip_address:
            result.pop('public_ip_address', public_ip_address)
        if r53_address:
            result.pop('r53_address', r53_address)
        if ssh_address:
            result.pop('ssh_address', ssh_address)
        if ui_address:
            result.pop('ui_address', ui_address)

        if minion_id:
            deployment_config['minions'][minion_id] = result
        else:
            deployment_config['master'] = result

        self.save_deployment_config(deployment_config)

    def set_proxy_info(self, ip, port):
        log.info("Configuring proxy information: {}:{}".format(ip, port))
        proxy = {
            'ip': ip,
            'port': port
        }

        slice_config = self.get_slice_config()
        slice_config['proxy'] = proxy
        self.save_slice_config(slice_config)

    @staticmethod
    def _get_vm_from_slice(slice_config, vm_uuid):
        if not vm_uuid:
            raise ValueError('vm_uuid required')

        vms = slice_config.get('vms') or []
        selected_vm = None
        for vm in vms:
            if vm.get('uuid') == vm_uuid:
                return vm

        return None

    @staticmethod
    def _get_cron_jobs_from_slice(slice_config, vm_uuid):
        '''
        Returns reference to appropriate cron_jobs object in the slice config.

        Warning! Will create an empty cron jobs list if it doesn't exist! An
        empty cron jobs list has a different meaning than an unset or None cron
        jobs list!
        '''
        selected_vm = Deployment._get_vm_from_slice(slice_config, vm_uuid)
        if not selected_vm:
            raise ValueError("Failed to find VM with uuid {}".format(vm_uuid))

        cron_jobs = selected_vm.get('cron_jobs') or []
        selected_vm['cron_jobs'] = cron_jobs
        return cron_jobs

    def unset_cron_jobs(self, vm_uuid):
        '''
        Unset the cron_jobs in slice, running default cron jobs for the machine.
        Leader VMs should do this.

        Note: this is awkward and should be refactored.
        '''
        slice_config = self.get_slice_config()
        selected_vm = Deployment._get_vm_from_slice(slice_config, vm_uuid)
        if not selected_vm:
            raise ValueError("Failed to find VM with uuid {}".format(vm_uuid))

        if 'cron_jobs' in selected_vm:
            del selected_vm['cron_jobs']
        self.save_slice_config(slice_config)

    def add_cron_job(self, cron_job, vm_uuid):
        '''
        Add the specified cron_job to the specified VM.
        '''
        slice_config = self.get_slice_config()
        cron_jobs = Deployment._get_cron_jobs_from_slice(slice_config, vm_uuid)
        if not cron_job in cron_jobs:
            cron_jobs.append(cron_job)
        else:
            deepy.log.warn("cron_job is already in slice")
        self.save_slice_config(slice_config)

    def remove_cron_job(self, cron_job, vm_uuid=None):
        '''
        Remove the specified cron_job. Does not throw an error if nothing happens
        '''
        slice_config = self.get_slice_config()
        cron_jobs = Deployment._get_cron_jobs_from_slice(slice_config, vm_uuid=vm_uuid)
        if cron_job in cron_jobs:
            cron_jobs.remove(cron_job)
            self.save_slice_config(slice_config)

    def get_cron_jobs(self, vm_uuid=None):
        '''
        Get the cron jobs from slice. Note: does not distinguish between [] and None,
        it will always return []. Use 'unset_cron_jobs' to unset.
        '''
        slice_config = self.get_slice_config()
        cron_jobs = Deployment._get_cron_jobs_from_slice(slice_config, vm_uuid=vm_uuid)
        return cron_jobs

    def _get_roles_from_slice(self, slice_config, vm_uuid=None):
        '''
        Returns reference to appropriate roles object in the slice config
        '''
        if not vm_uuid:
            roles = slice_config.get('roles') or []
            slice_config['roles'] = roles
        else:
            vms = slice_config.get('vms') or []
            selected_vm = Deployment._get_vm_from_slice(slice_config, vm_uuid)
            if not selected_vm:
                raise ValueError("Failed to find VM with uuid {}".format(vm_uuid))
            roles = selected_vm.get('roles') or []
            selected_vm['roles'] = roles
        return roles


    def add_role(self, role, vm_uuid=None):
        '''
        Add the specified role. If a vm_uuid is given, add it to that VM only,
        otherwise add it to the root roles.
        '''
        slice_config = self.get_slice_config()
        roles = self._get_roles_from_slice(slice_config, vm_uuid=vm_uuid)

        if not role in roles:
            roles.append(role)
        else:
            deepy.log.warn("Role is already in slice")
        self.save_slice_config(slice_config)

    def remove_role(self, role, vm_uuid=None):
        '''
        Remove the specified role. Does not throw an error if nothing happens
        '''
        slice_config = self.get_slice_config()
        roles = self._get_roles_from_slice(slice_config, vm_uuid=vm_uuid)
        if role in roles:
            roles.remove(role)
        self.save_slice_config(slice_config)

    def get_roles(self, vm_uuid=None):
        slice_config = self.get_slice_config()
        roles = self._get_roles_from_slice(slice_config, vm_uuid=vm_uuid)
        return roles

    def start(self):
        raise NotImplementedError("This deployment implementation does not implement start")


class PartialAWSDeploymentMixIn:
    '''
    Mix in AWS functionality that is available to both AWS deployments
    and Private deployments (but not isolated deployments)
    '''
    def refresh_group_policies(self):
        '''Drop existing S3Policy and SQSPolicy and reapply'''

        user, group = get_deployment_user_and_group(self.deployment_id, self.get_deployment_config())
        try:
            self.iam.delete_group_policy(group, 'S3Policy')
            self.iam.delete_group_policy(group, 'SQSPolicy')
        except boto.exception.BotoServerError:
            log.warn('group-policy-delete-failed-continuing')

        self.ensure_iam_group()

    def get_deployment_bucket(self):
        '''Return bucket for deployment'''
        return self.s3.get_bucket(
            get_bucket_name_for_deployment(self.deployment_id)
        )

    def ensure_credentials(self):
        '''Ensure valid credentials are in deployment descriptor'''
        return ensure_credentials(self.deployment_id, deployment_config=self.get_deployment_config(), s3=self.s3, iam=self.iam)

    def ensure_iam_user(self):
        '''
        Ensures IAM user for this deployment exists
        '''
        return ensure_iam_user(self.deployment_id, iam=self.iam, deployment_config=self.get_deployment_config())

    def ensure_iam_group(self):
        '''
        Ensures IAM group for this deployment exists
        '''
        return ensure_iam_group(self.deployment_id, deployment_config=self.get_deployment_config(), iam=self.iam)

    def configure_boto(self, minion_ids=None):
        '''
        Creates and uploads boto.cfg. Only requires SSH to be present
        '''

        # Need to figure out if we're a proxy deployment. If so, need to configure boto to use proxy
        deepy.cfg.init(self.deployment_id)
        proxy_connection_info = deepy.cfg.proxy_info

        credentials = self.ensure_credentials()

        # Save out boto config
        # file
        boto_cfg = '''
[Credentials]
aws_access_key_id = {id}
aws_secret_access_key = {secret}

[Boto]
http_socket_timeout = 30
'''.format(id=credentials[0],
secret=credentials[1])
        if deepy.cfg.proxy_deployment:
            boto_cfg += '''
proxy_port = {port}
proxy = {ip}
'''.format(ip=proxy_connection_info['ip'], port=proxy_connection_info['port'])
        self.execute(lambda: put(StringIO(boto_cfg), '/etc/boto.cfg', use_sudo=True), minion_ids=minion_ids)

    def bootstrap_deployment(self, location=Location.DEFAULT, region=None, branch=None):
        '''Bootstrap a deployment.

        Bootstraps a deployment by creating the S3 bucket for it, as well as an
        initial config/deployment.json and config/slice.json.

        Parameters:
            kind (str): The kind of deployment this is, aws|private|isolated.
                aws -> Amazon Web Services
                private -> Machines provisioned in private cloud or bare metal by
                    clients
                isolated -> Machines provisioned by clients that we don't have
                    access to
            location (boto.s3.connection.Location): The location to use for bucket
                creation.

        '''
        # Check to see if it's already bootstrapped
        if deployment_bootstrapped(self.deployment_id):
            log.warn("Deployment with id {} is already bootstrapped".format(self.deployment_id))

        # Ensure bucket exists
        bucket_name = get_bucket_name_for_deployment(self.deployment_id)
        ensure_bucket(bucket_name=bucket_name, s3=self.s3, location=location)
        log.debug('ensured-bucket-exists {} in {}'.format(bucket_name, location))

        # Ensure key pair exists
        key_pair_content, key_pair_public_key = self.ensure_deployment_keypair()
        log.debug("ensured-key-pair-exists: {}".format(key_pair_content))

        # Create config files
        log.debug('creating-initial-config-files')
        create_initial_config_files(self.deployment_id, kind=self.kind, region=region, branch=branch)
        log.debug('initial-config-files-created')

        # That's it, deployment is bootstrapped

    def configure_master_dns(self, set_hostname=False):
        '''Sets up DNS names for the master VM

        This function both updates route53 and sets the hostname on the VM
        '''
        master_definition = self.get_master_definition()
        log.debug("Configuring DNS records. {} -> {}".format(master_definition['public_dns_address'], self.deployment_id+'.deepfield.net'))
        hostname = 'master.'+self.deployment_id

        self.update_route53_entry(master_definition['public_dns_address'], name=hostname)
        if master_definition['public_dns_address'] != self.deployment_id + '.deepfield.net':
            self.update_route53_entry(master_definition['public_dns_address'], name=self.deployment_id)
        if set_hostname:
            self.sudo('hostname {}'.format(hostname+'.deepfield.net'))
            self.execute(
                lambda: put(StringIO(hostname+'.deepfield.net'),
               '/etc/hostname', use_sudo=True)
            )

    def initialize_master(self):
        '''
        Initialize master VM

        Side Effects:
            - Creates route53 entries
            - Installs system packages on remote machine
            - Installs and configures salt (e.g. overwrites /etc/salt/master and
              /etc/salt/minion)
            - Sets "initialized" flag in master section in deployment.json
        '''

        # Set up DNS
        log.info("Setting up master DNS records")
        self.configure_master_dns()

        # Configure boto
        log.info("Configuring boto")
        self.configure_boto()

        # Install packages
        log.info("Installing base packages")
        self.install_base_packages()

        # Configure salt
        log.info("Configuring salt")
        self.prepare_salt_installation()
        self.install_salt()
        ran_highstate = self.configure_salt()

        # Mark master as initialized
        if ran_highstate:
            self.mark_master_as_initialized()

    def destroy_ec2_volumes(self, tries=12, sleep_time=5):
        '''
        Delete ec2 volumes associated with this deployment
        '''
        filters = {'tag:deployment': self.deployment_id}
        volumes = self.ec2.get_all_volumes(filters=filters)
        for _ in xrange(tries):
            states = [(v.id, v.update()) for v in volumes]
            log.info("Waiting on volumes to become available: {}".format(
                states))

            if any(s[1] != "available" for s in states):
                time.sleep(sleep_time)
            else:
                break
        else:
            log.warn("volumes-not-in-good-deletion-state")
            return

        successful = [v.delete() for v in volumes]
        log.info("Volumes successfully deleted: {}".format(successful))

    def destroy_sqs_queue(self):
        '''
        Destroy the deployment's SQS queue
        '''
        queue_name = 'jobs_'+self.deployment_id
        conn = boto.connect_sqs()
        queue = conn.get_queue(queue_name)
        if queue:
            log.debug('Found queue: {}'.format(queue))
            queue = conn.get_queue(queue_name)
            if not queue:
                log.warn('Could not find queue {}'.format(queue_name))
            else:
                conn.delete_queue(queue)
                log.info("Deleted queue {}".format(queue_name))
        else:
            log.warn('Could not find queue {}'.format(queue_name))


    def destroy_deployment_security_group(self):
        try:
            groups = self.ec2.get_all_security_groups(self.deployment_id)
            if len(groups) == 0: return
            group = groups[0]
            for rule in group.rules:
                for grant in rule.grants:
                    group.revoke(ip_protocol=rule.ip_protocol,
                        from_port=rule.from_port,
                        to_port=rule.to_port,
                        src_group=grant
                    )

            self.ec2.delete_security_group(group.name)
        except Exception as e:
            log.warn("Failed to delete security group {}". format(self.deployment_id))

    def start(self):
        '''
        Start the deployment
            - Run "pipedream" salt state
            - Run deployment_sync.py -v
            - Run restart_daemons.py -v
        '''

        # Make required subdirectories
        self.sudo('mkdir -p /pipedream/h5flow /pipedream/cache/cubes /pipedream/cache/config /pipedream/cache/dimensions /pipedream/flows')

        # Add pipedream to roles
        self.add_role('pipedream')

        # Update slice/deployment config on VM
        self.execute(lambda: put(StringIO(json.dumps(self.get_slice_config(), indent=2)), '/pipedream/cache/config/slice.json', use_sudo=True))
        self.execute(lambda: put(StringIO(json.dumps(self.get_deployment_config(), indent=2)), '/pipedream/cache/config/deployment.json', use_sudo=True))
        self.sudo('chown -R support /pipedream')

        # Install pipedream
        self.sudo('salt-call state.sls pipedream')
        self.sudo('deployment_update_uuid.py')

        # Load initial data
        log.info("Loading map files")
        load_map_files(self.deployment_id)
        log.info("Loading dimensions")
        load_dimensions(self.deployment_id)
        log.info("Updating notices")
        update_notices([self.deployment_id], copy_eula=True, copy_notice=False, s3=self.s3)

        # Sync and start daemons
        self.sudo('deployment_sync.py -v')
        self.sudo('restart_daemons.py -v')


class AWSDeployment(PartialAWSDeploymentMixIn, Deployment):

    def __init__(self, deployment_id, s3, iam, ec2, r53):
        super(AWSDeployment, self).__init__(deployment_id)

        self.ec2 = ec2
        self.s3 = s3
        self.iam = iam
        self.r53 = r53
        self.kind = 'aws'

    def make_mount(self, instance=None, gb_size=DEFAULT_VOLUME_SIZE_GB, iops=None, volume_type="gp2"):
        '''
        Make an EBS volume of the specified size on the provided instance.

        If instance isn't specified, assume master instance
        '''
        if not instance:
            instance = self.get_master_instance()

        # Make EBS volume
        if iops:
            # If we have provisioned iops, use them
            volume = self.ec2.create_volume(gb_size, instance.placement, volume_type='io1', iops=iops)
        else:
            # Otherwise, create a standard one
            volume = self.ec2.create_volume(gb_size, instance.placement, volume_type=volume_type)

        log.info("Created volume with ID {}".format(volume.id))

        # Wait for the volume to come up
        start = time.time()
        while ((time.time() - start) < 300) and volume.update() != 'available':
            log.info("Waiting for volume to come up")
            time.sleep(2)

        # Attach the volume
        dev = None
        for label in range(ord('g'), ord('w')):
            dev = '/dev/sd'+chr(label)
            retval = self.ec2.attach_volume(volume.id, instance.id, dev)
            if retval:
                log.info('Attached volume {vid} to {dev} for instance {instance_id}'.format(vid=volume.id, dev=dev, instance_id=instance.id))
                break
        else:
            raise ValueError("Failed to attach volume. You may want to manually delete volume with id {}".format(volume.id))

        tags = {'Name': '{}-{}'.format(instance.id, dev), 'deployment': self.deployment_id}
        if 'type' in instance.tags:
            tags['type'] = instance.tags['type']
        if 'role' in instance.tags:
            tags['role'] = instance.tags['role']
        if 'minion_id' in instance.tags:
            tags['minion_id'] = instance.tags['minion_id']

        self.ec2.create_tags([volume.id], tags)

        return dev

    @parallel
    def mount_volume(self, dev, mount_point, minion_ids=None):
        '''
        Mount device
        '''
        dev_names = [dev, dev.replace('/dev/sd', '/dev/xvd')]

        # Format
        correct_dev_name = None
        retries = 0
        while True:
            retries += 1
            for dev_name in dev_names:
                try:
                    self.sudo('mkfs.ext4 {}'.format(dev_name), minion_ids=minion_ids)
                    correct_dev_name = dev_name
                except SystemExit as e:
                    log.info("Formatting {} failed, trying next".format(dev_name))

            if correct_dev_name:
                break
            elif retries > 10:
                raise ValueError("Dev names {} were not found after 10 retries".format(dev_names))
            else:
                log.info("Waiting for volume to attach")
                time.sleep(3)

        # Mount
        log.info('Mounting {dev} to {mount_point}'.format(dev=correct_dev_name, mount_point=mount_point))
        uuid = self.sudo('blkid -o value {}'.format(correct_dev_name), minion_ids=minion_ids).values()[0].splitlines()[0]
        mline = 'UUID=%s       %s      ext4    errors=remount-ro       0       1' % (uuid, mount_point)
        self.execute(lambda: append("/etc/fstab", mline, use_sudo=True), minion_ids=minion_ids)
        self.sudo('mkdir -p "{}"'.format(mount_point), minion_ids=minion_ids)
        self.execute(lambda: sudo('mount {}'.format(mount_point)), minion_ids=minion_ids)

    # Refactor in light of CI-1500
    @parallel
    def mount_swap(self, common_ununsed_mountpoints=None, minion_ids=None):
        '''
        Mount device as swap

        http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html#InstanceStoreSwapVolumes
        '''

        if common_ununsed_mountpoints is None:
            common_ununsed_mountpoints = ["/mnt"]

        columns = ["NAME", "MOUNTPOINT", "SIZE"]

        lsblk_result = self.sudo("lsblk -o {} -n -P".format(','.join(columns)),
            minion_ids=minion_ids)

        if len(lsblk_result) != 1:
            log.error("No swap mount point could be found")
            return

        blocks = lsblk_result.values()[0]

        def _extract_values(column, block_string):
            '''
            Turn: 'NAME="HI" NAME="FOO" NAME="BLAH"'
            Into: ['HI', 'FOO', 'BLAH']
            '''
            return re.findall(r'{}="(?P<{}>[^"]*)"'.format(column, column),
                block_string)

        blocks_values = [_extract_values(c, blocks) for c in columns]
        blocks_tuples = zip(*blocks_values)

        swap_devices = [name for name, mnt, size in blocks_tuples if mnt == "[SWAP]"]
        if swap_devices:
            log.info("Swap(s) already exist: {}".format(swap_devices))
            return

        unmounted_devices = [name for name, mnt, size in blocks_tuples if not mnt]

        # If we have no usable swap devices, check for possible unused
        # mountpoints and umount them
        if len(unmounted_devices) == 0:
            unused_mountpoints = [mnt for name, mnt, size in blocks_tuples if mnt
                in common_ununsed_mountpoints]
            unmounted_devices.extend([name for name, mnt, size in blocks_tuples if mnt
                in common_ununsed_mountpoints])
            for unused_mountpoint in unused_mountpoints:
                self.sudo("umount {}".format(unused_mountpoint), minion_ids=minion_ids)

        if len(unmounted_devices) == 0:
            log.info("No usable swap devices. This is not critical but you may want to enable a swap file.")
            return

        # Try all of them
        for device in unmounted_devices:
            size = [size for name, mnt, size in blocks_tuples if name == device][0]

            device_path = os.path.join('/', 'dev', device)
            log.info('Mounting {} ({}) as swap'.format(device_path, size))
            try:
                mkswap_result = self.sudo('mkswap {}'.format(device_path), minion_ids=minion_ids)

                mkswap_string = mkswap_result.values()[0]
                uuid = re.search(r'UUID=(?P<UUID>[a-z0-9\-]+)', mkswap_string).group("UUID")
                self.sudo('swapon -U {}'.format(uuid), minion_ids=minion_ids)

                mline = 'UUID={}       none  swap    sw      0       0'.format(uuid)
                self.execute(lambda: append("/etc/fstab", mline, use_sudo=True),
                    minion_ids=minion_ids)
                break
            except SystemExit:
                continue

    def _get_instances(self, filters):
        '''
        Return boto.Instance object that meet 'filters' criteria
        '''
        reservations = self.ec2.get_all_instances(filters=filters)
        return [i for i in self._get_all_instances(filters) if i.update() not in {'terminated', 'shutting-down'}]

    def _get_all_instances(self, filters):
        '''
        Return boto.Instance object that meet 'filters' criteria
        '''
        reservations = self.ec2.get_all_instances(filters=filters)
        return [i for r in reservations for i in r.instances]

    def _raise_bad_instance_count(self, instances, filters, lower_bound=True,
            upper_bound=True):
        '''
        Seatbelt for too many/little instances returned for a given
        deployment/minion ID
        '''
        if len(instances) > 1 and upper_bound:
            raise ValueError("Found more than one instance with filter {}".format(filters))
        elif len(instances) < 1 and lower_bound:
            raise ValueError("Found no instances with filter {}".format(filters))

    def get_minion_instance(self, minion_id):
        '''
        Return the boto.Instance object associated with the specified minion id
        '''
        filters = {
            'tag:deployment': self.deployment_id,
            'tag:role': 'minion',
            'tag:type': 'deployment',
            'tag:minion_id': minion_id
        }
        instances = self._get_instances(filters=filters)
        self._raise_bad_instance_count(instances, filters)

        return instances[0]

    def get_master_instance(self):
        '''
        Return the master boto.Instance object
        '''
        filters = {
            'tag:deployment': self.deployment_id,
            'tag:role': 'master',
            'tag:type': 'deployment'
        }
        instances = self._get_instances(filters=filters)
        self._raise_bad_instance_count(instances, filters, lower_bound=False)

        return instances[0] if len(instances) >= 1 else None

    def get_all_instances(self):
        '''
        Return list of all instances in this deployment
        '''
        filters = {
            'tag:deployment': self.deployment_id
        }
        instances = self._get_all_instances(filters=filters)

        return instances

    def ensure_security_groups(self, master=True, extra_groups=[]):
        '''
        Ensures that all security groups this deployment will need exist
        '''

        groups = [
            ('deployment', 'Necessary open ports for deployments'),
            (self.deployment_id, 'Security group specific to{}'.format(self.deployment_id))
            ]

        for group_name in extra_groups:
            groups.append((group_name, "Auto-generated group"))

        if master:
            groups.append(('deployment-master', 'Deployment master VMs'))
            groups.append((self.deployment_id+'-master', 'Deployment-specific minion VMs'))
        else:
            groups.append(('deployment-minion', 'Deployment minion VMs'))
            groups.append((self.deployment_id+'-minion', 'Deployment-specific minion VMs'))

        for name, description in groups:
            try:
                security_group = self.ec2.create_security_group(name, description)
            except boto.exception.EC2ResponseError as e:
                pass

        # Make sure we allow all traffic amongst the same deployment
        deployment_security_group = self.ec2.get_all_security_groups([self.deployment_id])[0]
        for proto in ('udp', 'tcp'):
            try:
                deployment_security_group.authorize(proto, 0, 65535, src_group=deployment_security_group)
            except boto.exception.EC2ResponseError as e:
                pass


        return groups

    def _get_salt_ami(self, region=DEFAULT_REGION, instance_type=None):
        '''
        Get first salt AMI in a given region
        '''

        ami_type = get_ami_type(instance_type)

        #Our hvm AMIs are not working, just use stock hvm
        if ami_type == "hvm":
            ami = 'ami-0745d26e'
            log.info("Using-stock-hvm-ami-{}".format(ami))
            return ami

        filters = {'tag:image_type': 'salt'}
        salt_images = self.ec2.get_all_images(filters=filters)

        if len(salt_images) == 0:
            raise KeyError("Could not find AMI in region {} with filters {}".format(region, filters))

        image = salt_images[0]
        if len(salt_images) > 0:
            log.info("Fetched salt AMI for region {} with id {} and tags {}".format(region, image.id, image.tags))
        if len(salt_images) > 1:
            log.warn("There is more than one salt base image to choose from, using the first")

        return image.id

    def _get_block_device_mapping(self, instance_type, root_size=100):
        '''
        http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/block-device-mapping-concepts.html
        http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html#InstanceStoreDeviceNames
        '''

        mapping = BlockDeviceMapping()

        chr_range = lambda s, e: [chr(i) for i in xrange(ord(s), ord(e) + 1)]
        dev_range = lambda s, e: ['/dev/sd{}'.format(c) for c in chr_range(s, e)]

        # /dev/sda1 is reserved for root
        mapping['/dev/sda1'] = EBSBlockDeviceType(size=root_size)

        # /dev/sd[b-e] is recommended for Instance Store Volumes
        store_count = INSTANCE_STORE_COUNTS.get(instance_type, 0)
        devices = dev_range('b', 'e')[:store_count]
        for i, dev in enumerate(devices):
            mapping[dev] = BlockDeviceType(ephemeral_name='ephemeral{}'.format(i))

        return mapping

    def get_placement(self):
        '''
        Returns the availability zone to use for this deployment
        '''
        deployment_config = self.get_deployment_config()
        placement = deployment_config.get('placement')
        return placement

    def launch_master_instance(self, instance_type=DEFAULT_INSTANCE_TYPE,
        security_groups=None, extra_tags=None, wait_time=240,
        region=None, root_size=100, ami=None, placement=None):
        '''
        Launches an instance for the master node and waits until it's running

        Returns:
            running instance (boto.Instance): An instance object representing
            the new node
        '''
        # Figure out placement
        placement = placement or self.get_placement()

        if security_groups is None:
            security_groups = []
        if extra_tags is None:
            extra_tags = {}
        if region is None:
            deployment_config = self.get_deployment_config()
            region = deployment_config.get('region', DEFAULT_REGION)
            self.ec2 = boto.ec2.connect_to_region(region)

        if not ami:
            salt_ami = self._get_salt_ami(region=region, instance_type=instance_type)
            ami = salt_ami
        block_mapping = self._get_block_device_mapping(instance_type, root_size)

        reservation = self.ec2.run_instances(
            ami,
            key_name=self.get_key_pair_name(),
            security_groups=security_groups,
            instance_type=instance_type,
            block_device_map=block_mapping,
            placement=placement
        )

        instance = reservation.instances[0]

        # Sometimes AWS fails to find the instance that it told us it just created
        # Give it some time
        for i in xrange(5):
            try:
                state = instance.update()
                break
            except boto.exception.EC2ResponseError as e:
                log.warn("Failed to get instance status from AWS; this is an AWS problem, but we will retry a few times in hopes that it resolves itself. Sleeping and trying again")
                time.sleep(5)

        # Wait for instance to come up
        start = time.time()
        for i in xrange(5):
            try:
                state = instance.update()
                break
            except boto.exception.EC2ResponseError as e:
                log.warn("Failed to find instance, AWS has failed us with its super-eventual consistency. Trying again after waiting a little while")
                time.sleep(3)

        while (time.time() - start) < wait_time:
            time.sleep(1)
            state = instance.update()
            log.info('Waiting on instance to come up')
            if state != 'pending':
                break
        else:
            raise SystemError('Requested instance never started')

        tag_ids = [instance.id]
        tag_ids += [v.volume_id for v in instance.block_device_mapping.values()]
        tags = {
            'deployment': self.deployment_id,
            'role': 'master',
            'type': 'deployment',
            'Name': self.deployment_id + '-master',
            'creator': get_id()
        }
        tags.update(extra_tags)
        self.ec2.create_tags(tag_ids, tags)

        return instance

    def launch_minion_instances(self, minion_ids, instance_type=DEFAULT_INSTANCE_TYPE,
        security_groups=None, extra_tags=None, wait_time=500,
        region=None, root_size=100, placement=None, ami=None):
        '''
        Launches an instance for the a minion node and waits until it's running

        Returns:
            running instance (boto.Instance): An instance object representing
            the new node
        '''
        placement = placement or self.get_placement()
        if security_groups is None:
            security_groups = []
        if extra_tags is None:
            extra_tags = {}
        if region is None:
            deployment_config = self.get_deployment_config()
            region = deployment_config.get('region', DEFAULT_REGION)

        salt_ami = ami or self._get_salt_ami(region=region, instance_type=instance_type)
        block_mapping = self._get_block_device_mapping(instance_type, root_size)

        reservation = self.ec2.run_instances(
            salt_ami,
            key_name=self.get_key_pair_name(),
            security_groups=security_groups,
            instance_type=instance_type,
            block_device_map=block_mapping,
            min_count=len(minion_ids),
            max_count=len(minion_ids),
            placement=placement
        )

        instances = reservation.instances
        log.info("Created reservation with id {}".format(reservation.id))

        # Wait for instances to come up
        start = time.time()
        instances_up = map(lambda x: x.update() != 'pending', instances)
        while (time.time() - start) < wait_time:
            time.sleep(1)
            instances_up = map(lambda x: x.update() != 'pending', instances)
            log.info('Waiting on instance(s) to come up')
            if all(instances_up):
                break
        else:
            raise SystemError('Requested instance never started')

        for i, minion_id in enumerate(minion_ids):
            tag_ids = [instances[i].id]
            tag_ids += [v.volume_id for v in
                instances[i].block_device_mapping.values()]
            tags = {
                'deployment': self.deployment_id,
                'role': 'minion',
                'type': 'deployment',
                'Name': self.deployment_id + '-' + minion_id,
                'minion_id': minion_id,
                'creator': get_id()
            }
            tags.update(extra_tags)
            self.ec2.create_tags(tag_ids, tags)

        return instances

    def _get_minion_names(self, pattern, count):
        minion_names = []
        deployment_config = self.get_deployment_config()
        minions = deployment_config.get('minions') or {}

        # Make sure they defined an appropriate pattern
        if pattern.format(count=0) == pattern.format(count=1):
            raise ValueError("You must specify a pattern of the form 'identifier-{count}'")

        for i in itertools.count():
            minion_name = pattern.format(count=i)
            if not minion_name in minions:
                minion_names.append(minion_name)
            if len(minion_names) == count:
                break

        if not len(minion_names) == len(set(minion_names)):
            raise ValueError("Template did not produce unique names")

        return minion_names

    def create_minion(self, minion_ids, instance_type=DEFAULT_INSTANCE_TYPE,
            extra_tags=None, extra_security_groups=None, count=1, gb_size=DEFAULT_VOLUME_SIZE_GB,
            make_mount=True, make_swap=True, region=None, ami=None,
            placement=None):
        '''
        Create minion instance(s)

        if minion_ids is a list and count is not specified, one minion will be created for each minion id

        use count if you want to launch many minions using a name pattern instead of explicit minion id assignment
        if count > 1, minion_ids should be a pattern of the form 'something-{count}'.format will be called
        on the pattern, subsitituting {count} for the index associated with the instance. E.g. the first instance will have
        count = 0, the second count = 1, etc.

        Numbers may be skipped if there already exists a minion in deployment.json with the same name as the specified one.
        '''
        if extra_security_groups is None:
            extra_security_groups = []
        if extra_tags is None:
            extra_tags = {}
        if region is None:
            deployment_config = self.get_deployment_config()
            region = deployment_config.get('region', DEFAULT_REGION)

        if instance_type not in INSTANCE_TYPES:
            raise ValueError('Unknown instance type provided {}'.format(instance_type))

        if count > 1 and len(minion_ids) == 1:
            minion_names = self._get_minion_names(pattern=minion_ids[0], count=count)
            #Remove the template so later code knows which ids to call
            minion_ids = minion_names
        else:
            minion_names = minion_ids

        # Make security groups and key pairs
        security_groups = self.ensure_security_groups(master=False, extra_groups=extra_security_groups)
        security_group_names = [g[0] for g in security_groups]
        key_pair, key_pair_material = self.ensure_deployment_keypair()

        # Launch the minion instance
        instances = self.launch_minion_instances(minion_ids=minion_names,
            instance_type=instance_type, security_groups=security_group_names,
            extra_tags=extra_tags, region=region, ami=ami, placement=placement)

        for i in xrange(len(instances)):
            min_id = minion_names[i]
            instance = instances[i]
            self.wait_for_ssh(host=instance.public_dns_name, ssh_user='ubuntu', minion_ids=[min_id])
            self.add_minion_to_config(min_id, instance=instance)


        if make_mount:
            # Mount /pipedream
            for i in xrange(len(instances)):
                instance = instances[i]
                min_id = minion_names[i]
                dev = self.make_mount(gb_size=gb_size, instance=instance)
                self.mount_volume(dev, '/pipedream', minion_ids=[min_id])

        if make_swap:
            self.mount_swap(minion_ids=minion_ids)



        return instance

    def create_master(self, instance_type=DEFAULT_INSTANCE_TYPE, extra_tags=None,
            extra_security_groups=None, make_mount=True,
            make_swap=True, gb_size=DEFAULT_VOLUME_SIZE_GB, ami=None):
        '''
        Create instance, set hostname to
        master.deployment_id.deepfield.net

        Returns:
            running instance (boto.Instance): Running master instance
        '''
        if extra_security_groups is None:
            extra_security_groups = []
        if extra_tags is None:
            extra_tags = {}

        if instance_type not in INSTANCE_TYPES:
            raise ValueError('Unknown instance type provided {}'.format(instance_type))

        # Check that we don't already have a master
        master_instance = self.get_master_instance()
        if not master_instance is None:
            raise ValueError(
                'Deployment with id {} already has a master'.format(self.deployment_id)
            )

        # Set up IAM user/group
        self.ensure_iam_user()
        self.ensure_iam_group()

        # Ensure security groups exist
        security_groups = self.ensure_security_groups(extra_groups=extra_security_groups)
        security_group_names = [g[0] for g in security_groups]

        # Ensure the public/private key pair exists for the deployment
        key_pair, key_pair_material = self.ensure_deployment_keypair()

        # Launch the master instance
        instance = self.launch_master_instance(instance_type=instance_type,
            security_groups=security_group_names, extra_tags=extra_tags,
            ami=ami)

        # Add master info to deployment.json
        current = self.get_deployment_config()
        current['master'] = {
            'architecture': instance.architecture,
            'launch_time': arrow.get(instance.launch_time).timestamp,
            'public_dns_address': instance.dns_name,
            'public_ip_address': instance.ip_address,
            'private_ip_address': instance.private_ip_address,
            'private_dns_address': instance.private_dns_name
        }
        self.save_deployment_config(current)

        # All done, wait for instance to come up
        self.wait_for_ssh()

        # Mount /pipedream
        if make_mount:
            dev = self.make_mount(gb_size=gb_size)
            self.mount_volume(dev, '/pipedream')

        if make_swap:
            self.mount_swap()

        return instance

    def check(self):
        '''
        Check the status of the deployment
        '''
        master_definition = self.get_master_definition()
        master_instance = self.get_master_instance()
        deployment_config = self.get_deployment_config()

        status = {}
        try:
            deployment_bucket = self.get_deployment_bucket()
            status['bucket_exists'] = deployment_bucket is not None
        except Exception as e:
            status['bucket_exists'] = False
        status['deployment_bootstrapped'] = deployment_bootstrapped(self.deployment_id)
        status['master_defined'] = master_definition is not None
        status['master_instance_created'] = master_instance is not None
        status['master_initialized'] = master_definition is not None and master_definition.get('initialized', False)

        # IAM status
        user, group = get_deployment_user_and_group(self.deployment_id, self.get_deployment_config())
        try:
            self.iam.get_user(user)
            status['iam_user_present'] = True
        except Exception as e:
            status['iam_user_present'] = False

        try:
            self.iam.get_group(group)
            status['iam_group_present'] = True
        except Exception as e:
            status['iam_group_present'] = False

        return status


    def destroy_minions(self, minion_ids):
        '''
        Decommission specified minion
        '''
        for minion_id in minion_ids:
            minion_destroyed = False
            try:
                minion_destroyed = self.destroy_minion_instance(minion_id)
            except ValueError:
                pass

            if not minion_destroyed:
                log.warn("Did not destroy any instances for minion with id {}".format(minion_id))

            try:
                self.remove_minions(minion_ids=[minion_id])
            except KeyError:
                log.warn("Did not remove minion with id {} from config because it was not present".format(minion_id))

    def suspend_minions(self, minion_ids, force=False, dry_run=False):
        '''
        Suspend specified minions
        '''
        instances = [self.get_minion_instance(mid) for mid in minion_ids]
        instance_ids = [i.id for i in instances]
        stopped = self.ec2.stop_instances(instance_ids, force=force)
        log.info("Stopping instances with IDs: {}".format(stopped))

    def resume_minions(self, minion_ids, dry_run=False):
        '''
        Resume specified minions
        '''
        instances = [self.get_minion_instance(mid) for mid in minion_ids]
        instance_ids = [i.id for i in instances]
        started = self.ec2.start_instances(instance_ids, dry_run=dry_run)
        log.info("Resuming instances with IDs: {}".format(started))

    def suspend(self, force=False, dry_run=False):
        '''
        Suspend the deployment
        '''
        instances = [self.get_master_instance()]
        instance_ids = [i.id for i in instances]
        stopped = self.ec2.stop_instances(instance_ids, force=force)
        log.info("Stopping instances with IDs: {}".format(stopped))

        deployment_config = self.get_deployment_config()
        minions = deployment_config.get('minions', {}).keys()
        self.suspend_minions(minions, force=force, dry_run=dry_run)

    def resume(self, dry_run=False):
        '''
        Resume the deployment
        '''
        instances = [self.get_master_instance()]
        instance_ids = [i.id for i in instances]
        started = self.ec2.start_instances(instance_ids, dry_run=dry_run)
        log.info("Resuming instances with IDs: {}".format(started))

        deployment_config = self.get_deployment_config()
        minions = deployment_config.get('minions', {}).keys()
        self.resume_minions(minions, dry_run=dry_run)

    def destroy_minion_instance(self, minion_id):
        '''
        Decommission specified minion instance

        Returns:
            True if minion instance terminated, False otherwise
        '''
        minion_instance = self.get_minion_instance(minion_id)
        if minion_instance is not None:
            minion_instance.terminate()
            log.info("Destroyed instance with id {}".format(minion_instance.id))
            return True
        else:
            return False

    def destroy_master_instance(self):
        '''
        Terminate the master instance. Returns True if it terminated, Falst
        otherwise.
        '''
        master_instance = self.get_master_instance()
        if master_instance is not None:
            master_instance.terminate()
            return True
        else:
            return False

    def destroy_iam_entries(self):
        '''
        Remove all of the associated IAM users, groups, etc.
        '''
        # Clean up IAM users and groups
        user, group = get_deployment_user_and_group(self.deployment_id, self.get_deployment_config())
        try:
            group_response = self.iam.get_groups_for_user(user)
            for group_entry in group_response['list_groups_for_user_response']['list_groups_for_user_result']['groups']:
                group_name = group_entry['group_name']
                try:
                    self.iam.remove_user_from_group(group_name, user)
                except Exception as e:
                    log.warn(
                        "Failed to remove user {} from deployment group".format(user)
                    )

        except Exception as e:
            log.warn("Failed to load groups for user {}".format(user))

        try:
            # Delete group policies
            group_policies = self.iam.get_all_group_policies(group)['list_group_policies_response']['list_group_policies_result']['policy_names']

            for gp in group_policies:
                self.iam.delete_group_policy(group, gp)
            self.iam.delete_group(group)
        except Exception as e:
            log.warn("Failed to delete group {}".format(group))

        try:
            delete_all_user_access_keys(user, self.iam)
            self.iam.delete_user(user)
        except Exception as e:
            log.error("Failed to remove user {}".format(user))

    def destroy(self, preserve_bucket=False):
        '''Terminate deployment instances, Remove users, etc'''

        deployment_config = self.get_deployment_config()
        if deployment_config:
            minions = deployment_config.get('minions') or {}
            self.destroy_minions(minion_ids=list(minions.keys()))
        self.destroy_master_instance()
        self.destroy_iam_entries()

        self.destroy_sqs_queue()
        self.destroy_deployment_security_group()

        if not preserve_bucket:
            try:
                deployment_bucket = self.get_deployment_bucket()
                deployment_bucket.delete_keys(deployment_bucket.list())
                deployment_bucket.delete()
            except boto.exception.S3ResponseError as e:
                log.warn("Failed to destroy bucket {}".format(get_bucket_name_for_deployment(self.deployment_id)))
                if e.status == 404:
                    pass
                else:
                    raise e

    def update_route53_entry(self, ip, name=None, action='CREATE',
            zone_name=DEEPFIELD_ZONE_NAME, record_type='A', ttl=3600,
            zone_id=DEEPFIELD_NET_ZONE_ID, minion_id=None):

        zone = self.r53.get_zone(zone_name)
        if name is None:
            name = minion_id
        if name is None:
            name = self.deployment_id

        fullname = '{}.{}'.format(name, zone_name)
        record = zone.find_records(fullname, record_type)

        try:
            if record:
                zone.update_record(record, ip)
            else:
                rrsets = boto.route53.record.ResourceRecordSets(self.r53, zone_id)
                change = rrsets.add_change(action, fullname, record_type, ttl=ttl)
                change.add_value(ip)
                rrsets.commit()
        except boto.route53.exception.DNSServerError:
            log.warn('dnsrecord-creation-failed')

        return fullname

    def list_route53_entries(self, name=None, zone_name=DEEPFIELD_ZONE_NAME,
            record_type='A', minion_id=None, all_ids=None):

        names = []
        if all_ids is not None:
            deployment_config = self.get_deployment_config()
            if deployment_config:
                names.extend(deployment_config.get('minions', {}).keys())
        if minion_id is not None:
            names.append(minion_id)
        if name is not None:
            names.append(name)
        names.append(self.deployment_id)

        fullnames = ['{}.{}'.format(n, zone_name) for n in names]

        zone = self.r53.get_zone(zone_name)
        records = [zone.find_records(n, record_type, all=True) for n in fullnames]
        records = funcy.flatten(records) # Flatten since 'all=True'
        records = [r for r in records if r] # Remove null values
        resource_records = get_resource_records_from_records(records)

        reservations = get_reservations()
        dns, ip = get_dns_ip_from_reservations(reservations)

        print_route53_entries(dns, ip, resource_records)

class PrivateDeployment(PartialAWSDeploymentMixIn, Deployment):

    def __init__(self, deployment_id, s3, r53, iam):
        super(PrivateDeployment, self).__init__(deployment_id)

        self.s3 = s3
        self.r53 = r53
        self.iam = iam
        self.kind = 'private'

    def destroy_minions(self, minion_ids):

        for minion_id in minion_ids:
            try:
                self.remove_minions(minion_ids=[minion_id])
            except KeyError:
                log.warn("Did not remove minion with id {} from config because it was not present".format(minion_id))

    def update_route53_entry(self, *args, **kws):
        '''
        Could move AWSDeployment update_route53_entry up into Deployment ...
        '''
        pass


    def create_master(self, **kwargs):
        '''Create the master for this deployment'''
        raise NotImplementedError(
            "Master creation is not implemented for private clouds"
        )

    def ensure_group_policy(self):
        '''Ensure that proper policy is applied to this deployment's AWS
        group'''
        username, group = get_deployment_user_and_group(self.deployment_id, self.get_deployment_config())
        s3_policy = json.dumps(generate_s3_policy(self.deployment_id))
        sqs_policy = json.dumps(generate_sqs_policy(self.deployment_id))

        self.iam.put_group_policy(group, 'S3Policy', s3_policy)
        self.iam.put_group_policy(group, 'SQSPolicy', sqs_policy)

    def initialize_master(self, **kwargs):
        '''Initialize the master for this deployment'''
        # Set up DNS
        self.configure_master_dns()

        # Ensure key pair exists
        key_pair, key_pair_content = self.ensure_deployment_keypair()

        # Set up IAM user/group
        self.ensure_iam_user()
        self.ensure_iam_group()
        self.ensure_group_policy()

        # Create the key pair and save it to the deployment descriptor
        self.ensure_credentials()

        # Save out boto config
        self.configure_boto()

        # Install packages
        self.install_base_packages()

        # Configure salt
        self.prepare_salt_installation()
        self.install_salt()
        self.configure_salt()

        # Mark master as initialized
        self.mark_master_as_initialized()

    def check(self):
        '''Return a dictionary of status checks'''
        # PrivateDeployment
        raise NotImplementedError('''deployment check not implemented yet for private deployments (but should be)''')

    def destroy(self, preserve_bucket=False):
        '''Destroy this deployment

        Since we have no control over the compute resources, only destroys bucket.
        '''

        if not preserve_bucket:
            try:
                deployment_bucket = self.get_deployment_bucket()
                deployment_bucket.delete_keys(deployment_bucket.list())
                deployment_bucket.delete()
            except boto.exception.S3ResponseError as e:
                if e.status == 404:
                    pass
                else:
                    raise e

class IsolatedDeployment(Deployment):

    def __init__(self, deployment_id):
        super(IsolatedDeployment, self).__init__(deployment_id)
        self.kind = 'isolated'

    def bootstrap_deployment(self):
        # Set force remote to local
        slice_update = {'isolated_deployment': True}
        deepy.util.ensure_directory('/pipedream/cache/config')
        with open('/pipedream/cache/config/slice.json', 'w') as f:
            json.dump(slice_update, f, indent=2)

        # Ensure key pair exists
        key_pair_content, key_pair_public_key = self.ensure_deployment_keypair()
        log.debug("ensured-key-pair-exists: {}".format(key_pair_content))

        # Create config files
        log.debug('creating-initial-config-files')
        create_initial_config_files(self.deployment_id, kind=self.kind, region=None)
        log.debug('initial-config-files-created')

    def setup_fabric(self, **kwargs):
        minion_ids = kwargs.get('minion_ids')

        env.forward_agent = True
        env.disable_known_hosts = True
        master_definition = self.get_master_definition()
        if master_definition is None:
            raise ValueError("Failed to retrieve master definition for deployment {}".format(self.deployment_id))
        env.hosts = [master_definition['public_ip_address']]
        env.user = 'ubuntu'
        private_key, public_key = self.ensure_deployment_keypair()
        tempfile_directory = os.path.join(deepy.cfg.hood_dir, 'tmp')
        deepy.util.ensure_directory(tempfile_directory)
        with tempfile.NamedTemporaryFile(delete=False, dir=tempfile_directory, prefix='tmpkey') as fp:
            fp.write(private_key)
            log.debug("Using key pair at {}".format(fp.name))
            env.key_filename = fp.name

    def initialize_master(self, **kwargs):
        '''Initialize the master for this deployment'''

        # Ensure key pair exists
        private_key, public_key = self.ensure_deployment_keypair()

        # Make sure we can log in using the deployment's key pair
        if not os.path.exists('/home/ubuntu'):
            raise ValueError('Isolated deployment cannot be initialized without the ubuntu user on the local machine')
        subprocess.call(['sudo', 'mkdir', '-p', '/home/ubuntu/.ssh'])
        subprocess.call('sudo bash -c \'echo "{}" > /home/ubuntu/.ssh/authorized_keys \'; sudo chown -R ubuntu /home/ubuntu/.ssh '.format(public_key), shell=True)

        # Configure salt (it was already installed by the installer)
        self.prepare_salt_installation()

        self.execute(lambda: append('/etc/salt/grains', 'offline: True', use_sudo=True))
        self.configure_salt(with_git=False)

        # Mark master as initialized
        self.mark_master_as_initialized()

    def start(self):
        raise NotImplementedError("Cannot start isolated deployments (the issue is that we need to copy maps and notices over by hand since we have no s3 connection)")

class TemporaryDeployment(AWSDeployment):
    '''
    A temporary deployment for use by build_ami.py

    Note: Fabric is assumed to already be set up!
    '''

    def __init__(self, deployment_id, master_definition, s3, iam, ec2, r53):
        super(TemporaryDeployment, self).__init__(deployment_id, s3=s3, r53=r53, iam=iam, ec2=ec2)
        self._master_definition = master_definition

    def get_master_definition(self):
        return self._master_definition

    def setup_fabric(self, **kwargs):
        pass

    def get_deployment_config(self):
        return {}
    def get_slice_config(self):
        return {}

class ShadowDeployment(AWSDeployment):

    def __init__(self, deployment_id, s3=None, iam=None, ec2=None, r53=None):
        super(ShadowDeployment, self).__init__(deployment_id, s3=s3, ec2=ec2, r53=r53, iam=iam)
        self.kind = 'shadow'

def get_hosts():
    config = get_deployment_config(deepy.cfg.deployment_id)
    hosts = [config['master']['private_ip_address']]
    minions = config.get('minions', {})
    for name, min_config in minions.items():
        ip = min_config['private_ip_address']
        hosts.append(ip)

    # too slow, up to a second
    # hosts = deepy.salt.get_hosts('*')
    return hosts

def get_vss_isp_positions():
    '''
    get vss (cablelabs) isp dimension positions
    '''
    isp_positions_filename = os.path.join(deepy.cfg.defines_dir, 'cablelabs-isp.json.gz')
    return deepy.store.simple_load_json(isp_positions_filename)

def generate_vss_master_s3_policy_statements():
    statements = []

    isp_positions = get_vss_isp_positions()
    isps = isp_positions.get('isps', {}).keys()

    isp_buckets = map(lambda x: '{}.pdrlabs.net'.format(x), isps)

    for bucket in isp_buckets:
        # List all buckets
        statements.append(
            generate_policy_statement(
                actions=['s3:List*'],
                effect='Allow',
                resource='arn:aws:s3:::' + bucket
            )
        )

        # Read and list access to VSS key prefix
        statements.append(
            generate_policy_statement(
                actions=['s3:Get*', 's3:List*'],
                effect='Allow',
                resource='arn:aws:s3:::{}/vss*'.format(bucket)
            )
        )

    return statements

def configure_role_vss_master(deployment):
    '''Give this deployment permissions to read /vss in all deployment buckets.'''
    master_uuid = deployment.get_master_uuid()
    if not master_uuid:
        deepy.log.error('No master uuid')
        return False

    deepy.log.info('Configuring S3 permissions for role vss-master on {}'.format(deployment.deployment_id))

    vss_master_statements = generate_vss_master_s3_policy_statements()

    username, group = get_deployment_user_and_group(deployment.deployment_id, deployment.get_deployment_config())

    # Extend default policy
    policy = generate_s3_policy(deployment.deployment_id)
    statements = policy['Statement']
    statements.extend(vss_master_statements)

    # Remove current policy
    deployment.iam.delete_group_policy(group, 'S3Policy')

    # Save
    deployment.iam.put_group_policy(group, 'S3Policy', json.dumps(policy))

    deepy.log.warning('Manually add vss-master-fetch to extra_cron_jobs! See CI-1467')
    # XXX refactor to insert in to extra_cron_jobs
    #deepy.log.info('Adding vss-master-fetch to cron_jobs on {}'.format(master_uuid))
    #deployment.add_cron_job('vss-master-fetch', master_uuid)
    return True

_ROLES_CONFIGURATORS = collections.defaultdict(lambda: None)
_ROLES_CONFIGURATORS['vss-master'] = configure_role_vss_master

def roles_configurators(roles):
    '''
    Returns functions to call to configure roles.

    These functions take deployment instances and return truthy on success.
    '''
    return filter(None, funcy.map(_ROLES_CONFIGURATORS, roles))

def to_device_mapping(device_name):
    if not device_name:
        return device_name

    replace = re.compile('sd')
    return '/dev/' + replace.sub('xvd', device_name, count=1)

