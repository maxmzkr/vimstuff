import os
import signal
import subprocess
import ConfigParser
import re

import deepy.ini
import deepy.util
import deepy.dimensions
import deepy.metrics
import deepy.deploy
import deepy.provision
import deepy.log as log
DEFAULT_CRON_CFG = '''#
# Automatically generated by restart_daemons.py
# Disable mail
MAILTO=""
PATH=/pipedream/bin:/usr/bin:/bin:/sbin:/usr/sbin:/usr/local/sbin:/var/local/pipedream/ui
#
# [minute] [hour] [day of month] [month] [day of week] [command]
@reboot root deployment_sync.py
@reboot root restart_daemons.py
@reboot root deployment_update_uuid.py 2>&1 | logger -t deployment_update_uuid.py
*/1 * * * * root restart_daemons.py
*/10 * * * * root deployment_sync.py
*/1 * * * * root deployment_sync.py -c
*/22 * * * *  root prune_files.py  2>&1 | logger -t prune_files.py
1-59/5 * * * * root heartbeat.py   2>&1 | logger -t heartbeat.py
*/5  * * * * root collect_metrics.py --collect-periodic --time 5min
*/30 * * * * root collect_metrics.py --collect-periodic --time 30min
0  * * * * root collect_metrics.py --collect-periodic --time 1h
*/30 * * * * root ntpdate ntp.ubuntu.com 2>&1 | logger -t ntpdate
30 23 * * * root rotate_conf.py 2>&1 | logger -t rotate_conf.py
'''

daemon_config = {
    'home.py':                  'sudo nice -n -19 %s/home.py 2>&1 | logger -t home.py &' % (deepy.cfg.ui_dir),
    'home.py-no-celery':        'sudo nice -n -19 %s/home.py --no-celery 2>&1 | logger -t home.py &' % (deepy.cfg.ui_dir),
    'home.py-store-only':       'sudo nice -n -19 %s/home.py --store-only -P home.py-store-only 2>&1 | logger -t home.py &' % (deepy.cfg.ui_dir),
    'redirect.py':              'sudo nice -n -19 %s/redirect.py 2>&1 | logger -t redirect.py &' % (deepy.cfg.ui_dir),
    'hdfs_daemon.py':           'hdfs_daemon.py 2>&1 | logger -i -t hdfs_daemon &',
    'jobs_daemon.py':           'jobs_daemon.py 2>&1 | logger -t jobs_daemon.py &',
     #'beacon.py':                'beacon.py 2>&1 | logger -t beacon.py &',
    'github_buildbot':          'github_buildbot.py -l %s --pidfile %s 2>&1 | logger -t github_buildbot &' % (os.path.join(deepy.cfg.logs_dir, "github_buildbot.log"), os.path.join(deepy.cfg.pids_dir, "github_buildbot.pid")),
    'flowd':                    'flow_d.py &',
    'h5flow_daemon1':           'flow.py --daemon 1 2>&1 | logger -t h5flow_daemon &',
    'h5flow_daemon2':           'flow.py --daemon 2 2>&1 | logger -t h5flow_daemon &',
    'h5flow_daemon3':           'flow.py --daemon 3 2>&1 | logger -t h5flow_daemon &',
    'h5flow_daemon4':           'flow.py --daemon 4 2>&1 | logger -t h5flow_daemon &',
    'h5flow_daemon5':           'flow.py --daemon 5 2>&1 | logger -t h5flow_daemon &',
    'h5flow_daemon6':           'flow.py --daemon 6 2>&1 | logger -t h5flow_daemon &',
    'dnsflow_daemon1':          'dnsflow.py --daemon 1 2>&1 | logger -t dnsflow_daemon &',
    'dnsflow_daemon2':          'dnsflow.py --daemon 2 2>&1 | logger -t dnsflow_daemon &',
    'dnsflow_daemon3':          'dnsflow.py --daemon 3 2>&1 | logger -t dnsflow_daemon &',
    'dnsflow_daemon4':          'dnsflow.py --daemon 4 2>&1 | logger -t dnsflow_daemon &',
    'dnsflow_daemon5':          'dnsflow.py --daemon 5 2>&1 | logger -t dnsflow_daemon &',
    'dnsflow_daemon6':          'dnsflow.py --daemon 6 2>&1 | logger -t dnsflow_daemon &',
    'classify_daemon1':         'classify.py --daemon 1 2>&1 | logger -t classify_daemon &',
    'classify_daemon2':         'classify.py --daemon 2 2>&1 | logger -t classify_daemon &',
    'classify_daemon3':         'classify.py --daemon 3 2>&1 | logger -t classify_daemon &',
    'classify_daemon4':         'classify.py --daemon 4 2>&1 | logger -t classify_daemon &',
    'classify_daemon5':         'classify.py --daemon 5 2>&1 | logger -t classify_daemon &',
    'classify_daemon6':         'classify.py --daemon 6 2>&1 | logger -t classify_daemon &',
    'classify_daemon7':         'classify.py --daemon 7 2>&1 | logger -t classify_daemon &',
    'classify_daemon8':         'classify.py --daemon 8 2>&1 | logger -t classify_daemon &',
    'cube_daemon1':             'cubes_from_h5flow.py --daemon 1 2>&1 | logger -t cube_daemon &',
    'cube_daemon2':             'cubes_from_h5flow.py --daemon 2 2>&1 | logger -t cube_daemon &',
    'cube_daemon3':             'cubes_from_h5flow.py --daemon 3 2>&1 | logger -t cube_daemon &',
    'cube_daemon4':             'cubes_from_h5flow.py --daemon 4 2>&1 | logger -t cube_daemon &',
    'cube_daemon5':             'cubes_from_h5flow.py --daemon 5 2>&1 | logger -t cube_daemon &',
    'cube_daemon6':             'cubes_from_h5flow.py --daemon 6 2>&1 | logger -t cube_daemon &',
    'cube_daemon7':             'cubes_from_h5flow.py --daemon 7 2>&1 | logger -t cube_daemon &',
    'cube_daemon8':             'cubes_from_h5flow.py --daemon 8 2>&1 | logger -t cube_daemon &',
    'snmpd':                    'sudo service snmpd restart',
    'bgpd':                     '/usr/lib/quagga/bgpd -i %s -d -n -A 127.0.0.1  2>&1 | logger -i -t bgpd' % (os.path.join(deepy.cfg.pids_dir, "bgpd.pid")),
    'bird':                     '/usr/local/sbin/bird -P %s 2>&1 | logger -i -t bird' % (os.path.join(deepy.cfg.pids_dir, "bird.pid")),
    'bird6':                    '/usr/local/sbin/bird6 -P %s 2>&1 | logger -t bird6' % (os.path.join(deepy.cfg.pids_dir, "bird6.pid")),

    # Set up for a particular deployment, maybe parameterize later
    'local_pcap_dnsflow':       'dnsflow -i eth0 -u 127.0.0.1 -X 5301 -P %s 2>&1 | logger -t local_pcap_dnsflow &' % (os.path.join(deepy.cfg.pids_dir, "local_pcap_dnsflow.pid")),
    'jmirror_dnsflow':          'dnsflow -i eth0 -u 127.0.0.1 -J 30030 -M 4 -P %s 2>&1 | logger -t jmirror_dnsflow &' % (os.path.join(deepy.cfg.pids_dir, "jmirror_dnsflow.pid")),

    # Testing
    'mock_queue_flow.py':       'mock_queue_flow.py  2>&1',
}

cron_jobs_config = {
    # added explicit ranges n-59 to attempt to reduce lock contention on important queuing operations
    # remember: * == 0-59
    'prune_files_hdfs':         '*/22 * * * *  root prune_files.py --hdfs 2>&1 | logger -t prune_files.py',
    'asndb_sync':               '0 12 * * * root asndb_sync.py | logger -i -t asndb_sync.py',
    'check_logs':               '0 */4 * * * root check_logs.py | /usr/bin/logger -i -t check_logs',
    'reload_bundles_cache':     '0 */2 * * * root reload_cache.py --bundles -l W | /usr/bin/logger -i -t reload_cache',
    'reload_files_cache':       '* * * * * root reload_cache.py --files | /usr/bin/logger -i -t reload_cache',
    'reload_dim_db_cache':      '4,34 * * * * root reload_cache.py --dim-db | /usr/bin/logger -i -t reload_cache',
    'save_cache':               '*/4 * * * * root save_cache.py | /usr/bin/logger -i -t save_cache',
    'save_flows_remote':        '1-59/5 * * * * root save_flows_remote.py 2>&1 | logger -i -t save_flows_remote.py',
    'fake_flowd_replay':        '1-59/5 * * * * root replay_flow.py 2>&1 | logger -i -t replay_flow.py',
    'fake_flowd':               '*/5 * * * * root fake_flowd.py 2>&1 | logger -i -t fake_flowd.py',
    'fake_flowd_dns':           '*/5 * * * * root fake_flowd.py -d 2>&1 | logger -i -t fake_flowd.py',
    'fake_flowd_s3':            '*/5 * * * * root fake_flowd_s3.py 2>&1 | logger -i -t fake_flowd_s3.py',
    'fake_h5flow':              '*/5 * * * * root fake_flowd.py --h5flow 2>&1 | logger -i -t fake_flowd.py',
    'fake_flowd_dns_s3':        '*/5 * * * * root fake_flowd_s3.py --dnsflow 2>&1 | logger -i -t fake_flowd_s3.py',
    'fake_flowd_snmp_s3':       '*/5 * * * * root fake_flowd_s3.py --snmp 2>&1 | logger -i -t fake_flowd_s3.py',
    'deployment_pull':          '*/5 * * * * root deployment_pull.py -R 2>&1 | logger -i -t deployment_pull.py',
    'mine':                     '8 * * * * root sub_mine.py -q all 2>&1 | logger -i -t sub_mine.py',
    'mine_forward':             '15 * * * * root mine_forward.py -r 4 2>&1 | logger -i -t sub_mine_forward.py',
    'build_h5flow':             '2-59/5 * * * * root jobs.py -r 48 make h5flow 2>&1 | logger -i -t jobs.py',
    'build_h5dns':              '*/4 * * * * root jobs.py -r 48 make h5dns 2>&1 | logger -i -t jobs.py',
    'classify_h5flow':          '2-59/5 * * * * root jobs.py -r 48 make classify_h5flow 2>&1 | logger -i -t jobs.py',
    'build_h5cubes_5min':       '3-59/4 * * * * root jobs.py -r 48 make cubes_from_h5flow_5min 2>&1 | logger -i -t jobs.py',
    'build_stream_bps':         '3-59/4 * * * * root jobs.py -r 72 make cubes_stream_bps_5min 2>&1 | logger -i -t build_stream_bps.py',
    'build_h5flow_long':             '17 * * * * root jobs.py -r 80 make h5flow 2>&1 | logger -i -t jobs.py',
    'build_h5dns_long':              '25 * * * * root jobs.py -r 80 make h5dns 2>&1 | logger -i -t jobs.py',
    'classify_h5flow_long':          '38 * * * * root jobs.py -r 80 make classify_h5flow 2>&1 | logger -i -t jobs.py',
    'build_h5cubes_5min_long':       '47 * * * * root jobs.py -r 80 make cubes_from_h5flow_5min 2>&1 | logger -i -t jobs.py',
    'build_h5cubes_hour':       '15 * * * * root jobs.py -r 25 make cubes_from_h5flow_ipcount_hour 2>&1 | logger -i -t jobs.py',

     # temorary to make site_peer still work in real-time deployments
    'build_site_peer':          '*/5 * * * * root cubes_from_h5flow.py --site-peer 2>&1 | logger -i -t build_site_peer',

    'searchips':                '*/5 * * * * root jobs.py -r 10 make searchips | logger -i -t jobs.py',
    'upload_localbgp':          '30 * * * * root upload_localbgp.py 2>&1 | logger -i -t upload_localbgp.py',
    'dump_localbgp':            '20 * * * * root pybird.py dump 2>&1 | logger -i -t dump_localbgp',
    'dump_localbgp6':           '20 * * * * root pybird.py -6 dump 2>&1 | logger -i -t dump_localbgp6',
    'build_h5bgp':              '45 */2 * * * root jobs.py make h5bgp 2>&1 | logger -i -t bgp.py ',
    'merge_h5flow':             '*/5 * * * * root merge_h5flow.py ',
    'merge_h5cube':             '*/5 * * * * root merge_h5cube.py ',
    'merge_h5hll':              '*/5 * * * * root merge_hll.py ',
    'merge_h5hll_hour':         '5 * * * *   root merge_hll.py --merge-hour',

    # qwilt
    'qwilt_download_logs':       '*/5 * * * * root get_qwilt_logs.py 2>&1 | logger -i -t qwilt_download_logs',
    'qwilt_prune_logs':          '37 * * * * root prune_qwilt_logs.py 2>&1 | logger -i -t qwilt_prune_logs.py',
    'qwilt_aggregate_logs':      '*/5 * * * * root qwilt_aggregate_logs.py -r 50 2>&1 | logger -i -t qwilt_aggregate_logs',
    'h5qwilt':                   '*/5 * * * * root jobs.py make h5qwilt -r 50 2>&1 | logger -i -t h5qwilt',
    'h5qwilt_manual':            '*/5 * * * * root h5qwilt.py -r 50 2>&1 | logger -i -t h5qwilt',

    # impala
    'repair_gaps':               '*/15 * * * * root repair_gaps.py | logger -i -t repair_gaps.py',
    'compact_cubes_repair':      '30 */1 * * * root compact_cubes.py --no-compute-stats --day-view-off --view-off --repair  2>&1 | logger -i -t repair_compact_cubes.py',
    'compact_cubes':             '0 */1 * * * root compact_cubes.py --no-compute-stats 2>&1 | logger -i -t compact_cubes.py',
    'compact_cubes_view':        '*/3 * * * * root compact_cubes.py --compact-off 2>&1 | logger -i -t compact_cubes_view.py',
    'loadcubequeue':             '*/3 * * * * root loadcubequeue.py --shuffle --from-slice 2>&1 | logger -i -t loadcubequeue.py',
    'test_impalad':              '*/1 * * * * root test_impalad.py 2>&1 | logger -i -t test_impalad.py',
    'install_udfs':              '0 0 * * * root install_udfs.py 2>&1 | logger -i -t install_udfs.py',
    'impala_index':              '*/5 * * * * root impala_index.py --serial --scan-all 2>&1 | logger -i -t impala_index.py',

    # bgp faster
    'dump_localbgp_5min':       '*/5 * * * * root pybird.py dump 2>&1 | logger -i -t dump_localbgp_5min',
    'upload_localbgp_1min':     '*/1 * * * * root upload_localbgp.py 2>&1 | logger -i -t upload_localbgp_1min',
    'build_h5bgp_1min':         '*/1 * * * * root jobs.py make h5bgp 2>&1 | logger -i -t bgp.py_1min',

    'big_cube_hour':            '15 * * * * root jobs.py -r 10 make cube_big_cube_hour | logger -i -t jobs.py',
    'backbone_hour':            '15 * * * * root jobs.py -r 10 make backbone_hour | logger -i -t jobs.py',
    'cube_from_hb':             '*/5 * * * * root cube_from_hb.py | logger -i -t cube_from_hb.py',
    'router_stat':              '*/5 * * * * root jobs.py -r 12 make router_stat | logger -i -t jobs.py',
    'cubes_hour':               '4-59/15 * * * * root jobs.py -r 15 make cubes_hour | logger -i -t jobs.py',
    'cube_snmp':                '49 * * * * root jobs.py -f make cube_snmp | logger -i -t jobs.py',
    'cubes_day':                '30 * * * * root jobs.py -r 7 make cubes_day | logger -i -t jobs.py',
    'cubes_month':              '*/30 * * * * root jobs.py -r 2 make cubes_month | logger -i -t jobs.py',
    'cubes_5min':               '*/5 * * * * root jobs.py -r 80 make cubes_5min | logger -i -t jobs.py',
    'aggregate_dimensions':     '*/5 * * * * root jobs.py -r 1 make aggregate_dimensions | logger -i -t jobs.py',

    # Craig new in sprint-14-snmp
    'cube_snmp_5min':           '*/5 * * * * root cube_from_snmp_new.py | logger -i -t cube_from_snmp_new.py',
    'cube_snmp_hour':           '*/5 * * * * root cube_from_snmp_new.py --use-hour | logger -i -t cube_from_snmp_new.py',
    'snmp5min':                 '*/5 * * * * root snmp.py 2>&1| logger -i -t snmp.py',

    # temorary to make site_peer still work in real-time deployments
    'build_site_peer':          '*/5 * * * * root cubes_from_h5flow.py --site-peer 2>&1 | logger -i -t build_site_peer',

    # bundles
    # Run once with -r 2 after each step period (day, month)
    'bundle2_backbone1_summary':    ['5 1-23/2 * * * root jobs.py      make bundle2_backbone1_summary | /usr/bin/logger -i -t bundle2_backbone1_summary',
                                     '5 3      1 * * root jobs.py -r 2 make bundle2_backbone1_summary | /usr/bin/logger -i -t bundle2_backbone1_summary'],
    'bundle2_h5backbone_summary':   ['5 1-23/2 * * * root jobs.py      make bundle2_h5backbone_summary| /usr/bin/logger -i -t bundle2_h5backbone_summary',
                                     '5 3      1 * * root jobs.py -r 2 make bundle2_h5backbone_summary| /usr/bin/logger -i -t bundle2_h5backbone_summary'],

    'drill_rules_day_summary':  ['0 0-22/2 * * * root jobs.py      make drill_rules_day_summary | /usr/bin/logger -i -t drill_rules_day_summary',
                                 '0 3      * * * root jobs.py -r 2 make drill_rules_day_summary | /usr/bin/logger -i -t drill_rules_day_summary'],
    'drill_rules_day_drill':    ['10 1-23/4 * * * root jobs.py      make drill_rules_day_drill | /usr/bin/logger -i -t drill_rules_day_drill',
                                 '10 1      * * * root jobs.py -r 2 make drill_rules_day_drill | /usr/bin/logger -i -t drill_rules_day_drill'],

    'drill_rules_month_summary':['30 0-22/2 * * * root jobs.py      make drill_rules_month_summary | /usr/bin/logger -i -t drill_rules_month_summary',
                                 '30 1      1 * * root jobs.py -r 2 make drill_rules_month_summary | /usr/bin/logger -i -t drill_rules_month_summary'],
    'drill_rules_month_drill':  ['30 1-23/6 * * * root jobs.py      make drill_rules_month_drill | /usr/bin/logger -i -t drill_rules_month_drill',
                                 '30 1      1 * * root jobs.py -r 2 make drill_rules_month_drill | /usr/bin/logger -i -t drill_rules_month_drill'],

    'bundles_deployment':       ['20 1-23/4 * * * root      jobs.py make bundles_deployment | /usr/bin/logger -i -t bundles_deployment',
                                 '20 1      * * * root -r 2 jobs.py make bundles_deployment | /usr/bin/logger -i -t bundles_deployment'],
    #Only run bundles a couple of times a day and avoid overlaps
    'limited_bundles': [
                        '0  20       *    * * root jobs.py      make bundle2_backbone1_summary | /usr/bin/logger -i -t bundle2_backbone1_summary',
                        '0  9        2-31 * * root jobs.py      make bundle2_backbone1_summary | /usr/bin/logger -i -t bundle2_backbone1_summary',
                        '0  9        1    * * root jobs.py -r 2 make bundle2_backbone1_summary | /usr/bin/logger -i -t bundle2_backbone1_summary',
                        '0  19    *    * * root jobs.py      make bundle2_h5backbone_summary| /usr/bin/logger -i -t bundle2_h5backbone_summary',
                        '0  8        2-31 * * root jobs.py      make bundle2_h5backbone_summary| /usr/bin/logger -i -t bundle2_h5backbone_summary',
                        '0  8        1    * * root jobs.py -r 2 make bundle2_h5backbone_summary| /usr/bin/logger -i -t bundle2_h5backbone_summary',
                        '0  10,15,18 *    * * root jobs.py      make drill_rules_day_drill | /usr/bin/logger -i -t drill_rules_day_drill',
                        '0  9        *    * * root jobs.py -r 2 make drill_rules_day_drill | /usr/bin/logger -i -t drill_rules_day_drill',
                        '0  11,14,21 *    * * root jobs.py      make drill_rules_day_summary | /usr/bin/logger -i -t drill_rules_day_summary',
                        '0  10       *    * * root jobs.py -r 2 make drill_rules_day_summary | /usr/bin/logger -i -t drill_rules_day_summary',
                        '0  18       *    * * root jobs.py      make drill_rules_month_drill | /usr/bin/logger -i -t drill_rules_month_drill',
                        '0  12       2-31 * * root jobs.py      make drill_rules_month_drill | /usr/bin/logger -i -t drill_rules_month_drill',
                        '0  12       1    * * root jobs.py -r 2 make drill_rules_month_drill | /usr/bin/logger -i -t drill_rules_month_drill',
                        '0  7,21       *    * * root jobs.py      make drill_rules_month_summary | /usr/bin/logger -i -t drill_rules_month_summary',
                        '0  14       2-31 * * root jobs.py      make drill_rules_month_summary | /usr/bin/logger -i -t drill_rules_month_summary',
                        '0  14       1    * * root jobs.py -r 2 make drill_rules_month_summary | /usr/bin/logger -i -t drill_rules_month_summary',
                        '0  7,20    *    * * root jobs.py      make bundles_deployment | /usr/bin/logger -i -t bundles_deployment',
                        '0  12       1    * * root jobs.py -r 2 make bundles_deployment | /usr/bin/logger -i -t bundles_deployment'],

    #Absolute last resort, run bundles once a day and no where near peak
    'super_limited_bundles': [
                        '0  9        2-31 * * root jobs.py      make bundle2_backbone1_summary | /usr/bin/logger -i -t bundle2_backbone1_summary',
                        '0  9        1    * * root jobs.py -r 2 make bundle2_backbone1_summary | /usr/bin/logger -i -t bundle2_backbone1_summary',
                        '0  6        2-31 * * root jobs.py      make bundle2_h5backbone_summary| /usr/bin/logger -i -t bundle2_h5backbone_summary',
                        '0  6        1    * * root jobs.py -r 2 make bundle2_h5backbone_summary| /usr/bin/logger -i -t bundle2_h5backbone_summary',
                        '0  17       *    * * root jobs.py      make drill_rules_day_drill | /usr/bin/logger -i -t drill_rules_day_drill',
                        '0  5        *    * * root jobs.py -r 2 make drill_rules_day_drill | /usr/bin/logger -i -t drill_rules_day_drill',
                        '0  18       *    * * root jobs.py      make drill_rules_day_summary | /usr/bin/logger -i -t drill_rules_day_summary',
                        '0  7        *    * * root jobs.py -r 2 make drill_rules_day_summary | /usr/bin/logger -i -t drill_rules_day_summary',
                        '0  11       1    * * root jobs.py -r 2 make drill_rules_month_drill | /usr/bin/logger -i -t drill_rules_month_drill',
                        '0  11       2-31 * * root jobs.py      make drill_rules_month_drill | /usr/bin/logger -i -t drill_rules_month_drill',
                        '0  14       1    * * root jobs.py -r 2 make drill_rules_month_summary | /usr/bin/logger -i -t drill_rules_month_summary',
                        '0  14       2-31 * * root jobs.py      make drill_rules_month_summary | /usr/bin/logger -i -t drill_rules_month_summary',
                        '0  16       2-31 * * root jobs.py       make bundles_deployment | /usr/bin/logger -i -t bundles_deployment',
                        '0  16       1    * * root jobs.py -r 2 make bundles_deployment | /usr/bin/logger -i -t bundles_deployment'],

    # no-peak bundles (Americas)
    'no_peak_bundles': ['0  12-21/3 *    * * root jobs.py      make bundle2_backbone1_summary | /usr/bin/logger -i -t bundle2_backbone1_summary',
                        '0  9       2-31 * * root jobs.py      make bundle2_backbone1_summary | /usr/bin/logger -i -t bundle2_backbone1_summary',
                        '0  9       1    * * root jobs.py -r 2 make bundle2_backbone1_summary | /usr/bin/logger -i -t bundle2_backbone1_summary',
                        '0  12-21/3 *    * * root jobs.py      make bundle2_h5backbone_summary| /usr/bin/logger -i -t bundle2_h5backbone_summary',
                        '0  9       2-31 * * root jobs.py      make bundle2_h5backbone_summary| /usr/bin/logger -i -t bundle2_h5backbone_summary',
                        '0  9       1    * * root jobs.py -r 2 make bundle2_h5backbone_summary| /usr/bin/logger -i -t bundle2_h5backbone_summary',
                        '0  10-22/4 *    * * root jobs.py      make drill_rules_day_drill | /usr/bin/logger -i -t drill_rules_day_drill',
                        '0  9       *    * * root jobs.py -r 2 make drill_rules_day_drill | /usr/bin/logger -i -t drill_rules_day_drill',
                        '0  11-23/4 *    * * root jobs.py      make drill_rules_day_summary | /usr/bin/logger -i -t drill_rules_day_summary',
                        '0  10      *    * * root jobs.py -r 2 make drill_rules_day_summary | /usr/bin/logger -i -t drill_rules_day_summary',
                        '0  16-20/4 *    * * root jobs.py      make drill_rules_month_drill | /usr/bin/logger -i -t drill_rules_month_drill',
                        '0  12      2-31 * * root jobs.py      make drill_rules_month_drill | /usr/bin/logger -i -t drill_rules_month_drill',
                        '0  12      1    * * root jobs.py -r 2 make drill_rules_month_drill | /usr/bin/logger -i -t drill_rules_month_drill',
                        '0  18-22/4 *    * * root jobs.py      make drill_rules_month_summary | /usr/bin/logger -i -t drill_rules_month_summary',
                        '0  14      2-31 * * root jobs.py      make drill_rules_month_summary | /usr/bin/logger -i -t drill_rules_month_summary',
                        '0  14      1    * * root jobs.py -r 2 make drill_rules_month_summary | /usr/bin/logger -i -t drill_rules_month_summary',
                        '0  16-20/4 *    * * root jobs.py      make bundles_deployment | /usr/bin/logger -i -t bundles_deployment',
                        '0  12      2-31 * * root jobs.py      make bundles_deployment | /usr/bin/logger -i -t bundles_deployment',
                        '0  12      1    * * root jobs.py -r 2 make bundles_deployment | /usr/bin/logger -i -t bundles_deployment'],

    'query_index':              '15 */2 * * * root query_index.py | /usr/bin/logger -i -t query_index',

    # run bundles at console, serially, starting at 0000, daily
    'bundles_serial':           '0 0 * * * root bundles_serial.sh | /usr/bin/logger -i -t bundles_serial.sh',

    # flow_op daily counts, right before midnight
    'sub_count_day':            '58 23 * * * root jobs.py make sub_count_day | /usr/bin/logger -i -t sub_count_day',

    'cube_from_log':            '33 * * * * root cube_from_log.py -f -m 2>&1 | logger -i -t cube_from_log',
    'snmp':                     '5 * * * * root snmp.py 2>&1| logger -i -t snmp.py',
    'upload_local_snmp':        '30 * * * * root upload_local_snmp.py 2>&1 | logger -i -t upload_local_snmp',
    'dimension_merge':          '*/10 * * * * root dimensions_merge.py 2>&1 | logger -i -t dimensions_merge',
    'dimension_index':          '45 * * * * root build_dimensions_index.py 2>&1 | logger -i -t build_dimensions_index.py',
    'build_status_summary':     '*/5 * * * * root build_status_summary.py 2>&1 | logger -i -t build_status_summary.py',
    'routemap':                 '20 3,12,20 * * * root jobs.py make routemap 2>&1 | logger -i -t jobs.py',
    'no_peak_routemap':         '20 10,16,22 * * * root jobs.py make routemap 2>&1 | logger -i -t jobs.py',
    'run_connector':            '40 * * * * root run_connector.py 2>&1 | logger -i -t run_connector.py',
    'rocketfuel_trace':         '40 23 * * * root jobs.py make rocketfuel_trace 2>&1 | logger -i -t rocketfuel_trace.py',
    'supplychain':              '*/5 * * * * root jobs.py -r 10 make supplychain 2>&1 | logger -i -t supplychain.py',
    'benchmark':                '30 5 * * * root benchmark.py 2>&1 | logger -i -t benchmark.py',
    'restart_ui':               '30 6 * * * root restart_ui.sh 2>&1 | logger -i -t restart_ui',
    'update_build':             '50 11 * * 1-5 root update_build.py 2>&1 | logger -i -t update_build',
    'check_files':              '20 * * * * root check_files.py 2>&1 | logger -i -t check_files',
    'subscribers':              '50 12 * * * root subscribers.py 2>&1 | logger -i -t subscribers',
    'routers':                  '*/6 * * * * root routers.py 2>&1 | logger -i -t routers.py',
    'interfaces':               '*/8 * * * * root interfaces.py 2>&1 | logger -i -t interfaces.py',
    'logo_sync':                '20 4 * * * root logo_sync.py 2>&1 | logger -i -t logo_sync',
    'localmap':                 '10 * * * * root localmap.py 2>&1 | logger -i -t localmap.py',
    'notice_sync':              '*/5 * * * * root notice_sync.py 2>&1 | logger -i -t notice_sync.py',
    'configure_proxy':          '*/1 * * * * root configure_proxy.py 2>&1 | logger -i -t configure_proxy.py',

    # Mapper only
    'genome_maps':              '0 * * * * root genome_maps.py -D 2>&1 | logger -i -t genome_maps.py',
    'asndb_update':             '36 11 * * * root asndb_update.py 2>&1 | logger -i -t asndb_update.py',


    # Status Only
    'email_alerts':             '3,33 * * * * root email_alerts.py --cache-update 2>&1 | logger -t email_alerts.py',
    'pull_heartbeats':          '*/5 * * * * root pull_heartbeats.py -r 12 2>&1 | logger -t pull_heartbeats.py',
    'check_deployment_ui':      '*/5 * * * * root check_deployment_ui.py -p 2>&1 | logger -t check_deployment_ui.py',
    'cube_heartbeats':          '*/5 * * * * root jobs.py -r 12 make cube_heartbeats | logger -t jobs.py',

    # Least-worst solution. Avoid installing scripts named "comcast_*" into
    # sbin on all deployments; Removing 'comcast_' risks name conflicts;
    # connectors/scripts dir isn't updated by scons... run out of git dir
    'comcast_apps':             '37 6 * * * root /home/support/pipedream/connectors/comcast/comcast_itrc.py -qbg 2>&1 | logger -t comcast_itrc.py',
    'comcast_dc_h5flow_scan':   '11 * * * * root /home/support/pipedream/connectors/comcast/comcast_datacenters.py -ufp 2>&1 | logger -t comcast_datacenters.py',
    'comcast_dc_dims':          '32 7 * * * root /home/support/pipedream/connectors/comcast/comcast_datacenters.py -cta0p 2>&1 | logger -t comcast_datacenters.py',

    # vss only
    'vss-master-fetch':         '10-59/20 * * * * root vss-master-fetch.py 2>&1 | logger -i -t vss-master-fetch.py',
    'vss-make-vss-cubes':       '*/20 * * * *     root vss-make-vss-cubes.py 2>&1 | logger -i -t vss-make-vss-cubes.py',
    'vss_air_miles':       '*/19 * * * *     root vss_air_miles.py 2>&1 | logger -i -t vss_air_miles.py',

    # vss warming
    # run on UI only
    'warm-vss-cache':           '30 */3 * * *     root warm_vss_cache.py 2>&1 | logger -i -t warm_vss_cache.py',
    # support older vss
    'warm-vss-cache-month':           '30 */3 * * *     root warm_vss_cache.py --month-end 2>&1 | logger -i -t warm_vss_cache.py',
    'impala_compaction':          '0 * * * * root jobs.py -r 2 make impala_compaction 2>&1 | logger -t jobs.py',
<<<<<<< HEAD
    'impala_inserts':               '*/5 * * * * root jobs.py -r 5 make impala_inserts 2>&1 | logger -t jobs.py',
=======
    'impala_inserts':               '*/5 * * * * root jobs.py -r 50 make impala_inserts 2>&1 | logger -t jobs.py'
>>>>>>> origin/master
}

def override_ranges(cron_jobs, overrides):

    assert isinstance(overrides, dict)

    range_rgx = ' (-r [0-9]+) '
    for job,r in overrides.iteritems():
        job_str = cron_jobs.get(job)
        if not job_str:
            deepy.log.warn('Could-not-find-job-for-override-{}'.format(job))
            continue
        m = re.search(range_rgx, job_str)
        if not m:
            deepy.log.warn('Override-configured-for-{}-no-range-arg-found'.format(job))
            deepy.log.warn(job_str)
        else:
            #Make sure r is a valid int
            try:
                r = int(r)
            except ValueError:
                deepy.log.warn("Invalid-range-arg-value-{}-skipping".format(r))
                continue
            except TypeError:
                deepy.log.warn("Invalid-range-arg-type-{}-skipping".format(r))
                continue

            current_range = m.group(1)
            new_range = '-r {}'.format(r)
            deepy.log.debug('replacing-range-for-job-{}-replacing/{}/{}'.format(job, current_range, new_range))
            cron_jobs[job] = job_str.replace(current_range, new_range)

    return cron_jobs

def read_file_buffer(filename):
    '''
    Read and return the buffer from file
    '''
    try:
        if os.path.exists(filename):
            return open(filename).read()
    except IOError:
        log.exception("could-not-read-file %s: %s" % (filename))
    return None

def write_file_buffer(filename, buf):
    '''
    Attempt to write buffer to file
    '''
    try:
        return open(filename, 'w').write(buf)
    except Exception as e:
        log.warn("could-not-write-file %s: %s" % (filename, str(e)))
        return None

def update_file(filename, buf_new, permission=0644):
    '''
    Check if buffer has changed for file and update if so
    '''
    buf_old = read_file_buffer(filename)

    if buf_new != buf_old:
        log.debug("config-file-change %s" % (filename))
        write_file_buffer(filename, buf_new)
        os.chmod(filename, permission)

    return buf_new != buf_old

def get_cron_jobs(save_cron_jobs= None):
    '''
    Implement logic for cron jobs that should run on this VM and return them
    '''
    vm_config = deepy.cfg.vm_config

    # Get set of cron jobs for VM
    cron_jobs = None
    extra_cron_jobs = set()
    disabled_cron_jobs = set()
    daemons = []

    if vm_config is not None:
        if 'cron_jobs' in vm_config:
            cron_jobs = set(vm_config.get('cron_jobs'))

        extra_cron_jobs = set(vm_config.get('extra_cron_jobs', []))
        disabled_cron_jobs = set(vm_config.get('disabled_cron_jobs', []))
        daemons = vm_config.get('daemons', [])
        roles = vm_config.get('roles', [])

    #Secondary vms should only run rsync and the vm stat
    if "ha_rsync" in cron_jobs_config:
        cron_jobs = set(["ha_rsync"])
        if save_cron_jobs:
            cron_jobs |= set(save_cron_jobs)

    # Default / "Leader", Non-leader VMs should have cron_jobs of None
    if cron_jobs is None:
        cron_jobs = set([
            "build_h5bgp",
            "build_h5dns",
            "build_h5flow",
            "classify_h5flow",
            'build_h5flow_long',
            'build_h5dns_long',
            'classify_h5flow_long',
            'build_h5cubes_5min',
            'build_h5cubes_5min_long',
            "build_h5cubes_hour",
            "cube_from_hb",
            "mine",
            "searchips",
            "big_cube_hour",
            "cubes_hour",
            "cubes_day",
            "cubes_month",
            "cubes_5min",
            "run_connector",
            "build_status_summary",

            'drill_rules_day_summary',
            'drill_rules_day_drill',
            'query_index',
            'drill_rules_month_summary',
            'drill_rules_month_drill',
            'bundle2_h5backbone_summary',
            'bundle2_backbone1_summary',
            'bundles_deployment',

            "dimension_merge",
            "dimension_index",
            "routemap",
            "rocketfuel_trace",
            "subscribers",
            "logo_sync",
            "update_build",
            "check_files",
            "snmp",
            "cube_snmp",
            "routers",
            "interfaces",

            "localmap",
            "notice_sync",
            "configure_proxy",
            "impala_compaction",
            "impala_inserts",
        ])

    if deepy.util.is_configured_on_vm('bird') or deepy.util.is_configured_on_vm('bird6'):
        cron_jobs.add("upload_localbgp")
        if deepy.util.is_configured_on_vm('bird6'):
            cron_jobs.add("dump_localbgp6")
        if deepy.util.is_configured_on_vm('bird'):
            cron_jobs.add("dump_localbgp")

        # setup path where bird has its control socket
        bird_control_path = '/usr/local/var/run'
        if not os.path.exists(bird_control_path):
            os.makedirs(bird_control_path)

    if deepy.util.is_configured_on_vm('bgpd'):
        cron_jobs.add("upload_localbgp")

    if deepy.util.is_configured_on_vm('flowd'):
        cron_jobs.add("router_stat")

    # Always run update_build and check files
    cron_jobs |= set(['update_build', 'check_files'])
    #If hdfs is on then add the prune_files hdfs to the cron jobs
    store_type = deepy.util.vm_or_slice_config_get("store_type")
    if store_type and store_type == "hdfs":
        cron_jobs |= set(["prune_files_hdfs"])

    if (deepy.util.vm_or_slice_config_get("archive_flow") or deepy.util.vm_or_slice_config_get("archive_dnsflow")) and 'flowd' in daemons:
        cron_jobs.add('save_flows_remote')

    # UI VM needs to always have these cron jobs
    for d in daemons:
        #There are different versions of home.py (home.py, home.py-no-celery) that we want to run
        #   these cron jobs on so we check if home.py is in the daemon string
        if 'home.py' == d or 'home.py-no-celery' == d:
            cron_jobs |= set(['cube_from_log'])
            cron_jobs.add('reload_bundles_cache')
            cron_jobs.add('reload_files_cache')
            cron_jobs.add('reload_dim_db_cache')
            cron_jobs.add('save_cache')

    # VM is a database namenode, so it needs to have all database cron jobs
    if 'hdfs.namenode' in roles:
        cron_jobs.add('asndb_sync')

    cron_jobs |= extra_cron_jobs
    cron_jobs -= disabled_cron_jobs

    return cron_jobs

def parse_slice_neighbors(neighbors):
    v6s = []
    buf = ""
    for neighbor in neighbors:
        if neighbor.get('ip'):
            ip = neighbor["ip"]
        else:
            log.warn('bgpd-missing-options ip')
            continue

        if neighbor.get('remote-as'):
            buf += "\tneighbor %s remote-as %d\n" % (ip, neighbor['remote-as'])
        else:
            log.warn('bgpd-missing-options remote-as')
            continue

        if neighbor.get('ebgp-multihop'):
            buf += "\tneighbor %s ebgp-multihop %d\n" % (ip, neighbor['ebgp-multihop'])
        elif neighbor.get('multihop'):
            buf += '\tneighbor %s ebgp-multihop 255\n' % (ip)

        if neighbor.get('password'):
            buf += "\tneighbor %s password %s\n" % (ip, neighbor['password'])
        if neighbor.get('passive'):
            buf += "\tneighbor %s passive\n" % (ip)
        if neighbor.get('description'):
            # Limit to 80 chars
            buf += "\tneighbor %s description %s\n" % (ip, neighbor['description'][:80])
        if neighbor.get('local-as'):
            buf += "\tneighbor %s local-as %d no-prepend\n" % (ip, neighbor['local-as'])
        if neighbor.get('v6'):
            v6s.append(ip)

    return buf, v6s

def parse_dim_neighbors(neighbors):
    v6s = []
    buf = ""
    for neighbor in neighbors:
        if neighbor.get('neighbor_ip'):
            ip = neighbor['neighbor_ip']
        else:
            log.warn('bgpd-missing-options ip')
            continue

        if neighbor.get('remote_asn'):
            buf += "\tneighbor %s remote-as %d\n" % (ip, neighbor['remote_asn'])
        else:
            log.warn('bgpd-missing-options remote_asn')
            continue

        if neighbor.get('ebgp_multihop'):
            buf += '\tneighbor %s ebgp-multihop %d\n' % (ip, neighbor['ebgp_multihop'])
        if neighbor.get('neighbor_password'):
            buf += "\tneighbor %s password %s\n" % (ip, neighbor['neighbor_password'])
        if neighbor.get('local_asn'):
            buf += "\tneighbor %s local-as %d\n" % (ip, neighbor['local_asn'])

        flags = set([f.strip() for f in neighbor.get('extra_bgp_flags', '').split(',')])
        if 'passive' in flags:
            buf += "\tneighbor %s passive\n" % (ip)
        if 'ebgp-multihop' in flags and not neighbor.get('ebgp_multihop'):
            buf += '\tneighbor %s ebgp-multihop 255\n' % (ip)
        if 'ipv6' in flags:
            v6s.append(ip)

    return buf, v6s

def get_bgpd_buffer(asn, bid, neighbors, secs, input_type="dim"):
    buf = "password default123\n"
    buf += "router bgp %d\n" % asn

    if bid:
        buf += "\tbgp router-id %s\n" % bid

    if input_type == "slice":
        neighbor_buf, v6s = parse_slice_neighbors(neighbors)
    elif input_type == "dim":
        neighbor_buf, v6s = parse_dim_neighbors(neighbors)

    buf += neighbor_buf

    if v6s:
        buf += 'address-family ipv6\n'
        for ip in v6s:
            buf += '\tneighbor %s activate\n' % (ip)
        buf += '\texit-address-family\n'

    mrt_file = os.path.join(deepy.cfg.data_tmp, "bgpdump_%Y-%m-%d-%H-%M.mrt")
    buf += "dump bgp routes-mrt %s %d\n" % (mrt_file, secs)
    buf += "log syslog\n"
    return buf

def get_bird_buffer(ipversion, asn, bid, neighbors, secs, input_type="dim"):
    buf  = "log stderr all;\n"
    buf += "\n";

    buf += "router id %s;\n" % bid
    buf += "\n";

    buf += "protocol kernel {\n"
    buf += "   import none;\n"
    buf += "   export none;\n"
    buf += "}\n";
    buf += "\n";

    if ipversion == 'v6':
        buf += "listen bgp v6only;\n"

    for neighbor in neighbors:

        local_ip = None

        if neighbor.get('neighbor_ip'):
            ip = neighbor['neighbor_ip']
        else:
            log.warn('bird-missing-options neighbor_ip')
            continue

        if ipversion == "v6" and deepy.util.ip_family(ip) != 'ipv6':
            continue

        if ipversion == "v4" and deepy.util.ip_family(ip) != 'ipv4':
            continue

        if not neighbor.get('remote_asn'):
            log.warn('bird-missing-options remote_asn')
            continue

        if neighbor.get('local_asn'):
            local_asn = neighbor.get('local_asn')
        else:
            local_asn = asn

        if neighbor.get('local_ip'):
            local_ip = neighbor.get('local_ip')

        #source address 198.51.100.14;   # What local address we use for the TCP connection
        #password "secret";      # Password used for MD5 authentication

        #Don't include ip in v6 names. Output gets too big and bird6 will croak with a syntax error
        if ipversion == "v6":
            bgp_name = ""
        else:
            bgp_name = "bgp_%s" % (ip.replace('.', '_'))

        buf += "protocol bgp %s {\n" % (bgp_name)
        buf += "    import all;\n"
        buf += "    export filter { reject; };\n"
        buf += "    local as %s;\n" % local_asn
        buf += "    multihop;\n"
        if neighbor.get('neighbor_password'):
            buf += "    password \"%s\";\n" % (neighbor['neighbor_password'])
        if local_ip:
            buf += "    source address %s;\n" % local_ip
        buf += "    neighbor %s as %s;\n" % (ip,  neighbor['remote_asn'])
        buf += "}\n"
        buf += "\n"

    return buf

def get_slice_bgpd_conf():
    daemons = deepy.cfg.vm_config.get("daemons", [])
    if "bgpd" not in daemons:
        return None

    if 'bgpd_options' not in deepy.cfg.vm_config:
        return None

    bgpd_options = deepy.cfg.vm_config.get("bgpd_options")
    if not bgpd_options.get('update', True):
        log.debug('bgpd-updating-disabled')
        return None

    asn = bgpd_options.get('asn')
    if not asn:
        log.debug('bgpd-config-missing-asn')
        return None

    neighbors = bgpd_options.get('neighbors', [])
    router_id = bgpd_options.get('router-id')
    update_interval_secs = bgpd_options.get('update_interval_secs', 3600)

    bgpd_buf = get_bgpd_buffer(asn, router_id, neighbors, update_interval_secs, "slice")
    return bgpd_buf

def get_dim_bgpd_conf():
    info = deepy.dimensions.DimensionsIdx().get_dim_by_name('bgp_routers')
    if info is None:
        return None

    id_str = str(info['id'])
    local_path = os.path.join(deepy.cfg.dimensions_dir, info['file'])
    routers = deepy.dimensions.DimensionsDBFrag(local_path).get_all_positions(id_str)

    if len(routers) == 0:
        return None

    bgp_info = {}
    for rid, r in sorted(routers.items()):
        if r.get('convert_to', True) is None:
            continue
        router_info = r.get('attributes', {}).get('computed',{}).get('bgp_router',{})
        local_ip = router_info.get('local_ip')
        if local_ip is None:
            continue
        neighbors = bgp_info.setdefault(local_ip, [])
        neighbors.append(router_info)

    if len(bgp_info) != 1:
        # XXX support multi
        log.warn("Multiple local BGP IPs not supported.")
        return None
    local_asns = []
    for r in bgp_info.values()[0]:
        local_asn = r.get('local_asn')
        #Only unique
        if local_asn and local_asn not in local_asns:
            local_asns.append(local_asn)

    #if not local_asn available assume iBGP and use remote_asn as local
    if not local_asn:
        for r in bgp_info.values()[0]:
            remote_asn = r.get('remote_asn')
            if remote_asn:
                local_asns.append(remote_asn)

    if len(local_asns) != 1:
        # XXX support multi
        log.warn("Multiple local BGP ASNs not supported.")
        return None

    bgpd_buf = get_bgpd_buffer(local_asns[0], local_ip, bgp_info.values()[0], 3600, "dim")
    return bgpd_buf

def get_dim_bird_conf():
    info = deepy.dimensions.DimensionsIdx().get_dim_by_name('bgp_routers')
    if info is None:
        return None

    default_local_ip = None
    default_local_asn = None
    id_str = str(info['id'])
    local_path = os.path.join(deepy.cfg.dimensions_dir, info['file'])
    routers = deepy.dimensions.DimensionsDBFrag(local_path).get_all_positions(id_str)
    neighbors = []

    for rid, r in sorted(routers.items()):
        if r.get('convert_to', True) is None:
            continue
        router_info = r.get('attributes', {}).get('computed',{}).get('bgp_router',{})

        local_ip = router_info.get('local_ip')

        # set defaults
        if router_info.get('local_ip', None):
            default_local_asn = router_info.get('local_asn')
        if local_ip and deepy.util.ip_family(local_ip) == 'ipv4':
            default_local_ip = local_ip

        #neighbors = bgp_info.setdefault(local_ip, [])
        neighbors.append(router_info)


    bird_buf_v4 = get_bird_buffer("v4", default_local_asn, default_local_ip, neighbors, 3600, "dim")
    bird_buf_v6 = get_bird_buffer("v6", default_local_asn, default_local_ip, neighbors, 3600, "dim")

    return  bird_buf_v4, bird_buf_v6

def update_s3cmd_cfg(aws_access_key_id=None, aws_secret_access_key=None,
        cfg_file=deepy.cfg.s3cmd_cfg_file):
    '''
    Update s3cmd configuration file
    '''
    if aws_access_key_id is None:
        aws_access_key_id = deepy.cfg.slice_config.get('credentials', [None, None])[0]

    if aws_secret_access_key is None:
        aws_secret_access_key = deepy.cfg.slice_config.get('credentials', [None, None])[1]

    if not (aws_access_key_id and aws_secret_access_key):
        log.warn("no-aws-key-found-boto-cfg")
        return

    data = {
        "default": {
            "access_key": aws_access_key_id,
            "secret_key": aws_secret_access_key,
            "bucket_location": "US",
            "cloudfront_host": "cloudfront.amazonaws.com",
            "cloudfront_resource": "/2010-07-15/distribution",
            "default_mime_type": "binary/octet-stream",
            "delete_removed": "False",
            "dry_run": "False",
            "encoding": "US-ASCII",
            "encrypt": "False",
            "follow_symlinks": "False",
            "force": "False",
            "get_continue": "False",
            "gpg_command": "None",
            "gpg_decrypt": "%(gpg_command)s -d --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)s",
            "gpg_encrypt": "%(gpg_command)s -c --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)s",
            "gpg_passphrase": "poo",
            "guess_mime_type": "True",
            "host_base": "s3.amazonaws.com",
            "host_bucket": "%(bucket)s.s3.amazonaws.com",
            "human_readable_sizes": "False",
            "list_md5": "False",
            "log_target_prefix": "",
            "preserve_attrs": "True",
            "progress_meter": "True",
            "recursive": "False",
            "recv_chunk": "4096",
            "reduced_redundancy": "False",
            "send_chunk": "4096",
            "simpledb_host": "sdb.amazonaws.com",
            "skip_existing": "False",
            "socket_timeout": "300",
            "urlencoding_mode": "normal",
            "use_https": "True",
            "verbosity": "WARNING"
        }
    }

    try:
        i = deepy.ini.Ini(filename=cfg_file)
        i.update(data)
        i.write(cfg_file)
    except ConfigParser.MissingSectionHeaderError:
        log.debug("force-writing {}".format(cfg_file))
        i = deepy.ini.Ini()
        i.update(data)
        i.force_write(cfg_file)

def update_boto_cfg(http_socket_timeout='30', aws_access_key_id=None,
        aws_secret_access_key=None, cfg_file=deepy.cfg.boto_cfg_file):
    '''
    Update Amazon boto configuration file
    '''
    if aws_access_key_id is None:
        aws_access_key_id = deepy.cfg.slice_config.get('credentials', [None, None])[0]

    if aws_secret_access_key is None:
        aws_secret_access_key = deepy.cfg.slice_config.get('credentials', [None, None])[1]

    if not (aws_access_key_id and aws_secret_access_key):
        log.warn("no-aws-key-found-boto-cfg")
        return

    data = {
        "Credentials": {
            "aws_access_key_id": aws_access_key_id,
            "aws_secret_access_key": aws_secret_access_key
        },
        "Boto": {
            "http_socket_timeout": http_socket_timeout
        }
    }

    try:
        i = deepy.ini.Ini(filename=cfg_file)
        i.update(data)
        i.write(cfg_file)
    except ConfigParser.MissingSectionHeaderError:
        log.debug("force-writing {}".format(cfg_file))
        i = deepy.ini.Ini()
        i.update(data)
        i.force_write(cfg_file)

def update_bgpd_cfg(cfg_file=deepy.cfg.bgpd_cfg_file):
    '''
    Update BGP configuration file
    '''

    bgpd_options = deepy.cfg.vm_config.get("bgpd_options", {})
    if not bgpd_options.get('update', True):
        log.debug('bgpd-updating-disabled')
        return

    # For backwards compatibility.  Should probably update the connector
    # to translate the slice config into the dim config.
    buf = get_dim_bgpd_conf()
    if buf is None:
        buf = get_slice_bgpd_conf()

    if buf is None:
        buf = ''

    changed = update_file(cfg_file, buf)

    if changed:
        pid_file = os.path.join(deepy.cfg.pids_dir, "bgpd.pid")
        pid = read_file_buffer(pid_file)
        if pid:
            log.warn("BGP Config Changed! sighup-bgpd-pid %s" % pid)
            #os.kill(int(pid), signal.SIGHUP)

def update_bird_cfg():
    '''
    Update BGP configuration file
    '''

    bgpd_options = deepy.cfg.vm_config.get("bgpd_options", {})
    if not bgpd_options.get('update', True):
        log.debug('bgpd-updating-disabled')
        return

    buf_v4, buf_v6 = get_dim_bird_conf()

    # V4
    if buf_v4 is None:
        buf_v4 = ''
    changed = update_file(deepy.cfg.bird_cfg_file, buf_v4)
    if changed:
        pid_file = os.path.join(deepy.cfg.pids_dir, "bird.pid")
        pid = read_file_buffer(pid_file)
        if pid:
            log.warn("BGP Config Changed! sighup-bird-pid %s" % pid)
            os.kill(int(pid), signal.SIGHUP)


    # V6
    if buf_v6 is None:
        buf_v6 = ''
    changed = update_file(deepy.cfg.bird6_cfg_file, buf_v6)
    if changed:
        pid_file = os.path.join(deepy.cfg.pids_dir, "bird6.pid")
        pid = read_file_buffer(pid_file)
        if pid:
            log.warn("BGP Config Changed! sighup-bird6-pid %s" % pid)
            os.kill(int(pid), signal.SIGHUP)

def update_cron_cfg(cfg_file=deepy.cfg.cron_file, save_cron_jobs=None, dry_run=False):
    '''
    Update deepfield cron jobs on this vm
    '''
    buf = DEFAULT_CRON_CFG

    buf += "#\n# VM specific environment variables\n"
    cron_env_vars = deepy.cfg.vm_config.get('cron_env_vars', [])
    for var_str in cron_env_vars:
        buf += var_str + "\n"

    cron_jobs_commands = cron_jobs_config

    #Apply slice overrides
    overrides = deepy.util.recursive_get(deepy.cfg.slice_config, "restart_daemons", "range_overrides")
    if overrides:
        cron_jobs_commands = override_ranges(cron_jobs_commands, overrides)

    buf += "#\n# VM specific cron jobs\n"
    cron_jobs = get_cron_jobs(save_cron_jobs=save_cron_jobs)
    for cron_job in cron_jobs:
        if cron_job not in cron_jobs_commands.keys():
            log.warn("missing-cron-job-definition %s" % (cron_job))
            continue

        commands = cron_jobs_commands[cron_job]
        if type(commands) != list:
            buf += "%s\n" % (commands)
        else:
            buf += "%s\n" % ('\n'.join(commands))

    deployment = deepy.provision.get_deployment(deepy.cfg.deployment_id)
    if deployment.is_aws():
        buf += "@reboot root aws_add_swap.py\n"
        buf += "*/10 * * * * root aws_add_swap.py\n"

    if dry_run:
        return buf
    else:
        return update_file(cfg_file, buf)

def form_rsync_cron_job(store_ip, user="support", key=deepy.cfg.pdrops_key_file, timeout = 5):
    job = '*/10 * * * * root pre-production.py -s -H %s 2>&1 | logger -i -t pre-production.py' % (store_ip)
    return job

def update_snmpd():
    use_snmpd = 'snmpd' in deepy.cfg.vm_config.get('daemons', [])

    snmpd = read_file_buffer(deepy.cfg.snmpd_file)
    snmpd_conf = read_file_buffer(deepy.cfg.snmpd_conf_file)

    if use_snmpd:
        ip_passes = deepy.cfg.vm_config.get('snmpd', [])
        rocommunity = ['rocommunity public 127.0.0.1/32']
        for ip_pass in ip_passes:
            ip = ip_pass['ip']
            secret = ip_pass['secret']
            s = 'rocommunity %s %s' % (secret, ip)
            rocommunity.append(s)
        snmpd_conf = snmpd_conf.format(rocommunity='\n'.join(rocommunity))
        snmpd = snmpd.format(snmpdrun='yes')
    else:
        snmpd_conf = snmpd_conf.format(rocommunity='')
        try:
            snmpd = snmpd.format(snmpdrun='no')
        except:
            return

    local_path = os.path.join(deepy.cfg.defines_dir, 'snmpd.conf')
    write_file_buffer(local_path, snmpd_conf)

    local_path = os.path.join(deepy.cfg.defines_dir, 'snmpd')
    write_file_buffer(local_path, snmpd)

def run_daemon(cmd):
    try:
        process = subprocess.Popen([cmd], shell=True)
        log.info('daemon-started pid={:d} cmd={}'.format(process.pid, cmd))
        return process
    except Exception as e:
        log.warning('daemon-start-failed {} {}'.format(cmd, str(e)))

def restart_daemons():
    vm = deepy.cfg.vm_config

    # Default
    if not vm:
        log.warn("no-vm-config-in-slice")
        vm['daemons'] = ["home.py", "redirect.py", "flowd"]

    #Only need special handling if this is the secondary vm. If its the primary continue on to the normal logic
    carp_ip = deepy.util.vm_or_slice_config_get("carp_ip")
    save_cron_jobs = None
    #Special case for telarg. we want a special worker minion
    carp_minion = deepy.util.vm_or_slice_config_get("carp_minion")
    if carp_ip and not carp_minion:
        if not deepy.metrics.carp_master(carp_ip):
           #Remove daemons
           global daemon_config
           global cron_jobs_config

           #Kill off any running daemons
           for d in daemon_config:
               #Since home has multiple processes we need a special command
               if d == "home.py":
                   subprocess.call(['sudo pkill -f \'python /var/local/pipedream/ui/home.py\''], shell=True)
               else:
                   deepy.util.kill_daemon(d)

           daemon_config = {}

           #Save the vm stat cube
           save_cron_jobs = ["cube_from_hb",'update_build', 'check_files']
           new_cron_jobs = {}
           for c in cron_jobs_config:
               if c in save_cron_jobs:
                   new_cron_jobs[c] = cron_jobs_config[c]
           cron_jobs_config = new_cron_jobs

           #Add the rsync cron job
           cron_jobs_config["ha_rsync"] = form_rsync_cron_job(carp_ip)

    # Set up for a particular deployment, maybe parameterize later

    daemon_override_cmds = vm.get('daemon_cmds', {})
    daemon_config.update(daemon_override_cmds)

    # Add entries
    for daemon_name, daemon_cmd in daemon_config.iteritems():
        if daemon_name not in vm.get('daemons', []):
            continue

     #If nginx is running then redirect/home will be taken care of through supervisor. For backwards compatability just skip starting these
        ignore_daemons = ['home.py', 'redirect.py', 'home.py-no-celery']
        if deepy.util.vm_has_role('nginx') and daemon_name in ignore_daemons:
             continue

        if deepy.util.daemon_is_running(daemon_name):
            log.debug('daemon-is-running %s' % daemon_name)
        else:
            log.debug('starting-daemon %s: %s' % (daemon_name, daemon_cmd))
            run_daemon(daemon_cmd)

    update_snmpd()
    update_cron_cfg(save_cron_jobs=save_cron_jobs)
    update_boto_cfg()
    update_s3cmd_cfg()

    if 'bgpd' in vm.get('daemons', []):
        update_bgpd_cfg()
        log.debug('reniceing-process quagga')
        deepy.util.renice_process('quagga', -19)

    if 'bird' in vm.get('daemons', []):
        update_bird_cfg()
        log.debug('reniceing-process bird')
        deepy.util.renice_process('bird', -19)

    if 'bird6' in vm.get('daemons', []):
        update_bird_cfg()
        log.debug('reniceing-process bird6')
        deepy.util.renice_process('bird6', -19)


