# Global config

import sys
import os
import json

import deepy.url

'''
Clear umask so files are created with full permissions. Done here as
a central place that most scripts call. Once this is done, the builtin
open() will create files with mode 666 by default (seems to be
undocumented) and os.makedirs() will use 777, which is what we want in
both cases.
'''
os.umask(0)

DEFAULT_DB_PASS = 'PipedreamPassword!' # set in salt/mysql

class Config:
    def __init__(self, pipedream_deployment_override=None,
                pipedream_hood_override=None, store=None,
                vm_uuid_override=None):
        '''
        The general idea is the take in configuration information from various
        locations and set it globally in this file so we can import this file in
        other places and access it like class variables
            i.e. import deepy.cfg; deepy.cfg.slice_config

        To do this we want to follow these general steps:
            -- Load various system information based on user input and
            environment input that will be used to incrementally build other
            variables
            -- Load directory information
            -- Load file information that's built up from the above steps
            -- Load various files into a cache that we will presumably be
            regularly using
            -- Load in VM info (non-deployment wide system information)
            -- Set any other miscellaneous pieces of information that may be
            used  elsewhere or we may want later

        ** WARNING:
            1. Since many variables defined later depend on earlier definitions,
            the order in  which these operations happen matters
        '''

        # Some constants for the ui
        self.DEFAULT_UI_PROCESSES = 8
        self.UI_BASE_PORT = 8080
        self.BASE_WS_PORT = 8118


        # Needs to be set
        self.isolated_deployment = False

        self.pipedream_hood_default = '/'
        self.pipedream_home_default = 'pipedream'

        # For dev, dir with multiple homes
        hood_env_dir = os.getenv('PIPEDREAM_HOOD')
        if pipedream_hood_override:
            self.hood_dir = pipedream_hood_override
        elif hood_env_dir:
            self.hood_dir = hood_env_dir
        else:
            self.hood_dir = self.pipedream_hood_default

        # Set home_dir via command line, env, or use default
        env_dir = os.getenv('PIPEDREAM_HOME')
        if pipedream_deployment_override:
            self.home_dir = os.path.join(self.hood_dir, pipedream_deployment_override)
        elif env_dir:
            self.home_dir = os.path.join(self.hood_dir, env_dir)
        else:
            self.home_dir = os.path.join(self.hood_dir, self.pipedream_home_default)
        self.home_dir = os.path.normpath(self.home_dir)

        self.force_remote = store

        self._load_directory_info()

        self._load_file_info()

        self._load_cache_files()

        # Set deployment_id via command line, env, or slice.json
        env_deployment = os.getenv('PIPEDREAM_DEPLOYMENT')
        if pipedream_deployment_override:
            self.deployment_id = pipedream_deployment_override
        elif env_deployment:
            self.deployment_id = env_deployment
        elif 'customer_id' in self.slice_config:
            self.deployment_id = self.slice_config['customer_id']
        else:
            self.deployment_id = None
        self.no_cron_jobs = self.slice_config.get('no_cron_jobs', False)

        #Move data directories into/out of cache based on slice flags
        self._override_directory_info()

        self._override_store_location()
        self._setup_isolated_deployment()

        self._load_uuid(vm_uuid_override)
        self._load_vm_info()
        self._load_minion_info()
        self.new_jobs_daemon = 'execution_main' in self.vm_config.get('daemons', [])
        self.is_development = 'PIPEDREAM_HOOD' in os.environ

        # Debug flag; defaults to false, set from home.py
        self.debug = False

        # HTTP Proxy deployment
        self.proxy_info = self._load_proxy_info()
        self.proxy_deployment = self.proxy_info is not None

        # Database settings
        self._load_database_settings()

        # Configuration filenames
        self.config_files = \
        [
            self.users_file,
            self.users_settings_file,
            self.ui_config_file,
            self.tile_config_file,
            self.slice_file,
            self.license_file,
            self.dimensions_idx_file,
            self.interfaces_regexp_file,
            self.setup_config_file,
            self.deployment_makefile,
            self.deployment_file,
            self.qwilt_config,
            self.config_last_update_file
        ]

        # XXX define all of these as self.*_file
        self.extended_config_files = [
            self.local_map_file,
            os.path.join(self.bgp_dir, 'bgp.h5'),
            os.path.join(self.routemap_dir, 'routemap.h5'),
            os.path.join(self.subscribers_dir, 'subscribers.h5'),
            self.eula_file,
            self.notice_package_file,
        ]


        self.genome_files = [
            self.genome_file,
            os.path.join(self.map_dir, 'geoip.h5'),
            os.path.join(self.mine_dir, 'master_names.json.gz'),
            os.path.join(self.routeview_dir, 'routeviews.h5'),
            os.path.join(self.routeview_dir, 'origins.h5'),
            os.path.join(self.genome_dir, 'linkedinlogos.tar.gz'),
            os.path.join(self.genome_dir, 'twitterlogos.tar.gz'),
            os.path.join(self.genome_dir, 'components_db.json'),
            self.asndb_file
        ]
        

        self.has_backbone = self.setup_config.get('backbone') is not None

        # websocket defaults
        self.ws_port = 8888
        self.ws_address = 'wss://127.0.0.1:{}/'.format(self.ws_port)


        # handy global for development
        # used to avoid production commits to store / s3
        # and which redis you write to
        self.dev_mode_flag = False

    def get_config_files(self, smaller_set=True, aws_paths=False):
        '''
        Returns path names for configuration files. If smaller_set is true,
        then omits larger bgp, genome, etc. Note: smaller set includes full
        dimensions db.

        If aws_paths, strip cache_dir from paths.

        For deployment_sync and others.
        '''
        ret = []

        # basic
        ret += self.config_files

        # dimensions
        idx = self.load_json_file(self.dimensions_idx_file)
        if idx:
            dims = idx.get('dimensions', {})
            files = [v.get('file', None) for k, v in dims.iteritems() if v is not None]
            pathnames = [os.path.join(self.dimensions_dir, f) for f in files if f is not None]

            ret += pathnames

        if not smaller_set:
            # extended
            ret += self.extended_config_files

        i = len(self.cache_dir) + 1 # include separator
        if aws_paths:
            ret = [f[i:] for f in ret]
        return ret

    def get_cache_dir(self):
        '''
        Needed to mock it out for the testing framework
        '''
        return os.path.join(self.home_dir, 'cache')

    def _load_database_settings(self):
        self.asndb_database_name = self.slice_config.get('asndb_database_name') or 'asndb'
        self.asndb_database_user = self.slice_config.get('asndb_database_user') or 'root'
        self.asndb_database_password = self.slice_config.get('asndb_database_password') or DEFAULT_DB_PASS

        # Override from environment variables
        if os.environ.get('ASNDB_DATABASE_NAME') is not None:
            self.asndb_database_name = os.environ.get('ASNDB_DATABASE_NAME')
        if os.environ.get('ASNDB_DATABASE_USER') is not None:
            self.asndb_database_user = os.environ.get('ASNDB_DATABASE_USER')
        if os.environ.get('ASNDB_DATABASE_PASSWORD') is not None:
            self.asndb_database_password = os.environ.get('ASNDB_DATABASE_PASSWORD')

        # setup more generic database info
        self.db_database_user = 'root'
        self.db_database_pass = DEFAULT_DB_PASS
        self.db_database = 'deepfield'

    def _load_proxy_info(self):
        '''
        Attempt to load http proxy information from ui.json and/or slice.json,
        ui.json takes precedence
        '''

        proxy_url = self.ui_config.get("proxy_url", "")
        ui_ip, ui_port, ui_user, ui_pass = deepy.url.parse_url(proxy_url)
        ip = ui_ip
        port = ui_port
        username = ui_user
        password = ui_pass

        proxy = self.slice_config.get("proxy", {})
        if not ip:
            ip = proxy.get("ip")
        if not port:
            port = proxy.get("port")
        if not username:
            username = proxy.get("username")
        if not password:
            password = proxy.get("password")

        result = None
        if ip and port:
            result = {}
            result['ip'] = ip
            result['port'] = port
            if username and password:
                result['username'] = username
                result['password'] = password

            no_proxy = proxy.get('no_proxy', None)
            if no_proxy:
                result['no_proxy'] = no_proxy

        return result

    def _load_directory_info(self):
        '''
        Define frequently used directories
        '''

        self.coredumps_dir               = os.path.join('/', 'var', 'coredumps')
        self.install_dir                 = os.path.join('/', 'var', 'local', 'pipedream')
        self.jar_dir                     = os.path.join('/', 'var', 'local', 'pipedream', 'jar')
        self.udfs_dir                    = os.path.join('/', 'var', 'local', 'pipedream', 'udfs')
        self.locks_dir                   = os.path.join('/', 'var', 'tmp')
        self.flows_dir                   = os.path.join(self.home_dir, 'flows')
        self.dnsflows_dir                = os.path.join(self.home_dir, 'dnsflows')
        self.sysflows_dir                = os.path.join(self.home_dir, 'sysflows')
        self.h5flow_dir                  = os.path.join(self.home_dir, 'h5flow')
        self.h5flow_unclassified_dir     = os.path.join(self.home_dir, 'unclassified')
        self.h5flow_classified_dir       = os.path.join(self.home_dir, 'classified')
        self.h5dns_dir                   = os.path.join(self.home_dir, 'h5dns')
        self.logs_dir                    = os.path.join(self.home_dir, 'log')

        self.daemons_dir                 = os.path.join(self.home_dir, 'daemons')
        self.stats_dir                   = os.path.join(self.home_dir, 'stats')
        self.cache_dir                   = self.get_cache_dir()

        self.data_tmp                    = os.path.join(self.home_dir, 'tmp')
        self.geoip_dir                   = os.path.join(self.home_dir, 'geoip')
        self.fake_flows_dir              = os.path.join(self.home_dir, 'fake_flows')
        self.hdfs_fuse_dir               = os.path.join(self.home_dir, 'fuse-dfs')

        self.defines_dir                 = os.path.join(self.install_dir, 'defines')
        self.ui_dir                      = os.path.join(self.install_dir, 'ui')
        self.staging_dir                 = os.path.join(self.install_dir, 'staging')

        # Adjust the directories for these so they don't require a 'make install'
        # when running locally
        # use scons as a flag to determine if we are running in a dev environment
        # should run this from a dev config 
        pipedream_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        self.scons_file = os.path.join(pipedream_path, 'SConstruct')
        if os.path.exists(self.scons_file):
            for path, var in [('config', 'defines'), ('ui', 'ui'), ('staging', 'staging')]:
                repo_path = os.path.join(pipedream_path, path)
                if os.path.exists(repo_path):
                    setattr(self, '%s_dir' % (var), repo_path)

        self.cubes_dir                   = os.path.join(self.cache_dir, 'cubes')
        self.h5qwilt_dir                 = os.path.join(self.cache_dir, 'qwilt', 'h5qwilt')
        self.qwilt_logs_dir              = os.path.join(self.cache_dir, 'qwilt', 'logs')
        self.qwilt_raw_logs_dir          = os.path.join(self.cache_dir, 'qwilt', 'raw_logs')
        self.hll_dir                     = os.path.join(self.cache_dir, 'hll')
        self.hll_dir                     = os.path.join(self.cache_dir, 'hll')
        self.bundles_dir                 = os.path.join(self.cache_dir, 'bundles')
        self.bundles2_dir                = os.path.join(self.cache_dir, 'bundles2')
        self.bundles2_partition_dir      = os.path.join(self.cache_dir, 'bundles2_partition')
        self.dimensions_dir              = os.path.join(self.cache_dir, 'dimensions')
        self.context_dir                 = os.path.join(self.cache_dir, 'context')
        self.aggregation_maps            = os.path.join(self.dimensions_dir, 'aggregation_maps')
        self.archive_dir                 = os.path.join(self.cache_dir, 'archive')
        self.dim_archive_dir             = os.path.join(self.archive_dir, 'dimensions')
        self.config_archive_dir          = os.path.join(self.archive_dir, 'config')
        self.pos_archive_dir             = os.path.join(self.cache_dir, 'pos_archive')
        self.mine_dir                    = os.path.join(self.cache_dir, 'mine')
        self.map_dir                     = os.path.join(self.cache_dir, 'maps')
        self.localmap_dir                = os.path.join(self.cache_dir, 'localmaps')
        self.routemap_dir                = os.path.join(self.cache_dir, 'routemap')
        self.routeview_dir               = os.path.join(self.cache_dir, 'routeviews')
        self.cache_config_dir            = os.path.join(self.cache_dir, 'config')
        self.logging_config              = os.path.join(self.cache_config_dir, 'logging.json')
        self.cache_connectors_dir        = os.path.join(self.cache_dir, 'connectors')
        self.probe_results_dir           = os.path.join(self.cache_dir, 'probe_results')
        self.heartbeat_dir               = os.path.join(self.cache_dir, 'heartbeat')
        self.ui_status_dir               = os.path.join(self.cache_dir, 'ui_status')
        self.snmp_dir                    = os.path.join(self.cache_dir, 'snmp')
        self.cache_status_dir            = os.path.join(self.cache_dir, 'status')
        self.uploads_dir                 = os.path.join(self.cache_dir, 'uploads')
        self.templates_dir               = os.path.join(self.cache_dir, 'templates')
        self.netflow_templates           = os.path.join(self.cache_dir, 'templates')
        self.notices_dir                 = os.path.join(self.cache_dir, 'notices')
        self.data_index_dir              = os.path.join(self.cache_dir, 'data_index')
        self.benchmarks_dir              = os.path.join(self.cache_dir, 'benchmarks_new2')
        self.bgp_dir                     = os.path.join(self.cache_dir, 'bgp')
        self.subscribers_dir             = os.path.join(self.cache_dir, "subscribers")
        self.supplychain_dir             = os.path.join(self.cache_dir, "supplychain")
        self.genome_dir                  = os.path.join(self.cache_dir, "genome")
        self.cache_tmp_dir               = os.path.join(self.cache_dir, 'tmp')
        self.components_dir              = os.path.join(self.cache_dir, 'components')
        self.jobs_stats_dir              = os.path.join(self.cache_dir, 'jobs_stats')
        self.pids_dir                    = os.path.join(self.data_tmp, 'pids')
        self.jobs_dir                    = os.path.join(self.data_tmp, 'jobs')
        self.jobs_queue_dir              = os.path.join(self.data_tmp, 'jobs_queue')
        self.cache_status_flowd_dir      = os.path.join(self.cache_status_dir, 'flowd')
        self.cache_status_dns_dir        = os.path.join(self.cache_status_dir, 'dns')
        self.cache_status_jobs_daemon    = os.path.join(self.cache_status_dir, 'jobs_daemon')
        self.cube_heartbeat_dir          = os.path.join(self.cubes_dir, "heartbeats")
        # Within marker dir, add dirs based on rule name.
        self.cubes_from_h5flow_marker_dir = os.path.join(self.cubes_dir, "cubes_from_h5flow_markers")
        self.merge_hll_marker_di         = os.path.join(self.cubes_dir, "merge_hll_markers")
        self.search_ips_dir              = os.path.join(self.cache_dir, "search_ips")
        self.logos_dir                   = os.path.join(self.ui_dir, "static", "images", "logos")
        self.asndb_dir                   = os.path.join(self.cache_dir, 'asndb')
        self.mysql_dir                   = os.path.join(self.cache_dir, 'mysql')
        self.bgp_dumps_dir               = os.path.join(self.bgp_dir, "dumps")
        self.bps_dir                     = os.path.join(self.cache_dir, 'bps')
        self.vss_dir                     = os.path.join(self.cache_dir, 'vss')
        self.top_procs_dir               = os.path.join(self.cache_dir, 'top_procs')

    #XXX These are overriden after the file names are set. Do not override any directory that
    #  contains any files within it. For example ok to override deepy.cfg.flows_dir, but not deepy.cfg.cache_dir
    def _override_directory_info(self):
        '''
        Manual directory overrides defined from slice
        '''

        directory_overrides = self.slice_config.get("data_dir_overrides", {})
        flow_override = directory_overrides.get("flow", None)
        if flow_override:
            self.flows_dir = os.path.join(self.home_dir, flow_override)

        h5flow_override = directory_overrides.get("h5flow", None)
        if h5flow_override:
            self.h5flow_dir = os.path.join(self.home_dir, h5flow_override)

        dnsflow_override = directory_overrides.get("dnsflow", None)
        if dnsflow_override:
            self.dnsflows_dir = os.path.join(self.home_dir, dnsflow_override)

        h5dns_override = directory_overrides.get("h5dns", None)
        if h5dns_override:
            self.h5dns_dir = os.path.join(self.home_dir, h5dns_override)

        # If data is being synced remotely it should be in cache
        if self.slice_config.get("archive_flow"):
            self.flows_dir = os.path.join(self.cache_dir, 'flows')

        if self.slice_config.get("archive_h5dns"):
            self.h5dns_dir = os.path.join(self.cache_dir, 'h5dns')
            self.dnsflows_dir = os.path.join(self.cache_dir, 'dnsflows')

        if self.slice_config.get("archive_h5flow"):
            self.h5flow_dir = os.path.join(self.cache_dir, 'h5flow')
            self.h5flow_unclassified_dir = os.path.join(self.cache_dir, 'unclassified')
            self.h5flow_classified_dir = os.path.join(self.cache_dir, 'classified')

    def _load_file_info(self):
        '''
        Build up frequently used files from directories previously defined
        '''

        self.current_install_file            = os.path.join(self.home_dir, 'current-install.json')
        self.upgrade_log_file                = os.path.join(self.home_dir, 'upgrade_log.json')
        self.code_root_file                  = os.path.join(self.home_dir, 'code_root')
        self.support_user_file               = os.path.join(self.home_dir, 'support_user')
        self.eula_file                       = os.path.join(self.notices_dir, 'eula.json.gz')
        self.notice_file                     = os.path.join(self.notices_dir, 'notice.json.gz')
        self.notice_package_file             = os.path.join(self.notices_dir, 'notice.tar.gz')
        self.tags_file                       = os.path.join(self.cache_config_dir, 'tags.json')
        self.ui_config_file                  = os.path.join(self.cache_config_dir, 'ui.json')
        self.setup_config_file               = os.path.join(self.cache_config_dir, 'setup.json')
        self.users_file                      = os.path.join(self.cache_config_dir, 'users.json')
        self.users_settings_file             = os.path.join(self.cache_config_dir, 'users_settings.json')
        self.slice_file                      = os.path.join(self.cache_config_dir, 'slice.json')
        self.license_file                    = os.path.join(self.cache_config_dir, 'license.json')
        self.deployment_file                 = os.path.join(self.cache_config_dir, 'deployment.json')
        self.qwilt_config                    = os.path.join(self.cache_config_dir, 'qwilt_config.json')

        # Both soon to be deprecated in favor of using directory 
        #self.deployment_context_file         = os.path.join(self.cache_config_dir, 'context.json')
        #self.context_file                    = os.path.join(self.cache_config_dir, 'context.json')
        
        self.tile_config_file                = os.path.join(self.cache_config_dir, 'tile_config.json')
        self.config_last_update_file         = os.path.join(self.cache_config_dir, 'last_update.json')
        self.config_local_last_update_file   = os.path.join(self.cache_config_dir, 'local_last_update.json')
        self.dimensions_idx_file             = os.path.join(self.cache_config_dir, 'dimensions_idx.json')
        self.interfaces_regexp_file          = os.path.join(self.cache_config_dir, 'interface_regexp.json')
        self.deployment_makefile             = os.path.join(self.cache_config_dir, 'deployment_makefile.py')
        self.dimensions_db_file_unused       = os.path.join(self.dimensions_dir, 'dimensions_db.json.gz')
        self.dimensions_db_small_file_unused = os.path.join(self.dimensions_dir, 'dimensions_db_small.json.gz')
        self.peers_dimension_db_file         = os.path.join(self.dimensions_dir, 'peer.json.gz')
        self.nav_default_file                = os.path.join(self.defines_dir, 'nav_default.json')
        self.components_db_master            = os.path.join(self.defines_dir, 'components_db.json')
        self.components_db                   = os.path.join(self.genome_dir, 'components_db.json')
        self.system_roles_file               = os.path.join(self.defines_dir, 'roles.json')
        self.deployment_roles_file           = self.users_file
        self.attributes_config_file          = os.path.join(self.defines_dir, 'attributes.json')
        self.services_file                   = os.path.join(self.defines_dir, 'services.json')
        self.geoip_latlon_db_file            = os.path.join(self.defines_dir, 'geoip_latlon_db.json')
        self.jobs_makefile                   = os.path.join(self.defines_dir, 'jobs_makefile.json')
        self.rc_file                         = os.path.join(self.defines_dir, 'rc.json')
        self.log_cube_config                 = os.path.join(self.defines_dir, 'log_cube_config.json')
        #self.system_context_file             = os.path.join(self.defines_dir, 'system_context.json')
        self.certfile                        = os.path.join(self.ui_dir, 'cert', 'deepfield.net.chained.crt')
        self.certkeyfile                     = os.path.join(self.ui_dir, 'cert', 'deepfieldkey.pem')
        self.local_map_file                  = os.path.join(self.map_dir, 'localmap.h5')
        self.genome_file                     = os.path.join(self.map_dir, 'genome.h5')
        self.ui_log                          = os.path.join(self.logs_dir, 'ui.log')
        self.perf_log                        = os.path.join(self.logs_dir, 'perf.log')
        self.change_log                      = os.path.join(self.logs_dir, 'change.log')
        self.code_state                      = os.path.join(self.logs_dir, 'code_state.json')
        self.system_state                    = os.path.join(self.logs_dir, 'system_state.json')
        self.users_log                       = os.path.join(self.logs_dir, 'users.json')
        self.metric_queries_file             = os.path.join(self.logs_dir, 'queries.json')
        self.flowd_status                    = os.path.join(self.cache_status_flowd_dir, 'flowd.json')
        self.local_queue_file                = os.path.join(self.data_tmp, 'local_queue.json')
        self.local_queue_lock_file           = os.path.join(self.locks_dir, 'local_queue.lock')
        self.data_index                      = os.path.join(self.data_index_dir, 'index.json.gz')
        self.authorized_keys_file            = os.path.join(self.staging_dir, 'shared', 'authorized_keys')
        self.pdrops_key_file                 = os.path.join(self.staging_dir, 'shared', 'pdrops.key')
        self.s3cmd_cfg_file                  = os.path.join('/', 'home', 'support', '.s3cfg')
        self.boto_cfg_file                   = os.path.join('/', 'etc', 'boto.cfg')
        self.bgpd_cfg_file                   = os.path.join('/', 'etc', 'quagga', 'bgpd.conf')
        self.bird_cfg_file                   = os.path.join('/', 'usr', 'local', 'etc', 'bird.conf')
        self.bird6_cfg_file                  = os.path.join('/', 'usr', 'local', 'etc', 'bird6.conf')
        self.cron_file                       = os.path.join('/', 'etc', 'cron.d', 'deepfield')
        self.snmpd_conf_file                 = os.path.join('/', 'etc', 'snmp', 'snmpd.conf')
        self.snmpd_file                      = os.path.join('/', 'etc', 'default', 'snmpd')
        self.asndb_file                      = os.path.join(self.asndb_dir, 'asndb.sql.gz')
        self.asndb_installed_file            = os.path.join(self.asndb_dir, 'asndb-installed.json')
        self.h5qwilt_template                = os.path.join(self.h5qwilt_dir, 'qwilt.%Y-%m-%d-%H-%M.h5')
        self.qwilt_log_template              = os.path.join(self.qwilt_logs_dir, 'qwilt.%Y-%m-%d-%H-%M.log.gz')
        self.top_procs_template              = 'procs.%Y-%m-%d-%H-%M-%S.h5'

    def _load_uuid(self, vm_uuid_override):
        '''
        This function loads uuid information from 'uuid.txt' unless
        an override is provided.
        '''

        self.uuid_file = os.path.join(self.home_dir, "uuid.txt")
        if vm_uuid_override is not None:
            # For testing.
            self.vm_uuid = vm_uuid_override
        elif os.path.isfile(self.uuid_file):
            self.vm_uuid = open(self.uuid_file).read().strip()
        else:
            self.vm_uuid = None

    def _load_vm_info(self):
        '''
        This function loads config information on the VM level. This is a
        subset of the deployment level. This information only pertains to
        the actual machine/vm this is running on, not the deployment as a
        whole.
        '''

        # vm.get() default to "" so we don't get None == None if vm_uuid still undefined
        it = iter(vm for vm in self.slice_config.get("vms", []) \
            if vm.get("uuid", "") == self.vm_uuid)

        # Take the first (should only be one) vm with this uuid
        self.vm_config = next(it, {})

        # this will get the vm config in deployment.json
        master = self.deployment_config.get("master") or {}
        minions = self.deployment_config.get("minions") or {}
        vm = {}
        if self.vm_uuid == master.get('uuid', ''):
            vm = master
        else:
            for minion in minions:
                minion = minions[minion]
                if self.vm_uuid == minion.get("uuid", ''):
                    vm = minion
                    break

        # merge the two config files
        self.vm_config = dict(self.vm_config.items() + vm.items())

    def  _load_vm_meta(self):
       vms = self.deployment_config.get('minions') or {}
       master = self.deployment_config.get('master') or {}
       master_name = master.get("name")
       if not master_name:
           #Fall back to the master name in slice
           master_name = self.vm_config.get('name')

       vms[master_name] = self.deployment_config.get('master', {})
       return vms

    def _load_minion_info(self):
        '''
        This function loads config information on the VM level. This is a
        subset of the deployment level. This information only pertains to
        the actual machine/vm this is running on, not the deployment as a
        whole.
        '''

        minions = self._load_vm_meta()
        for name,minion in minions.iteritems():
            if not minion:
                continue
            if self.vm_uuid == minion.get("uuid"):
                #Make sure name is populated
                if 'name' not in minion:
                    minion['name'] = name
                self.minion_config = minion
                return
        self.minion_config = {}


    def _load_cache_files(self):
        '''
        This function loads config files that are saved via config_commit
        '''

        cache_vars = \
        [
            # Variable name, file name
            ("current_install", self.current_install_file),
            ("slice_config", self.slice_file),
            ("deployment_config", self.deployment_file),
            ("ui_config", self.ui_config_file),
            ("setup_config", self.setup_config_file)
        ]

        # Warn only if no hood
        warn = self.hood_dir == self.pipedream_hood_default

        for var_name, file_name in cache_vars:
            setattr(self, var_name, self.load_json_file(file_name, warn))

    def _override_store_location(self):
        '''
        This function provides the means to override where certain directories
        store to (s3, local, remote, ect)

        This could be extended in the future to include a way to change the
        store location on a per-directory basis or using regex's

        NOTE: override_store = 'local' is also used by jobs.py and
        jobs_daemon.py to signify that SQS shouldn't be used.
        '''

        self.override_directories = \
        [
            self.dimensions_dir,
            self.context_dir,
            self.dim_archive_dir,
            self.cache_config_dir,
            self.cache_connectors_dir,
            self.benchmarks_dir,
            self.bgp_dir,
            self.heartbeat_dir,
            self.cache_tmp_dir,
            self.cache_status_dir,
            self.genome_dir,
            self.mine_dir,
            self.notices_dir,
            self.map_dir,
            self.routemap_dir,
            self.notices_dir,
            self.routeview_dir,
            self.geoip_dir,
            self.snmp_dir,
            self.pos_archive_dir,
            self.uploads_dir,
            self.subscribers_dir,
            self.supplychain_dir,
            self.cube_heartbeat_dir,
            self.components_dir,
            self.asndb_dir,
            self.bps_dir
        ]
        self.override_store = self.slice_config.get('override_store', 's3')

    def _setup_isolated_deployment(self):
        '''
        Special logic to handle isolated deployment config, 'isolated_deployment'
        is used to mark a deployment that shouldn't try connect externally at all.

        Use 'isolated_deployment' to trigger other necessary settings. These
        settings are poorly named and confusing to explain:

            'store_local' tells the vm to use local files for the normal case,
            typically the data files.

            'override_store' tells the vm where to get the "override"
            directories (overriding the normal case). Typically these go in s3
            and are config files.

        XXX Don't really like overwriting slice_config. Maybe change store to
        read from top level of deepy.cfg?
        '''

        slice_isolated_deployment = self.slice_config.get('isolated_deployment',
            False)

        # Allow dev option for force_remote to override isolated_deployment
        # so I can force config_commit to s3.
        if self.force_remote is None and slice_isolated_deployment:
            self.isolated_deployment = slice_isolated_deployment
            self.slice_config['store_local'] = True
            self.override_store = 'local'
        else:
            self.isolated_deployment = False

    ### Utility functions ###

    @staticmethod
    def load_json_file(file_name, warn=True):
        # XXX Looking at files on disk? FIX THIS
        try:
            return json.load(open(file_name))
        except (IOError, ValueError):
            # Logging config depends on this, can't log here
            # f = log.warn if warn else log.debug
            # f()
            #print >> sys.stderr, 'cfg-load-failed %s' % (file_name)
            return {}

    def init(self, pipedream_deployment_override=None,
                pipedream_hood_override=None, store=None,
                vm_uuid_override=None, force_init=False):
        '''
        For backwards compatability with calling deepy.cfg.init()
        '''

        if not force_init and pipedream_deployment_override == self.deployment_id:
            return

        self.__init__(pipedream_deployment_override=pipedream_deployment_override,
            pipedream_hood_override=pipedream_hood_override, store=store,
            vm_uuid_override=vm_uuid_override)

    def test_init(self, pipedream_path, vm_uuid_override=None):
        '''
        For use with unit tests. Change options a bit to make it simpler to do the right thing.
        '''

        hood, pipedream_dir = os.path.split(pipedream_path)
        self.__init__(pipedream_deployment_override=pipedream_dir,
                pipedream_hood_override=hood, store='local',
                vm_uuid_override=vm_uuid_override)

    def deployment_swap(self, new_deployment_id):
        '''
        For backwards compatability with calling deepy.cfg.deployment_swap()
        '''

        old_deployment_id = self.deployment_id
        self.init(pipedream_deployment_override=new_deployment_id)
        return old_deployment_id


# XXX: This keeps the old module reference from being garbage collected. If
# this wasn't here we'd lose the ability to reference imported modules
reference = sys.modules[__name__]

# XXX: This line is important, it replaces module level functionality with
# the Config class. I.e. deepy.cfg.whatever will be translated to the
# respective class method or variable in Config
sys.modules[__name__] = Config()

