"""defines implementation specific targets"""

import collections
import urlparse
import funcy
import fnmatch
import json
import os


import deepy.store
import snakebite
import snakebite.errors
from deepy.build.util import deepy_substitution
from builder.targets import Target, LocalFileSystemTarget, GlobLocalFileSystemTarget



class StoreBackedLocalFileSystemTarget(LocalFileSystemTarget):
    """A local file system target is backedup in S3

    The information about the file is retrieved from S3 first and
    falls back to the local file system
    """
    @staticmethod
    def non_cached_mtime(local_path):
        """Gets the non cached mtime of a the local_path

        is static so get_bulk_exists_mtime can use it

        returns:
            The value of the mtime if the file exists, otherwise None
        """
        ls_files = deepy.store.ls_files_remote([local_path])
        if ls_files.get(local_path, None) is None:
            mtime = super(
                StoreBackedLocalFileSystemTarget,
                StoreBackedLocalFileSystemTarget).non_cached_mtime(local_path)
            return mtime
        else:
            return ls_files[local_path]

    @staticmethod
    def get_bulk_exists_mtime(targets):
        """Gets all the exists and mtimes for the local paths and returns them
        in a dict. Just as efficient as normal mtime and exists

        returns:
            dict = {
                "file_name": {
                        "exists": exists,
                        "mtime": mtime,
                },
            }
        """
        exists_mtime_dict = {}
        local_paths = [x.unique_id for x in targets]
        deepy.log.debug("Bulk listing {} local paths".format(len(local_paths)))

        result_dict = deepy.store.ls_files_remote(local_paths, bulk=True)

        for target in targets:
            local_path = target.unique_id
            if local_path not in result_dict:
                mtime = super(
                    StoreBackedLocalFileSystemTarget,
                    StoreBackedLocalFileSystemTarget).non_cached_mtime(local_path)
                exists = mtime is not None
            else:
                mtime = result_dict[local_path]
                exists = mtime is not None

            exists_mtime_dict[local_path] = {
                "exists": exists,
                "mtime": mtime,
            }
            target.mtime = mtime
            target.cached_mtime = True

        deepy.log.debug("Completed bulk listing {} local paths".format(len(local_paths)))
        return exists_mtime_dict

    def do_get_mtime(self):
        """Returns the value of the mtime of the file as reported
        by the local filesystem

        Also caches the existance value of the file

        Returns:
            The value of the mtime if the file exists, otherwise None
        """
        mtime = StoreBackedLocalFileSystemTarget.non_cached_mtime(self.unique_id)

        return mtime


class StoreBackedGlobLocalFileSystemTarget(GlobLocalFileSystemTarget):
    """Used to get information about glob targets."""
    unexpanded_id = "s3_backed_glob_local_file_system_target"

    @staticmethod
    def s3_prefix_from_glob(pattern):
        """Takes in a glob path and returns the s3 prefix

        Args:
            pattern: a glob pattern

        Return:
            Removes everything from the first glob pattern to the
            end of the filename and returns this new pattern
        """
        start = 0
        while True:
            start = pattern.find("*", start)
            if start == -1:
                break
            if (start == 0 or start == len(pattern) or
                    (pattern[start - 1] != "[" and pattern[start + 1] != "]")):
                return pattern[:start]
            start = start + 1
        return pattern

    @staticmethod
    def non_cached_mtime(pattern):
        """Gets the maximum mtime of the files that match the glob pattern

        Is statis so get_bulk_exists_mtime can use it

        returns:
            The value of the maximum mtime if at least one file exists that
            matches the patterns, otherwise None
        """
        s3_prefix = (StoreBackedGlobLocalFileSystemTarget
                         .s3_prefix_from_glob(pattern))
        s3_list = deepy.store.list_files_remote([s3_prefix])
        max_mtime = None
        for s3_file_name in s3_list:
            if fnmatch.fnmatchcase(s3_file_name, pattern):
                if max_mtime is None or max_mtime < s3_list[s3_file_name]:
                    max_mtime = s3_list[s3_file_name]

        if max_mtime is None:
            max_mtime = GlobLocalFileSystemTarget.non_cached_mtime(pattern)

        return max_mtime

    def get_mtime(self):
        """The mtime retrieved corresponds to the largest mtime matching the
        pattern
        """
        mtime = self.non_cached_mtime(self.unique_id)

        return mtime

    @staticmethod
    def get_bulk_exists_mtime(targets):
        """Uses the non chached mtime to retrieve mtimes in bulk.
        Just as efficient as getting the values individually
        """
        patterns = [x.unique_id for x in targets]
        exists_mtime_dict = {}
        for pattern in patterns:
            mtime = StoreBackedGlobLocalFileSystemTarget.non_cached_mtime(pattern)
            exists = mtime is not None
            exists_mtime_dict[pattern] = {
                    "exists": exists,
                    "mtime": mtime,
            }
        return exists_mtime_dict



class DeepyStoreBackedLocalFileSystemTarget(StoreBackedLocalFileSystemTarget):
    """Used to make sure that all the nodes have the deepy $(var)s replaced"""
    def __init__(self, unexpanded_id, local_path, build_context, config=None):
        super(DeepyStoreBackedLocalFileSystemTarget, self).__init__(
                unexpanded_id, local_path, build_context,
                config=config)
        self.unique_id = deepy_substitution(self.unique_id)

class DeepyStoreBackedGlobLocalFileSystemTarget(StoreBackedGlobLocalFileSystemTarget):
    """Used to make sure that all the nodes have the deepy $(var)s replaced"""
    def __init__(self, unexpanded_id, pattern, build_context, config=None):
        super(DeepyStoreBackedGlobLocalFileSystemTarget, self).__init__(
                unexpanded_id, pattern, build_context,
                config=config)
        self.unique_id = deepy_substitution(self.unique_id)


# pdtest.py run
class ImpalaTimePartitionedTarget(Target):
    """An ImpalaTableTarget is a target that corresponds to the insertion of a single partition
    into an impala table

    The mtime is retrieved from the impala data index
    The existence value is determined by the impala data index
    """
    def __init__(self, unexpanded_id, unique_id, build_context, dataset_name, time_step,
                 compacted=False, config=None):
        super(ImpalaTimePartitionedTarget, self).__init__(unexpanded_id, unique_id, build_context, config=config)
        self.dataset_name = dataset_name
        self.time_step = time_step
        self.table_manager = self._get_table_manager()
        self.compacted = compacted

    def _get_snakebite_client(self):
        return snakebite.client.AutoConfigClient()

    table_managers = collections.defaultdict(dict)

    def _get_table_manager(self):
        dataset_table_managers = self.table_managers[self.dataset_name]
        table_manager = dataset_table_managers.get(self.time_step)
        if not table_manager:
            import deepy.impala.tables as impala_tables
            table_manager = impala_tables.make_cube_data_manager(
                    self.dataset_name, self.time_step)
            dataset_table_managers[self.time_step] = table_manager

        print table_managers

        return table_manager

    @staticmethod
    def _partition_exists_mtime(full_partition_path, hdfs_client):
        try:
            paths = tuple(hdfs_client.ls([full_partition_path]))
        except snakebite.errors.FileNotFoundException:
            return {'mtime': None, 'exists': False}
        exists_paths = filter(lambda x: x['length'] !=0, paths)
        if len(exists_paths) == 0:
            return {'mtime': None, 'exists': False}
        mtime = max(map(lambda x: x['modification_time'], paths))
        exists =  len(exists_paths) > 0
        return {'mtime': mtime, 'exists': exists}

    @staticmethod
    def get_bulk_exists_mtime(targets):
        """Gets all the exists and mtimes for the local paths and returns them
        in a dict. Just as efficient as normal mtime and exists
        """

        # Get unique dataset/time_step pairs
        tables = funcy.flatten(map(lambda x: x.get_tables(), targets))
        unique_tables = set(tables)

        # Get locations for each table from impala
        table_locations = {}
        for table in unique_tables:
            table_locations[table] = table.location

        # For each target, decide if it exists and get its mtime if it does
        exists_mtime_dict = {}
        for target in targets:
            tables = target.get_tables()
            for table in tables:
                location = table_locations[table]
                if location is not None:
                    location = urlparse.urlparse(location).path
                else:
                    exists_mtime_dict[target.unique_id] = {'exists': False, 'mtime': None}
                    continue
                partition_path = target.get_partition_path(target.build_context['start_time'])
                full_path = '{}{}'.format(location, partition_path)
                hdfs_client = target._get_snakebite_client()
                exists_mtime = ImpalaTimePartitionedTarget._partition_exists_mtime(full_path, hdfs_client)
                #TODO does this evaluate to false when missing?
                if not exists_mtime:
                    continue
                exists_mtime_dict[target.unique_id] = ImpalaTimePartitionedTarget._partition_exists_mtime(
                    full_path, hdfs_client)

        return exists_mtime_dict

    def _get_exists_mtime(self):
        result = {'mtime': None, 'exists': False}
        for table in self.get_tables():
            location = table.location
            if location is not None:
                location = urlparse.urlparse(location).path
            else:
                continue
            partition_path = self.get_partition_path(self.build_context['start_time'])
            full_path = '{}{}'.format(location, partition_path)

            result = self._partition_exists_mtime(full_path, self._get_snakebite_client())
            if result:
                break
        return result

    def do_get_mtime(self):
        """Returns the value of the mtime of the file as reported
        by the hdfs
        """
        exists_mtime = self._get_exists_mtime()
        return exists_mtime['mtime']

    def get_tables(self):
        """Gets the table that contains this target's data

        Returns:
            [table_name (string)]
        """
        tables = self.table_manager.get_insertion_tables(self.build_context['start_time'])
        return tables

    def get_partition_path(self, timestamp):
        """Get partition path for this target. Does not include the prefix, this is only the
        partition path, e.g. /year=2015/month=02/day=06
        """
        return self.table_manager.partition_path(timestamp, compacted=self.compacted)
